<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R on Gertjan Verhoeven</title>
    <link>https://gsverhoeven.github.io/categories/r/</link>
    <description>Recent content in R on Gertjan Verhoeven</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Thu, 20 Jun 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://gsverhoeven.github.io/categories/r/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Exploring Process Mining in R</title>
      <link>https://gsverhoeven.github.io/post/exploring-process-mining/</link>
      <pubDate>Thu, 20 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://gsverhoeven.github.io/post/exploring-process-mining/</guid>
      <description>In this post, we’ll explore the BupaR suite of Process Mining packages created by Gert Janssenswillen from Hasselt University.
We start with exploring the patients dataset contained in the eventdataR package. According to the documentation, this is an “Artifical eventlog about patients”.
Getting startedAfter installing all required packages, we can load the whole “bupaverse” by loading the bupaR package.
library(ggplot2)library(bupaR)#library(eventdataR)df &amp;lt;- eventdataR::patientsThe data is already in event log format, let’s check it out.</description>
    </item>
    
    <item>
      <title>Designing an introductory course on Causal Inference</title>
      <link>https://gsverhoeven.github.io/post/causal-inference-course/</link>
      <pubDate>Sat, 11 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://gsverhoeven.github.io/post/causal-inference-course/</guid>
      <description>Introduction(Short intro) This is me learning causal inference (CI) by self-study together with colleagues using online resources.
(Longer intro) A modern data scientist needs to become skilled in at least three topics (I left out visualization):
(Bayesian) Statistical modelingMachine LearningCausal inferenceFor the first two topics, great introductory books exist that
focus on learning-by-doing andare low on math and high on simulation / programming in Rare fun / well writtenFor Bayesian statistical modeling, we have the awesome textbook “Statistical Rethinking” by Richard mcElreath.</description>
    </item>
    
    <item>
      <title>The validation set approach in caret</title>
      <link>https://gsverhoeven.github.io/post/validation-set-approach-in-caret/</link>
      <pubDate>Thu, 21 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://gsverhoeven.github.io/post/validation-set-approach-in-caret/</guid>
      <description>In this blog post, we explore how to implement the validation set approach in caret. This is the most basic form of the train/test machine learning concept. For example, the classic machine learning textbook “An introduction to Statistical Learning” uses the validation set approach to introduce resampling methods.
In practice, one likes to use k-fold Cross validation, or Leave-one-out cross validation, as they make better use of the data. This is probably the reason that the validation set approach is not one of caret’s preset methods.</description>
    </item>
    
    <item>
      <title>BART vs Causal forests showdown</title>
      <link>https://gsverhoeven.github.io/post/bart_vs_grf/bart-vs-grf-showdown/</link>
      <pubDate>Sat, 15 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://gsverhoeven.github.io/post/bart_vs_grf/bart-vs-grf-showdown/</guid>
      <description>Load packages# library(devtools)#devtools::install_github(&amp;quot;vdorie/dbarts&amp;quot;)library(dbarts)library(ggplot2)library(tidyverse)## -- Attaching packages ----------------------------------------------- tidyverse 1.2.1 --## v tibble 2.0.1 v purrr 0.2.5## v tidyr 0.8.2 v dplyr 0.7.8## v readr 1.3.1 v stringr 1.3.1## v tibble 2.0.1 v forcats 0.3.0## -- Conflicts -------------------------------------------------- tidyverse_conflicts() --## x dplyr::filter() masks stats::filter()## x dplyr::lag() masks stats::lag()library(grf)#devtools::install_github(&amp;quot;vdorie/aciccomp/2017&amp;quot;)library(aciccomp2017)library(cowplot)## ## Attaching package: &amp;#39;cowplot&amp;#39;## The following object is masked from &amp;#39;package:ggplot2&amp;#39;:## ## ggsavesource(&amp;quot;calcPosteriors.</description>
    </item>
    
    <item>
      <title>Improving a parametric regression model using machine learning</title>
      <link>https://gsverhoeven.github.io/post/interaction_detection/interaction-detection/</link>
      <pubDate>Fri, 01 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://gsverhoeven.github.io/post/interaction_detection/interaction-detection/</guid>
      <description>The idea is that comparing the predictions of an RF model with the predictions of an OLS model can inform us in what ways the OLS model fails to capture all non-linearities and interactions between the predictors. Subsequently, using partial dependence plots of the RF model can guide the modelling of the non-linearities in the OLS model. After this step, the discrepancies between the RF predictions and the OLS predictions should be caused by non-modeled interactions.</description>
    </item>
    
  </channel>
</rss>