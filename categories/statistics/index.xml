<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Statistics | Gertjan Verhoeven</title>
    <link>/categories/statistics/</link>
      <atom:link href="/categories/statistics/index.xml" rel="self" type="application/rss+xml" />
    <description>Statistics</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2019-2022</copyright><lastBuildDate>Thu, 21 Sep 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Statistics</title>
      <link>/categories/statistics/</link>
    </image>
    
    <item>
      <title>The Super League: Tournament Blood Bowl online</title>
      <link>/post/blood-bowl-super-league/</link>
      <pubDate>Thu, 21 Sep 2023 00:00:00 +0000</pubDate>
      <guid>/post/blood-bowl-super-league/</guid>
      <description>


&lt;p&gt;&lt;em&gt;(Photo by &lt;a href=&#34;https://twitter.com/erikcats&#34;&gt;Erik Cats&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Yet another Blood Bowl post! If you don’t know about Blood Bowl and/or FUMBBL, See my &lt;a href=&#34;https://gsverhoeven.github.io/categories/blood-bowl/&#34;&gt;previous blog posts on Blood Bowl&lt;/a&gt; for more background and stats.&lt;/p&gt;
&lt;p&gt;This one is to introduce the Super League, where top (and not so top) coaches compete with each other online on FUMBBL. The Super League is a system of interconnected leagues, in a pyramid-like system. This is not uncommon in sports, a well known example is the English Football league system, also called the “football pyramid”.
The creator and tireless promotor of this league, the infamous &lt;strong&gt;Tripleskull&lt;/strong&gt; from Denmark, asked me to do some data analysis on the SL, with this blog as a result.&lt;/p&gt;
&lt;p&gt;How does it work? Each season coaches play a match against all the other coaches in their division / tier (“single round-robin”). At the end of the season coaches are promoted to a higher division, remain, or are relegated to a lower division, based on how well they did. New coaches have to start in the lowest division and work their way to the top. The highest division is called the “Premier League”, where some of the world’s best Blood Bowl coaches compete for the crown (well ring in fact, designed and 3D printed by &lt;strong&gt;Flashman1234&lt;/strong&gt;).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2023-09-21_fumbbl_super_league_files/Strider.jpeg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In a post on Discord, current head admin &lt;strong&gt;Storr&lt;/strong&gt; explains the design of the Super League:
&lt;!-- https://discord.com/channels/1082212787796836383/1083357378453442610/1133125373903777853 --&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;The Super League is designed as a tiered league with a pyramid structure to allow coaches to play against opponents of their own level. This is done intentionally in contrast to the TSD*, where we have completely random divisions each season with a playoff at the end. We do have a rather high number of relegations and promotions to allow everyone to pass rather quickly to the tier they &amp;quot;should&amp;quot; be in, but we need to try to keep the balance between the two. I know it is a bit unfortunate to join now and having to take the long way to the top, but that&amp;#39;s what it is in a tiered league. There will not be a reset in the Super League, as that would be against the very nature and idea of the league. There is, however, the option to make the lower tiers a bit wider, as we don&amp;#39;t follow through with the pyramid structure through all tiers at the moment. The reason for that is that the league grew quite large rather quick, and a decision was made to implement the divisional structure as it is now, possibly because it fit the participant numbers well (i.e. &amp;quot;historical reasons). We will think about expanding the pyramid structure, especially if the league grows even further, as we also don&amp;#39;t want to handle a giant swiss division at the bottom (ask LemonheadWallenstein why), but I won&amp;#39;t make any promises.

*TackleZone Star Division, another competitive league/tournament on FUMBBL&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this point we should mention &lt;strong&gt;TheFear&lt;/strong&gt;, who together with &lt;strong&gt;TripleSkull&lt;/strong&gt; started it all, and was sole admin for Season 0 and 1.
His historic blog post on FUMBBL introducing the Super League can be found &lt;a href=&#34;https://fumbbl.com/p/blog&amp;amp;c=TheFear&amp;amp;id=24149&#34;&gt;here on FUMBBL&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To illustrate the (somewhat) pyramid-like structure of the SL I made a schematic:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2023-09-21_fumbbl_super_league_files/pyramid.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Each rectangular block is a division with 8 coaches, except for the lowest division, which uses a Swiss pairing system (i.e. coaches are initially randomly matched and after that are matched based on their performance so-far). At the time of writing the league is going into its third season, with 188 coaches competing.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2023-09-21_fumbbl_super_league_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A “season 0” Swiss tournament was played to fill the “empty” pyramid with coaches according to their performance in S0. This filled the top half of the pyramid, after that new coaches were divided randomly across the remaining divisions (D3 and D4), hence this season was nicknamed “the season of shortcuts” by &lt;strong&gt;Lorebass&lt;/strong&gt;. In November 2022 Season 1 kicked off. Going into S2 (march 2023) and now S3 (sept 2023), the system is working as intended with coaches moving through the pyramid via promotion and relegation.&lt;/p&gt;
&lt;div id=&#34;ranking-the-stars&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Ranking the stars!&lt;/h1&gt;
&lt;p&gt;So what can be learned from two seasons of Super League competition play?
As this is about competition, let’s try and create a ranking over who did best over those two seasons.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2023-09-21_fumbbl_super_league_files/rel_pro_coaches.png&#34; style=&#34;width:100.0%&#34; /&gt;
Here we use the ranking primarily to summarise / show part of the data.
In creating the ranking, I made several choices that can be considered arbitrary, I mention these in parentheses.&lt;/p&gt;
&lt;p&gt;The ranking is constructed as follows:
Main idea is that competing in the Premier League is valued the most (&lt;em&gt;Arbitrary Decision #1, division level as main score&lt;/em&gt;). This puts &lt;strong&gt;Strider84&lt;/strong&gt; and &lt;strong&gt;RolexWL&lt;/strong&gt; firmly at the top as the only coaches that ended up, and managed to stayed in the Premier League for all three seasons. Here, &lt;strong&gt;Strider84&lt;/strong&gt; is clearly number one, since he also managed to win the PL (&lt;em&gt;Arbitrary Decision #2, trophies as first tie breaker&lt;/em&gt;).
Next we have a block of six coaches that played twice in the PL, and once in the league directly below (Championship A/B).
Using trophies as a tie breaker puts &lt;strong&gt;Purplegoo&lt;/strong&gt; in third place with two trophies, and &lt;strong&gt;Storr&lt;/strong&gt; in fourth place with one trophy.&lt;/p&gt;
&lt;p&gt;How to rank the remaining four coaches?
For this I looked at placement within the division, averaged over S1 and S2. This puts &lt;strong&gt;Stimme&lt;/strong&gt; in fifth place, as his average placement was 2. (&lt;em&gt;Arbitrary decision #3, average placement over season disregarding division level as second tie breaker&lt;/em&gt;).
Finally I cut the list at a threshold of score 12 or less (I award 6 points for playing one season in the PL, 5 points for a single season in the CH etc).
This resulted in one Dutch Coach making the list, and that honour goes to &lt;strong&gt;Schwifty&lt;/strong&gt;, who managed to accomplish this playing Nurgle! Respect.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rising-to-the-top&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Rising to the top&lt;/h1&gt;
&lt;p&gt;As all new coaches have to start at the bottom (with Season 1 being an exception where coaches could be randomly placed in D2, halfway in the pyramid), we can change perspective. Let’s have a look at ALL coaches who in the upcoming season (S3) play in the top two divisions and how they got there.
The top two divisions are Premier League and Championship A/B, containing 24 coaches:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2023-09-21_fumbbl_super_league_files/s3_top_coaches.png&#34; style=&#34;width:100.0%&#34; /&gt;
In the Premier League, we see two new names: &lt;strong&gt;Junior84&lt;/strong&gt; and &lt;strong&gt;siggi&lt;/strong&gt;. They rose with lightning speed to the top division, and show what is possible for the rest of us. Good luck up there competing with the best!
In the Championship A/B division, we have a group of five coaches that steadfastly marched their way up from Division 2.
Here we have a Dutch coach as well, go &lt;strong&gt;Edwin&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-teams-do-successful-coaches-pick&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What teams do successful coaches pick?&lt;/h1&gt;
&lt;p&gt;Note that the league placing belongs to the &lt;strong&gt;coach&lt;/strong&gt;. Coaches can swap teams (between seasons) and retain their ranking.
So what teams did successful coaches pick in Season 2?&lt;/p&gt;
&lt;p&gt;In season 2 the World Cup ruleset was used, with one exception: no star players.
Now, we already know the World Cup win rates by team, thanks to Mike Davies and his awesome Tableau stats site.
Here I show the top 10 teams with the highest win rate at the World Cup:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2023-09-21_fumbbl_super_league_files/wc_winrates.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Top five teams in the WC ruleset are Underworld, Dark Elf, Amazon, Lizardmen and Shambling Undead.
Indeed, the same teams did well in the Super League:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2023-09-21_fumbbl_super_league_files/successful_coach_team_picks.png&#34; style=&#34;width:100.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here I looked at teams from Coaches that played in the top three divisions in Season 2, and managed to either remain in their league, or that got promoted to a higher league.&lt;/p&gt;
&lt;p&gt;Interestingly, two coaches managed to do well with &lt;strong&gt;Imperial Nobility&lt;/strong&gt;, and also low tier teams such as &lt;strong&gt;Nurgle&lt;/strong&gt;, &lt;strong&gt;Chaos Chosen&lt;/strong&gt; and &lt;strong&gt;Black Orc&lt;/strong&gt; has some degree of success. Note that the absence of Old World Alliance in the Super League is likely because its success in the World Cup depended heavily on Star players.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;final-remarks&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Final remarks&lt;/h1&gt;
&lt;p&gt;If you’re interested in following along head over to Discord or FUMBBL. If you would like to join you have to wait a few months, Season 4 will likely start in Feb/March 2024.
If you’re interested in the data analysis itself, both data and R code are available on my GitHub, the repository &lt;a href=&#34;https://github.com/gsverhoeven/fumbbl_super_league&#34;&gt;fumbbl_super_league&lt;/a&gt; is a good place to start.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Visualizing Blood Bowl Tournament rosters</title>
      <link>/post/blood-bowl-eurobowl-rosters/</link>
      <pubDate>Mon, 27 Feb 2023 00:00:00 +0000</pubDate>
      <guid>/post/blood-bowl-eurobowl-rosters/</guid>
      <description>


&lt;p&gt;&lt;em&gt;(Photo by &lt;a href=&#34;https://twitter.com/erikcats&#34;&gt;Erik Cats&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Yet another Blood Bowl post! This one is to warm up to the upcoming World Cup, analyzing variation in Roster choices by top coaches. For the uninitiated: In my spare time I like to play Blood Bowl, a chess-like board game where two teams play a match of fantasy football. It is set in the Warhammer universe, with teams populated by Orcs, Dwarves, Elves etc. See my &lt;a href=&#34;https://gsverhoeven.github.io/categories/blood-bowl/&#34;&gt;previous blog posts on Blood Bowl&lt;/a&gt; for more background and stats. To get an estimate of the player base: a popular Blood Bowl channel on Youtube called “Bonehead podcast” currently has some 16K subscribers.&lt;/p&gt;
&lt;p&gt;So the cool thing of Blood Bowl is that it has a highly competitive Tournament scene. With the ultimate tournament of course being the World Cup! The previous World Cup was held in 2019 in Dornbirn, Austria, and attracted 1400 coaches. This year the World Cup is hosted by Spain, and at least as many coaches are expected in Alicante to battle it out.&lt;/p&gt;
&lt;p&gt;Blood Bowl tournaments typically come with a “Rule pack”. This describes which teams are allowed, how much gold coaches have to buy players, and how much skills can be allotted to players to improve their ball handling, or ability to knock other players down.&lt;/p&gt;
&lt;p&gt;It turns out that the World Cup is pretty similar to the Eurobowl ruleset, so we can learn from the Eurobowl rosters to prepare for the World Cup. Bad roster choices will put you at a disadvantage even before the actual game has started.
So let’s check out what the pro’s are using!&lt;/p&gt;
&lt;div id=&#34;online-blood-bowl-tournaments-on-fumbbl&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Online Blood Bowl Tournaments on FUMBBL&lt;/h1&gt;
&lt;p&gt;Blood Bowl can also be played online. Here we focus on &lt;a href=&#34;https://fumbbl.com/&#34;&gt;FUMBBL.com&lt;/a&gt;, consisting of a Game client (a Java app), linked to a website that performs match making, calculates rankings, provides a forum for discussions etc.&lt;/p&gt;
&lt;p&gt;There are many things great with respect to FUMBBL. Here I like to mention three key elements that allow for this post:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Online Blood Bowl tournaments are organized on FUMBBL that use exactly the same rules as large tabletop tournaments such as Eurobowl,&lt;/li&gt;
&lt;li&gt;FUMBBL has an API where we can fetch the data we need for roster analysis,&lt;/li&gt;
&lt;li&gt;FUMBBL stores full replays of all FUMBBL matches, so we can observe how successful rosters can be played&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;the-2022-road-to-malta-online-naf-tournament&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The 2022 Road to Malta Online NAF tournament&lt;/h1&gt;
&lt;p&gt;Last November, Eurobowl took place in Malta. To get an impression check &lt;a href=&#34;https://www.youtube.com/watch?v=G2JCLe7nikM&#34;&gt;this vid on Youtube&lt;/a&gt;. A few months before Eurobowl 2022, the &lt;a href=&#34;thenaf.net&#34;&gt;Nuffle Amorical Football (NAF)&lt;/a&gt; organized an online Tournament “Road to Malta” on FUMBBL to warm up to this event. The Online Tournament used the exact same ruleset as the Eurobowl itself, and the Rulepack for the 2023 World cup is highly similar.&lt;/p&gt;
&lt;p&gt;The rulepack for the Road to Malta can be found &lt;a href=&#34;https://member.thenaf.net/index.php?module=NAF&amp;amp;type=tournaments&amp;amp;func=view&amp;amp;id=7005&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This blog post’s main goal is to share the analysis results. &lt;a href=&#34;https://github.com/gsverhoeven/hugo_source/tree/master/content/post/fumbbl_rosters_post&#34;&gt;Check my Github&lt;/a&gt; for the technical stuff (R/Python web scraping and visualization of FUMBBL data).&lt;/p&gt;
&lt;p&gt;I’ll discuss the most popular races, and leave the rest for the interested reader. I created &lt;a href=&#34;https://github.com/gsverhoeven/hugo_source/tree/master/content/post/fumbbl_rosters_post/roster_book.pdf&#34;&gt;a PDF rosterbook&lt;/a&gt; containing all 29 different races that participated in the tournament. For each race I made a plot (a clustered heatmap) displaying the roster as a colorcoded matrix, where teams that are similar (based on a cost weighted similarity score) are put side by side. To give some information about the performance of the various builds, I added the tournament points scored behind the coach name.&lt;/p&gt;
&lt;p&gt;Of course, roster build is only one factor influencing tournament results, with skill being much more important, so we have to keep that in mind. Occasionaly, I’ll compare with the Eurobowl rosters and their performance, for this I use the great &lt;a href=&#34;https://public.tableau.com/app/profile/mike.sann0638.davies/viz/Eurobowl22/Introduction&#34;&gt;Tableau resource&lt;/a&gt; compiled by Mike Davies.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;top-10-races-in-road-to-malta&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Top 10 races in Road To Malta&lt;/h1&gt;
&lt;p&gt;The Road to Malta tournament featured 46 teams of 4 coaches each, supplying a total of 184 rosters for our analysis.
Here we focus on the top 10 most popular teams:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2023-03-01_fumbbl_rosters_blood_bowl_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The two most popular teams with 16 rosters each were &lt;strong&gt;Underworld Denizens&lt;/strong&gt; and &lt;strong&gt;High Elf&lt;/strong&gt;.&lt;/p&gt;
&lt;div id=&#34;underworld-denizens-tier-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Underworld Denizens (Tier 2)&lt;/h2&gt;
&lt;p&gt;We start with Underworld Denizens. Even after the nerf to UW by GW in November 2021 they kept their popularity.
And 10 out of 16 rosters include a star player! All rosters pick the full allowance of Blitzer, Thrower, Runner and three clan rats. From the plot below, we see that the Morg plus Troll build was most popular, but that there were various other “Big guy + star player” builds, with the big guy being either a Troll or the Rat Ogre, and the star player being either Varag, Glart, or Kreek.&lt;/p&gt;
&lt;p&gt;Surprisingly, there are no Hakflem builds ! This might be due to the additional skill cost, although Morg too had an additional skill cost, leaving only a single skill left, that was unanimously used to gain an extra re-roll by putting the &lt;code&gt;leader&lt;/code&gt; skill on the thrower.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2023-03-01_fumbbl_rosters_blood_bowl_files/uw_roster_malta.png&#34; style=&#34;width:150.0%&#34; /&gt;
The roster with the best performance (by Sandune) was the Varag plus Troll build.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;high-elf-tier-4&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;High elf (Tier 4)&lt;/h2&gt;
&lt;p&gt;UW shared the top spot for most popular team with High Elves. High Elf’s recent popularity is likely related to their low tier placement, for this tournament it was Tier 4. This allowed for 8 primary skills and 1 secondary skill (or 6 primary skills and 2 secondary skills), on a 12 player line up.
From the rosters, it appears that there are two roster variants popular. Both variants have the same lineup wrt players (2 blitzers, 3 catcher, 1 thrower).
Coaches either choose three rerolls without an apo, or two rerolls with apo. The two-reroll builds all use leader to compensate for the “missing” reroll.&lt;/p&gt;
&lt;p&gt;The best performing roster here was by Hartl78, who fielded a Sneaky Git/ Dirty player lineman, a most unelvish thing to do :) For the World cup, High elves moved from Tier 4 to Tier 2, so performance of the team will likely suffer.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2023-03-01_fumbbl_rosters_blood_bowl_files/he_roster.png&#34; style=&#34;width:100.0%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;necromantic-horror-tier-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Necromantic Horror (Tier 2)&lt;/h2&gt;
&lt;p&gt;Necro was in tier 2, allowing for seven primary skills. All rosters went for this skill pack.
Skill wise, well, a lot of guard and block!
For Necro, there where three variants popular:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;One werewolf, 6 zombies, 3 rerolls&lt;/li&gt;
&lt;li&gt;Two werewolves, 4 zombies, 2 rerolls&lt;/li&gt;
&lt;li&gt;Two werewolves, 3 zombies, 3 rerolls&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Performance-wise, no clear differences to be seen.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2023-03-01_fumbbl_rosters_blood_bowl_files/nh_roster_malta.png&#34; style=&#34;width:200.0%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;shambling-undead-tier-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Shambling Undead (Tier 1)&lt;/h2&gt;
&lt;p&gt;The Shambling Undead rosters are all surprisingly similar:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2 mummies with guard,&lt;/li&gt;
&lt;li&gt;2 wights (one guard, one tackle or mighty blow),&lt;/li&gt;
&lt;li&gt;4 ghoul runners (two with block, 1 wrestle), and&lt;/li&gt;
&lt;li&gt;three rerolls.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The only noticable variation is whether to complement that lineup with 5 zombies, 4 zombies plus 1 skeleton, or 3 zombies plus 2 skeletons. Performance wise there is little to guide that last judgement, as all three variants performed similar.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2023-03-01_fumbbl_rosters_blood_bowl_files/su_roster_malta.png&#34; style=&#34;width:200.0%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;dark-elf-tier-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Dark Elf (Tier 1)&lt;/h2&gt;
&lt;p&gt;All coaches selected the skill pack with 6 primary skills.
Skill wise, there appears to be strong consensus: dodge on the blitzers, block and wrestle on the Witch Elves, and if you pick a runner put leader on it for that extra reroll.&lt;/p&gt;
&lt;p&gt;There is &lt;strong&gt;some&lt;/strong&gt; variation in the positional choices, we can distinguish three builds:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;No assassins, 1 runner, 4-5 linemen&lt;/li&gt;
&lt;li&gt;1 assassin, 1 runner, 3 linemen&lt;/li&gt;
&lt;li&gt;2 assassins, no runners, 3 linemen&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;No clear performance differences between the three builds.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2023-03-01_fumbbl_rosters_blood_bowl_files/de_roster_malta.png&#34; style=&#34;width:200.0%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;chaos-dwarf-tier-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Chaos Dwarf (Tier 1)&lt;/h2&gt;
&lt;p&gt;For chaos dwarves, the big question is whether to take a minotaur or not. The teams without minotaur did slightly worse.
Without a minotaur, the full set of five hobgoblins can be fielded, as well as an additional reroll and apothecary.
Skill wise, we can see that the three rosters with sure hands on a hobgoblin underperformed, suggesting that this skill choice is subtoptimal here.
We can see that the most common skill allocation is to put block on the bull centaurs and guard on the dwarf blockers.&lt;/p&gt;
&lt;p&gt;Interestingly, the highest scoring build by Liam has an uncommon skill choice, with guard on the bull centaurs and two blockers with Mighty Blow.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2023-03-01_fumbbl_rosters_blood_bowl_files/cd_roster_malta.png&#34; style=&#34;width:200.0%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;skaven-tier-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Skaven (Tier 2)&lt;/h2&gt;
&lt;p&gt;Skaven did very well during the Eurobowl, with three Skaven teams in the top 10 best performing coaches.
Two of those coaches also participated in Road to Malta, Olivierdulac and Sokratez (Tank).&lt;/p&gt;
&lt;p&gt;First the commonalities: All rosters have the Rat ogre (Block or Juggernaut) and at least three gutter runners, of which one has strip ball.
For Skaven I highlighted three variants:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Star player builds without blitzer (Either Kreek or Glart, this leaves 4 skills)&lt;/li&gt;
&lt;li&gt;3 reroll builds without Thrower&lt;/li&gt;
&lt;li&gt;2 reroll build with thrower (+ leader)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/post/2023-03-01_fumbbl_rosters_blood_bowl_files/sk_roster_malta.png&#34; style=&#34;width:200.0%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lizardmen-tier-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Lizardmen (Tier 1)&lt;/h2&gt;
&lt;p&gt;For lizardmen coaches life is simple. 6 saurusses, 5 skinks, krox and an apo. Put block on all saurusses, done.
That leaves time to bicker on about bigger questions in life, such a whether or not to swap one regular skink for a &lt;strong&gt;Chameleon skink&lt;/strong&gt; :)
Just as with all the previous roster variants, performance wise there is no clear winner.
Some coaches swap one to three blocks for other skills such as guard, tackle or wrestle.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2023-03-01_fumbbl_rosters_blood_bowl_files/lz_roster_malta.png&#34; style=&#34;width:200.0%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;orc-tier-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Orc (Tier 1)&lt;/h2&gt;
&lt;p&gt;Orcs also did very well in Malta, with three teams in the top 10.
The orcs show some serious roster variation! Agreed, all rosters take four blitzers with mighty blow and tackle, and four big uns with block and guard, but apart from that we see rosters with and without the Troll, with and without an apo, with and without a Thrower, and with varying amounts of goblins. Interestingly, the most exotic roster, with three goblins, and no thrower, performed best here.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2023-03-01_fumbbl_rosters_blood_bowl_files/orc_roster_malta.png&#34; style=&#34;width:200.0%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;wood-elf-tier-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Wood Elf (Tier 1)&lt;/h2&gt;
&lt;p&gt;Wood elf coaches always take the treeman, two wardancers, and at least two catchers.
Typical skills choices are one wardancer with tackle, and one wardancer with strip ball.&lt;/p&gt;
&lt;p&gt;As the wood elf players are expensive, to get at least 2 rerolls requires a lineup with only 11 players.
Variation is in the number of catchers (2-4) and taking a thrower, versus taking ordinary linemen.
On one end, we see rosters that take all four catchers and a thrower, with only three linemen.
At the other end, we see rosters that take only two catchers and a thrower, or three catchers, and take five linemen.
This allows for three rerolls or two rerolls and an apothecary.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2023-03-01_fumbbl_rosters_blood_bowl_files/we_roster_malta.png&#34; style=&#34;width:200.0%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;final-remarks&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Final remarks&lt;/h1&gt;
&lt;p&gt;For now my curiosity on roster variation has been satisfied, and I really looking forward to playing more and learning the finesses of this great game. The World cup in Alicante will be my first international tournament, hope to see you all there!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Classifying Blood Bowl teams using clustered heatmaps</title>
      <link>/post/blood-bowl-cluster-heatmap/</link>
      <pubDate>Tue, 01 Nov 2022 00:00:00 +0000</pubDate>
      <guid>/post/blood-bowl-cluster-heatmap/</guid>
      <description>


&lt;p&gt;&lt;em&gt;(Photo by &lt;a href=&#34;https://twitter.com/erikcats&#34;&gt;Erik Cats&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;“If you graph the numbers of any system, patterns emerge” is one of my favorite movie quotes (from Darren Aronofsky’s cult movie about mathematics &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; (“pi”)). In this post we’ll graph the numbers from the Blood Bowl Fantasy football game, and see what patterns emerge. Blood Bowl is a board game that can be summarized as “fantasy-chess-with-dice”, but this would hardly do the game justice. For example, in chess both players play with the same pieces, but in Blood Bowl, almost 30 different teams (e.g. orcs, elves, etc) are available to choose from, each team with different skills that require different playing styles. In addition, Blood Bowl coaches must assemble and paint their playing pieces themselves, making it a creative hobby as well.&lt;/p&gt;
&lt;p&gt;For this blog post, we have a look at similarities and differences between the different Blood Bowl 2020 teams, and see where the newly introduced &lt;strong&gt;Black Orc&lt;/strong&gt; and &lt;strong&gt;Khorne&lt;/strong&gt; teams fit in. Using data analysis, we can cluster teams that have similar (average) match performance statistics and graph the data using &lt;strong&gt;heatmaps&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Heatmaps are a graphical representation of the data, with for example darker colors representing higher numbers. This allows patterns to emerge visually, and deviations on the patterns are also easy to spot. (Fun fact: a hundred years ago, people already “shaded matrices” but did not call it a heatmap yet &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-wilkinson_friendly09&#34; role=&#34;doc-biblioref&#34;&gt;Wilkinson and Friendly 2009&lt;/a&gt;)&lt;/span&gt;.)&lt;/p&gt;
&lt;p&gt;We use match performance data from &lt;a href=&#34;https://fumbbl.com&#34;&gt;FUMBBL.com&lt;/a&gt; where Blood Bowl 2020 can be played online. For a previous blog post that describes the process of scraping the data &lt;a href=&#34;https://gsverhoeven.github.io/post/blood-bowl-fumbbl-dataset/&#34;&gt;see here&lt;/a&gt;. I made the match data (currently from August 2020 up to June 2022) publicly available in a &lt;a href=&#34;https://github.com/gsverhoeven/fumbbl_datasets&#34;&gt;Github repository&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;team-play-style-categories&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Team play style categories&lt;/h1&gt;
&lt;p&gt;The most common way to classify the 25-30 different Blood Bowl teams is to distinguish four categories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bash (e.g. Orcs)&lt;/li&gt;
&lt;li&gt;Agile (or Dash) (e.g. Wood Elf)&lt;/li&gt;
&lt;li&gt;Hybrid (e.g. Humans)&lt;/li&gt;
&lt;li&gt;Stunty (e.g. Halflings)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To formally classify a team we can use the follow decision tree, taking as input the most common roster choices for a team:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Does the team roster have a lot of Stunty players and a few Big Guys with negatrait?
&lt;ul&gt;
&lt;li&gt;yes classify as &lt;strong&gt;Stunty&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;no continue&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Does the team roster have 4+ players with Strength skill access but &amp;lt; 4 players with Agility access?
&lt;ul&gt;
&lt;li&gt;yes classify as &lt;strong&gt;Bash&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;no continue&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Does the team roster have 4+ players with Agility skill access but &amp;lt; 4 players with Strength access?
&lt;ul&gt;
&lt;li&gt;yes classify as &lt;strong&gt;Agile / Dash&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;no classify as &lt;strong&gt;Hybrid&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As an example, &lt;strong&gt;Shambling Undead&lt;/strong&gt; are typically played with 2 wights and 2 mummies (4 players with Strength access), but also with four Ghoul runners (4 players with Agility access), so this team is classified as &lt;strong&gt;Hybrid&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Main source for this scheme is &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-dode17&#34; role=&#34;doc-biblioref&#34;&gt;Dode 2017&lt;/a&gt;)&lt;/span&gt;, but the bash/dash/hybrid/stunty categorization is widespread, for example at &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-breidr15&#34; role=&#34;doc-biblioref&#34;&gt;Breidr 2015&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-amiral17&#34; role=&#34;doc-biblioref&#34;&gt;Amiral 2017&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-schlice18a&#34; role=&#34;doc-biblioref&#34;&gt;Schlice 2018&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;It will be interesting to compare the patterns in the data with this categorization.
In the next section we’ll discuss the various match statistics available from FUMBBL, but first we need to prep the data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;read-and-prep-the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Read and prep the data&lt;/h1&gt;
&lt;p&gt;We start with reading in the scraped FUMBBL match data, see my previous blog posts mentioned above for details.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load packages
library(tidyverse)
library(ggfortify)
library(ggrepel)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_mbt &amp;lt;- read.csv(file = &amp;quot;../../../fumbbl_datasets/datasets/current/df_mbt.csv&amp;quot;)

race_types &amp;lt;- unique(df_mbt %&amp;gt;% 
                       select(race_name, race_type) %&amp;gt;%
                       filter(race_type != &amp;quot;&amp;quot;) %&amp;gt;%
                       arrange(race_type))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This blog post focusses on the Blood Bowl 2020 ruleset, for this we need the “Competitive” division from FUMBBL.
(I performed the analysis for the older divisions using the 2016 ruleset as well, the plots can be found at the end of this blog post. )&lt;/p&gt;
&lt;p&gt;A blog post from Schlice &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-schlice18a&#34; role=&#34;doc-biblioref&#34;&gt;Schlice 2018&lt;/a&gt;)&lt;/span&gt; got me interested in BB team classification using data. In his post, he makes heavy use of functional programming using R’s &lt;code&gt;purrr&lt;/code&gt; package. This allows us to write a function and have this function work in parallel on a list of objects, and have it return the results also in list form.&lt;/p&gt;
&lt;p&gt;As this was new to me, I decided to adapt his code to process the four divisions simultaneously. To do so, I wrote a function &lt;code&gt;filter_division()&lt;/code&gt; that takes the source data and selects only matches from a particular division:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;divisions &amp;lt;- c(&amp;quot;Competitive&amp;quot;, &amp;quot;Blackbox&amp;quot;, &amp;quot;Ranked&amp;quot;, &amp;quot;Regular_league&amp;quot;)

filter_division &amp;lt;- function(div_name){
  df_mbt %&amp;gt;% 
  filter(division_name == div_name) %&amp;gt;%
  filter(race_name != &amp;quot;Treeman&amp;quot;) %&amp;gt;%
  filter(race_name != &amp;quot;Simyin&amp;quot;)
}

data_tables &amp;lt;- purrr::map(divisions, filter_division)
names(data_tables) &amp;lt;- divisions&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;data_tables&lt;/code&gt; now contains a list of four datasets, one for each division.&lt;/p&gt;
&lt;p&gt;Next step is to select the variables we are interested in.
Again we use the &lt;code&gt;purrr::map()&lt;/code&gt; function to apply our selection function at once to all four datasets:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;select_stats &amp;lt;- function(df) {
  df %&amp;gt;% 
  select(race_name, race_type, team_score, away_team_score:away_cas_rip) %&amp;gt;%
  select(-(home_cas_bh:home_cas_rip), -(away_cas_bh:away_cas_rip))
}

data_tables &amp;lt;- map(data_tables, select_stats)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We end up with the following set of variables:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_tables$Competitive %&amp;gt;% colnames()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;race_name&amp;quot;       &amp;quot;race_type&amp;quot;       &amp;quot;team_score&amp;quot;      &amp;quot;away_team_score&amp;quot;
##  [5] &amp;quot;home_comp&amp;quot;       &amp;quot;home_pass&amp;quot;       &amp;quot;home_rush&amp;quot;       &amp;quot;home_block&amp;quot;     
##  [9] &amp;quot;home_foul&amp;quot;       &amp;quot;home_cas&amp;quot;        &amp;quot;away_comp&amp;quot;       &amp;quot;away_pass&amp;quot;      
## [13] &amp;quot;away_rush&amp;quot;       &amp;quot;away_block&amp;quot;      &amp;quot;away_foul&amp;quot;       &amp;quot;away_cas&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;a-quick-summary-of-the-fummbl-match-statistics&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A quick summary of the FUMMBL match statistics&lt;/h1&gt;
&lt;p&gt;The match statistics that are made available by FUMBBL are all related to important events during Blood Bowl gameplay.
Of course, teams with similar stats can still be different on some aspect not captured in the data, so we keep that in mind.&lt;/p&gt;
&lt;p&gt;The most obvious one is &lt;strong&gt;scoring Touchdowns&lt;/strong&gt; (&lt;code&gt;team_score&lt;/code&gt; and &lt;code&gt;away_team_score&lt;/code&gt;). As this involves getting the ball in the end zone of the opposing player, stats for actions that contribute to scoring such as &lt;strong&gt;passing the ball&lt;/strong&gt; are also present. There is the number of &lt;strong&gt;completed passes&lt;/strong&gt; (&lt;code&gt;home_comp&lt;/code&gt; and &lt;code&gt;away_comp&lt;/code&gt;) as well as the &lt;strong&gt;net passing distance&lt;/strong&gt; (with distance towards the opposing endzone counted positive and counted negative for passes in the opposite direction) (&lt;code&gt;home_pass&lt;/code&gt; and &lt;code&gt;away_pass&lt;/code&gt;). As passing is risky, running the ball into the endzone is the most common way to score. For running with the ball FUMBBL has the &lt;strong&gt;rushing&lt;/strong&gt; statistic. This name was chosen some 20 years ago, and now may lead to confusion as under the BB2020 rules moving extra squares is called “rushing” (previously called “Going-for-it” or GFI).&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;rushing&lt;/strong&gt; statistic is described in a &lt;a href=&#34;https://www.fumbbl.com/index.php?name=PNphpBB2&amp;amp;file=viewtopic&amp;amp;t=1344&amp;amp;postdays=0&amp;amp;postorder=asc&amp;amp;start=15&#34;&gt;2004 (!) FUMBBL forum post by SkiJunkie (the author of the predecessor of FUMBBL)&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;running/passing in the wrong direction counts negative. 
So if you run one step forward then one step back, your net rushing is 0.

Running/passing up and down neither adds nor subtracts. Getting pushed/thrown does not count. 
Only movement made during the players move who has the ball counts.

So, you can end up with negative rushing/passing. &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is great! The rushing statistic captures both &lt;strong&gt;ball possession&lt;/strong&gt; as well as &lt;strong&gt;the maximum movement of typical ball handlers in each team&lt;/strong&gt;. For example, in a Skaven team, the ball is typically handled by Gutter Runners, that have a maximum movement of 9. Compare this to the Dwarf team, where the Dwarf runner only has movement 6.&lt;/p&gt;
&lt;p&gt;Finally, we have the statistics related towards &lt;strong&gt;taking out opposing players&lt;/strong&gt;. This involves &lt;strong&gt;blocking&lt;/strong&gt;, leading to blocking &lt;strong&gt;casualties&lt;/strong&gt;, as well as &lt;strong&gt;fouling&lt;/strong&gt; (illegally hitting players that are already down). For blocking, the total number of blocks thrown is tracked (&lt;code&gt;home_block&lt;/code&gt; and &lt;code&gt;away_block&lt;/code&gt;), for casualties we have the number of players who suffer a casualty (&lt;code&gt;home_cas&lt;/code&gt; , &lt;code&gt;away_cas&lt;/code&gt;) and for fouling we have the number of fouls performed by each team (&lt;code&gt;home_foul&lt;/code&gt;, &lt;code&gt;away_foul&lt;/code&gt;). For casualties, we use both the number of casualties &lt;strong&gt;inflicted&lt;/strong&gt; as well as casualties &lt;strong&gt;suffered&lt;/strong&gt; (&lt;code&gt;home_cas&lt;/code&gt; and &lt;code&gt;away_cas&lt;/code&gt;). The average amount of casualties suffered quantifies the vulnerability of a team, and will likely be related to the average armour value (AV) of a team.&lt;/p&gt;
&lt;p&gt;As we are interested in statistics at the team level (not for individual matches), we take the average of all the statistics by team:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;divisions &amp;lt;- c(&amp;quot;Competitive&amp;quot;, &amp;quot;Blackbox&amp;quot;, &amp;quot;Ranked&amp;quot;, &amp;quot;Regular_league&amp;quot;)

df_agg &amp;lt;- data_tables$Competitive %&amp;gt;% 
  group_by(race_name, race_type) %&amp;gt;% 
  summarise(Blocks = mean(home_block), 
            Completions = mean(home_comp),
            Fouls = mean(home_foul),
            Distance_passed = mean(home_pass), 
            Running_w_ball = mean(home_rush), 
            Cas_inflicted = mean(home_cas),
            Cas_suffered = mean(away_cas), 
            Touchdowns = mean(team_score),
            size = n())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` has grouped output by &amp;#39;race_name&amp;#39;. You can override using the
## `.groups` argument.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dim(df_agg)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 28 11&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This gives us 11 numbers for 28 different BB2020 teams.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;so-which-teams-have-similar-stats&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;So which teams have similar stats?&lt;/h1&gt;
&lt;p&gt;Next, we want to compare the different Blood Bowl teams on their match statistics with each other, and see which races have comparable stats.&lt;/p&gt;
&lt;p&gt;If we have only one statistic, it is easy: we can just plot the different teams on one axis, the value of the statistic on the other axis, sort them and see which races are closest by.&lt;/p&gt;
&lt;p&gt;Lets do that for the &lt;strong&gt;number of blocks made&lt;/strong&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;options(dplyr.summarise.inform = FALSE)

ggplot(df_agg, aes(x = reorder(race_name, Blocks), y = Blocks, size = size, col = race_type)) +
         geom_point() + scale_size_area() + coord_flip() + labs(x = &amp;quot;team name&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2022-11-01_clustered_heatmaps_blood_bowl_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;
The different colors visualize the Team classification described above.
We can see that the average number of blocks is already pretty predictive of team type.&lt;/p&gt;
&lt;p&gt;But races can be similar if we look at &lt;strong&gt;blocks&lt;/strong&gt;, but dissimilar if we look at e.g. &lt;strong&gt;passing&lt;/strong&gt;. So lets make a few plots with each plot comparing two different metrics (code hidden for readability):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(patchwork)

(p1 | p2)/(p3 | p4) / (p5 | plot_spacer()) +   plot_layout(guides = &amp;#39;collect&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2022-11-01_clustered_heatmaps_blood_bowl_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looking at the plots, we can see that there are clear patterns here. Some metrics are inherently related: Teams that make more passes during a match also cover more passing distance, that is only logical. But there are also patterns that are related to what makes teams different: teams that make more blocks tend to pass less (because they are less good at it).&lt;/p&gt;
&lt;p&gt;Given these patterns, we also see teams that do not conform to the pattern: For example, in the top left plot, the &lt;strong&gt;Goblin&lt;/strong&gt; and &lt;strong&gt;Snotling&lt;/strong&gt; teams tend to make less blocks, but also less passes compared to other teams. And for the top middle plot we see that &lt;strong&gt;Halflings&lt;/strong&gt; are an exception: Given how much they pass, we would expect a higher passing distance. (A quick look at their roster explains this: Halflings have dedicated Catchers (making passing attractive), but also the &lt;strong&gt;Stunty&lt;/strong&gt; trait, making long distance passes more risky, decreasing the average passing distance).&lt;/p&gt;
&lt;p&gt;Finally, which teams are similar depends on which plot we look at! Take for example the &lt;strong&gt;Black Orc&lt;/strong&gt; team: Based on fouling and distance passed (Bottom left plot), it is very similar to the four &lt;strong&gt;Stunty&lt;/strong&gt; teams. However, In the plot that combines Fouls and Blocks (lower right), it suddenly becomes unique!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pca-versus-clustered-heatmaps&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;PCA versus clustered heatmaps&lt;/h1&gt;
&lt;p&gt;So how can we compare the teams on all stats simultaneously? We can try to reduce the number of variables by looking at correlations between variables. For example, we observed a strong (negative) correlation between the number of blocks and the number of passes. We could take these two variables together and map (“project”) each team on a single new “axis” (the straight line in the top left plot). For this a commonly used technique is PCA (“principal component analysis”). The original variables are combined into “principle components” along which the most variation occurs. A nufflytics blog post by &lt;strong&gt;Schlice&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-schlice18a&#34; role=&#34;doc-biblioref&#34;&gt;Schlice 2018&lt;/a&gt;)&lt;/span&gt; applies this technique to classify the Blood Bowl teams of the online &lt;strong&gt;Blood Bowl 2&lt;/strong&gt; variant of the game. Another example of this approach is &lt;a href=&#34;https://www.diva-portal.org/smash/get/diva2:1541669/FULLTEXT02.pdf&#34;&gt;the master thesis project of Tadas Ivanauskas&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-ivanauskas20&#34; role=&#34;doc-biblioref&#34;&gt;Ivanauskas 2020&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;With PCA, a common approach is to focus on the first two principal components, as these can be visualized on a 2D coordinate system (i.e. a scatter plot). However, a big disadvantage of PCA is that the new dimensions are difficult to interpret, because the original data is transformed and combined, and therefore no longer recognizable. In the above mentioned blog post, this becomes apparent when the author puts in serious effort to understand the first four principal components, see &lt;a href=&#34;https://www.nufflytics.com/post/bash-dash-hybrid-diving-deeper/&#34;&gt;his “diving deeper” follow up post&lt;/a&gt;. Because each component is a (different) linear combination of ALL 17 variables (!), interpreting the principal components becomes very difficult. To do so requires a high cognitive load, and it is unclear where the signal ends and noise starts.&lt;/p&gt;
&lt;p&gt;An attractive alternative is performing a cluster analysis on the original data, and using the cluster order to directly visualize the data as a heatmap. This technique is called &lt;strong&gt;clustered heatmaps&lt;/strong&gt;, and is widely used in bioinformatics to display patterns in data.&lt;/p&gt;
&lt;p&gt;Two of the most popular clustering algorithms are &lt;strong&gt;Hierarchical clustering&lt;/strong&gt; and &lt;strong&gt;k-means clustering&lt;/strong&gt;. For example we can ask the computer to cluster the 28 teams into e.g. 5 clusters using the K-means algorithm. This assigns each team to one of &lt;span class=&#34;math inline&#34;&gt;\(k = 5\)&lt;/span&gt; clusters, with the cluster centers chosen such that teams within a cluster are close by the cluster center and far away from the other cluster centers. However, this forces us to pick the number of clusters, AND it does not provide information on team similarity WITHIN a cluster.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;clustered-heatmaps-for-the-win&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Clustered heatmaps for the win&lt;/h1&gt;
&lt;p&gt;Hierarchical clustering circumvents both drawbacks as it clusters ALL the observations in such a way that the most similar observations end up next to each other, and thus provide a complete ordering of all observations. All that remains is to plot the data in this order on a colored grid, and BAM the patterns hit us right between the eyes!&lt;/p&gt;
&lt;p&gt;The basic functionality to make a heatmap is provided in the R package &lt;code&gt;ggplot2&lt;/code&gt; by the &lt;code&gt;geom_tile()&lt;/code&gt; geometric object.
Many wrapper functions around &lt;code&gt;geom_tile()&lt;/code&gt; exist to make clustered heatmaps, however they all combine the clustering step with the plotting step.&lt;/p&gt;
&lt;p&gt;I am currently experimenting with my own wrapper function &lt;code&gt;ggorder_heatmap()&lt;/code&gt;, which I turned into an R package called &lt;code&gt;ggoheatmap&lt;/code&gt;. See for details the GitHub repository for &lt;a href=&#34;https://github.com/gsverhoeven/ggoheatmap&#34;&gt;the ggoheatmap R package&lt;/a&gt;.
Note that it is unstable, under development, and possible not even a good idea, so do not use it yourself.
Instead use &lt;code&gt;pheatmap&lt;/code&gt; by Raivo Kolde, available from CRAN, with the official repo &lt;a href=&#34;https://github.com/raivokolde/pheatmap&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggoheatmap)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;varnames &amp;lt;- c(&amp;quot;Blocks&amp;quot;, &amp;quot;Fouls&amp;quot;, &amp;quot;Cas_inflicted&amp;quot;,  &amp;quot;Distance_passed&amp;quot;, &amp;quot;Completions&amp;quot;, &amp;quot;Running_w_ball&amp;quot;, &amp;quot;Cas_suffered&amp;quot;)
yorder &amp;lt;- data.frame(yorder = 1:length(varnames), variable = varnames)

df_long &amp;lt;- df_agg %&amp;gt;%
  pivot_longer(cols = !c(race_name, race_type, size), names_to = &amp;quot;variable&amp;quot;) %&amp;gt;%
  group_by(variable) %&amp;gt;%
  mutate(sd_value = scale(value)) %&amp;gt;%
  left_join(yorder, by = &amp;quot;variable&amp;quot;)

df_long &amp;lt;- ggoheatmap::hclust_order(df_long, 
                        xvar = &amp;quot;race_name&amp;quot;, 
                        yvar = &amp;quot;variable&amp;quot;, 
                        value_var = &amp;quot;value&amp;quot;,
                   clust_method = &amp;quot;complete&amp;quot;,
                   dist_method = &amp;quot;euclidean&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(patchwork)

p1 &amp;lt;- ggplot(df_long, aes(x = reorder(race_name, cluster_order), y = 1, fill = race_type)) +
  geom_tile() + 
  coord_flip() + 
  labs(x = &amp;quot;&amp;quot;, y = NULL) + 
  scale_y_discrete(labels = NULL, breaks = NULL) 

p2 &amp;lt;- ggorder_heatmap(df_long, 
                xvar = &amp;quot;race_name&amp;quot;, 
                yvar = &amp;quot;variable&amp;quot;, 
                col_var = &amp;quot;sd_value&amp;quot;, 
                order_var = &amp;quot;cluster_order&amp;quot;,
                yorder_var =&amp;quot;yorder&amp;quot;,
                legend = FALSE,
                label_var = &amp;quot;value&amp;quot;, round.digits = 1) + coord_flip() +
  scale_y_discrete(position = &amp;quot;right&amp;quot;)

p1 + p2 + 
  plot_layout(widths = c(1, 6), guides = &amp;quot;collect&amp;quot;) + 
  plot_annotation(title = &amp;#39;BB2020 team typology&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2022-11-01_clustered_heatmaps_blood_bowl_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;1344&#34; /&gt;
And there we have it, our main result.&lt;/p&gt;
&lt;p&gt;First, we see that the existing team categorization does a good job, with the different categories showing different patterns in the match statistics.
Note that I did not pre-specify the sort order here, it emerged naturally from the match statistics by the hierarchical clustering algorithm. So the quote from the Pi movie was correct (at least for Blood Bowl): we graphed the numbers, and patterns emerged!&lt;/p&gt;
&lt;p&gt;What else can we learn from this plot?
The difference between Bash and Hybrid appears not so clear cut: for example: Are Norse (the old BB2020 roster, not the new one) and Chaos Chosen bash or hybrid?
Agile and Stunty teams on the other hand are very distinct.
Underworld Denizens are an interesting edge case:
In BB2020, they cluster right in between Goblin and Skaven teams, I ended up classifying them as Agile.
Finally, it is apparent that Vampires indeed are a category of their own.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusions&lt;/h1&gt;
&lt;p&gt;And now for the question we set out to answer: Where do the new Khorne and Black Orc teams fit in?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Khorne&lt;/strong&gt; are clearly a hybrid team (because of the lack of skills and frenzy reduces base contact and thus blocks).
&lt;strong&gt;Black Orcs&lt;/strong&gt; appear to be quite unique, being a combination of Bashy Hybrid and Fouling.&lt;/p&gt;
&lt;p&gt;Finally we produce the clustered heatmap for all four divisions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;source(&amp;quot;heatmap_post/make_heatmap.R&amp;quot;)

for(i in 1:length(divisions)){
  print(make_heatmap(data_tables[[divisions[i]]], division_name = divisions[i]))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2022-11-01_clustered_heatmaps_blood_bowl_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;1152&#34; /&gt;&lt;img src=&#34;/post/2022-11-01_clustered_heatmaps_blood_bowl_files/figure-html/unnamed-chunk-14-2.png&#34; width=&#34;1152&#34; /&gt;&lt;img src=&#34;/post/2022-11-01_clustered_heatmaps_blood_bowl_files/figure-html/unnamed-chunk-14-3.png&#34; width=&#34;1152&#34; /&gt;&lt;img src=&#34;/post/2022-11-01_clustered_heatmaps_blood_bowl_files/figure-html/unnamed-chunk-14-4.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-amiral17&#34; class=&#34;csl-entry&#34;&gt;
Amiral, Taureau. 2017. &lt;span&gt;“Blood Bowl Teams Strength.”&lt;/span&gt; &lt;em&gt;Blood Bowl Strategies&lt;/em&gt; (blog). November 24, 2017. &lt;a href=&#34;https://bloodbowlstrategies.com/en/relative-strength-of-teams/&#34;&gt;https://bloodbowlstrategies.com/en/relative-strength-of-teams/&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-breidr15&#34; class=&#34;csl-entry&#34;&gt;
Breidr. 2015. &lt;span&gt;“Cyanide - &lt;span&gt;‘Bashy’&lt;/span&gt; Race Choice for League Play.”&lt;/span&gt; Blood Bowl Tactics Forum. 2015. &lt;a href=&#34;https://bbtactics.com/forum/threads/bashy-race-choice-for-league-play.4961/&#34;&gt;https://bbtactics.com/forum/threads/bashy-race-choice-for-league-play.4961/&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-dode17&#34; class=&#34;csl-entry&#34;&gt;
Dode. 2017. &lt;span&gt;“What Is the Bashiest Agility Team After Dark Elves? :: Blood Bowl 2 General Discussions.”&lt;/span&gt; 2017. &lt;a href=&#34;https://steamcommunity.com/app/236690/discussions/0/135512931355543270/&#34;&gt;https://steamcommunity.com/app/236690/discussions/0/135512931355543270/&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-ivanauskas20&#34; class=&#34;csl-entry&#34;&gt;
Ivanauskas, Tadas. 2020. &lt;em&gt;BloodBowl 2 race clustering by different playstyles&lt;/em&gt;. &lt;a href=&#34;http://urn.kb.se/resolve?urn=urn:nbn:se:mau:diva-41540&#34;&gt;http://urn.kb.se/resolve?urn=urn:nbn:se:mau:diva-41540&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-schlice18a&#34; class=&#34;csl-entry&#34;&gt;
Schlice. 2018. &lt;span&gt;“Bash/Dash/Hybrid by the Numbers.”&lt;/span&gt; February 27, 2018. &lt;a href=&#34;http://www.nufflytics.com/post/bash-dash-hybrid-by-the-numbers/&#34;&gt;http://www.nufflytics.com/post/bash-dash-hybrid-by-the-numbers/&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-wilkinson_friendly09&#34; class=&#34;csl-entry&#34;&gt;
Wilkinson, Leland, and Michael Friendly. 2009. &lt;span&gt;“The History of the Cluster Heat Map.”&lt;/span&gt; &lt;em&gt;The American Statistician&lt;/em&gt; 63 (2): 179–84. &lt;a href=&#34;https://doi.org/10.1198/tas.2009.0033&#34;&gt;https://doi.org/10.1198/tas.2009.0033&lt;/a&gt;.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Optimal performance with Random Forests: does feature selection beat tuning?</title>
      <link>/post/random-forest-rfe_vs_tuning/</link>
      <pubDate>Sat, 09 Jul 2022 00:00:00 +0000</pubDate>
      <guid>/post/random-forest-rfe_vs_tuning/</guid>
      <description>


&lt;p&gt;&lt;em&gt;(Photo by &lt;a href=&#34;https://unsplash.com/@skamenar?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText&#34;&gt;Steven Kamenar&lt;/a&gt; on &lt;a href=&#34;https://unsplash.com/s/photos/forest?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText&#34;&gt;Unsplash&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The Random Forest algorithm is often said to perform well “out-of-the-box”, with no tuning or feature selection needed, even with so-called high-dimensional data, where we have a high number of features (predictors) relative to the number of observations.&lt;/p&gt;
&lt;p&gt;Here, we show that Random Forest can still be harmed by irrelevant features, and offer two ways of dealing with it. We can do so either by removing the irrelevant features (using a procedure called &lt;strong&gt;recursive feature elimination (RFE)&lt;/strong&gt;), or by &lt;strong&gt;tuning the algorithm&lt;/strong&gt;, increasing the number of features available during each split (the &lt;code&gt;mtry&lt;/code&gt; parameter in R) during training (model building).&lt;/p&gt;
&lt;p&gt;Furthermore, using a simulation study where I gradually increase the amount of &lt;strong&gt;noise&lt;/strong&gt; (irrelevant features) relative to the &lt;strong&gt;signal&lt;/strong&gt; (relevant features), we find that at some point the RF tuning approach no longer is able to achieve optimal performance. Under such (possibly extreme) circumstances, RF feature selection keeps performing well, filtering out the signal variables from the noise variables.&lt;/p&gt;
&lt;p&gt;But first, why should we care about this 2001 algorithm in 2022? Shouldn’t we be all be using deep learning by now (or doing bayesian statistics)?&lt;/p&gt;
&lt;div id=&#34;why-random-forest-is-my-favorite-ml-algorithm&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Why Random Forest is my favorite ML algorithm&lt;/h2&gt;
&lt;p&gt;The Random Forest algorithm &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-breiman01&#34; role=&#34;doc-biblioref&#34;&gt;Breiman 2001&lt;/a&gt;)&lt;/span&gt; is my favorite ML algorithm for cross-sectional, tabular data. Thanks to &lt;a href=&#34;https://mnwright.github.io/&#34;&gt;Marvin Wright&lt;/a&gt; a fast and reliable implementation exists for R called &lt;code&gt;ranger&lt;/code&gt;&lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-wright_ziegler17&#34; role=&#34;doc-biblioref&#34;&gt;Wright and Ziegler 2017&lt;/a&gt;)&lt;/span&gt;. For tabular data, RF seems to offer the highest value per unit of compute compared to other popular ML methods, such as Deep learning or Gradient Boosting algorithms such as &lt;strong&gt;XGBoost&lt;/strong&gt;. In this setting, predictive performance is often on par with Deep Learning or Gradient boosting &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-svetnik_etal05&#34; role=&#34;doc-biblioref&#34;&gt;Svetnik et al. 2005&lt;/a&gt;; &lt;a href=&#34;#ref-xu_etal21&#34; role=&#34;doc-biblioref&#34;&gt;Xu et al. 2021&lt;/a&gt;)&lt;/span&gt;. For classification prediction models, it has been shown to outperform logistic regression &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-couronne_etal18&#34; role=&#34;doc-biblioref&#34;&gt;Couronné, Probst, and Boulesteix 2018&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The Random Forest algorithm can provide a quick benchmark for the predictive performance of a set of predictors, that is hard to beat with models that explicitly formulate a interpretable model of a dependent variable, for example a linear regression model with interactions and non-linear transformations of the predictors. For a great talk on the Random Forest method, check out &lt;a href=&#34;https://www.youtube.com/watch?v=iVmsJJYjgNs&#34;&gt;Prof. Marvin Wright’s UseR talk from 2019 on YouTube&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;but-it-is-not-perfect-the-effect-of-irrelevant-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;But it is not perfect: the effect of irrelevant variables&lt;/h2&gt;
&lt;p&gt;In that talk, Marvin Wright discusses the common claim that “Random Forest works well on high-dimensional data”. High-dimensional data is common in genetics, when we have say complete genomes for only a handful of subjects. The suggestion is that RF can be used on small datasets with lots of (irrelevant / noise) features without having to do variable selection first.&lt;/p&gt;
&lt;p&gt;To check this claim, Wright shows that RF performance is unaffected by adding 100 noise variables to the &lt;code&gt;iris&lt;/code&gt; dataset, a simple example classification problem with three different species. Because RF uses decision trees, it performs “automatic” feature (predictor / variable) selection as part of the model building process. This property of the algorithm is used to explain this result. A tree model will simply ignore the noise predictors and choose the relevant predictors instead.&lt;/p&gt;
&lt;p&gt;See the accompanying R notebook &lt;a href=&#34;https://github.com/gsverhoeven/hugo_source/blob/master/content/post/rf_rfe_post/prep_and_fit.Rmd&#34;&gt;prep_and_fit.Rmd&lt;/a&gt; that contains all the simulations I performed for this blog post. It also includes the simulation on the &lt;code&gt;iris&lt;/code&gt; dataset:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_list &amp;lt;- readRDS(&amp;quot;rf_rfe_post/fitlist.rds&amp;quot;)
fit_list$algo &amp;lt;- paste0(fit_list$method, &amp;quot;_mtry_&amp;quot;, fit_list$mtry)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_list %&amp;gt;% filter(ds_group == &amp;quot;df_iris&amp;quot; &amp;amp; algo == &amp;quot;ranger_mtry_default&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 17
##   model_id method mtry    ds_id ds_name      ds_group target  forest_type n_obs
##      &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;   &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;        &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;       &amp;lt;int&amp;gt;
## 1        1 ranger default     4 df_iris      df_iris  Species class         150
## 2        1 ranger default     5 df_iris_N100 df_iris  Species class         150
## # … with 8 more variables: n_features &amp;lt;dbl&amp;gt;, fit_id &amp;lt;int&amp;gt;, performance &amp;lt;dbl&amp;gt;,
## #   algo &amp;lt;chr&amp;gt;, performance_lower &amp;lt;dbl&amp;gt;, performance_upper &amp;lt;dbl&amp;gt;,
## #   performance_boot &amp;lt;dbl&amp;gt;, performance_boot_corr &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;source(&amp;quot;rf_rfe_post/plot_performance.R&amp;quot;)

plot_performance(data = fit_list %&amp;gt;% filter(ds_group == &amp;quot;df_iris&amp;quot; &amp;amp; algo == &amp;quot;ranger_mtry_default&amp;quot;),
                 axis_label = &amp;quot;Accuracy (% correct)&amp;quot;,
                 plot_title = &amp;quot;Adding 100 noise variables to the iris dataset&amp;quot;, 
                 facet_var = &amp;quot;algo&amp;quot;, x_var = &amp;quot;ds_name&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2022-07-09-random_forest_rfe_vs_tuning_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Indeed, the performance (here i looked at &lt;em&gt;Accuracy&lt;/em&gt; as percentage of observations correctly classified) is hardly affected.&lt;/p&gt;
&lt;p&gt;(As we will be doing a lot of model performance comparison, I added 90% bootstrapped confidence intervals for the performance metrics used in this post. This interval was generated from 1000 bootstrapped values of R2 using resampling on the vectors of (out-of-bag) predictions and the observed y-values (This deserves its own future blog post, if you can’t wait just check the &lt;a href=&#34;https://github.com/gsverhoeven/hugo_source/blob/master/content/post/rf_rfe_post/prep_and_fit.Rmd&#34;&gt;&lt;code&gt;prep_and_fit.Rmd&lt;/code&gt; notebook&lt;/a&gt; on my Github)).&lt;/p&gt;
&lt;p&gt;However, a counter example, where adding irrelevant features &lt;strong&gt;does hurt&lt;/strong&gt; performance, is quickly found. In &lt;a href=&#34;https://topepo.github.io/caret/recursive-feature-elimination.html&#34;&gt;Chapter 20&lt;/a&gt; of the documentation of &lt;code&gt;caret&lt;/code&gt;, a popular ML package in &lt;code&gt;R&lt;/code&gt;, Max Kuhn introduces the regression problem &lt;strong&gt;Friedman 1&lt;/strong&gt; to illustrate the problem, as well as a possible solution. See for a different example in the literature &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-genuer_etal10&#34; role=&#34;doc-biblioref&#34;&gt;Genuer, Poggi, and Tuleau-Malot 2010&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;mlbench.friedman1()&lt;/code&gt; simulates the regression problem &lt;strong&gt;Friedman 1&lt;/strong&gt;. Inputs are 10 independent variables uniformly distributed on the interval [0,1], only 5 out of these 10 are actually used in the formula to generate the dependent variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(y = 10 sin(\pi x_1 x_2) + 20 (x_3 - 0.5)^2 + 10 x_4 + 5 x_5 + \epsilon\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; is distributed &lt;span class=&#34;math inline&#34;&gt;\(Normal(0, 1)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;(source: &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-friedman91&#34; role=&#34;doc-biblioref&#34;&gt;Friedman 1991&lt;/a&gt;; &lt;a href=&#34;#ref-breiman96&#34; role=&#34;doc-biblioref&#34;&gt;Breiman 1996&lt;/a&gt;)&lt;/span&gt;)&lt;/p&gt;
&lt;p&gt;Kuhn added 40 more noise variables to the dataset and simulated N = 100 observations. Now only five predictors contain signal, whereas the other 45 contain noise. Random Forest with its default setting of &lt;code&gt;mtry&lt;/code&gt; shows poor performance, and only after performing feature selection (removing the irrelevant variables) optimal performance is achieved (see below for more about feature selection, here the point is the reduced performance after adding noise).&lt;/p&gt;
&lt;p&gt;I also reproduced this analysis, but with N = 1000 and with 0, 100 and 500 &lt;strong&gt;additional&lt;/strong&gt; noise variables added (instead of 40).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_performance(data = fit_list %&amp;gt;% filter(ds_group == &amp;quot;df_friedman1&amp;quot; &amp;amp; algo %in% c(&amp;quot;ranger_mtry_default&amp;quot;)),
                 axis_label = &amp;quot;R-squared (%)&amp;quot;,
                 plot_title = &amp;quot;Adding increasing amount of noise to the Friedman 1 dataset&amp;quot;, facet_var = &amp;quot;algo&amp;quot;, x_var = &amp;quot;ds_name&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2022-07-09-random_forest_rfe_vs_tuning_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can check the optimal performance by only including the relevant predictors &lt;span class=&#34;math inline&#34;&gt;\(x_1, x_2,x_3,x_4, x_5\)&lt;/span&gt; in the RF algorithm: such a model has an R-squared of around 88% (not shown). RF including both signal and five noise predictors, the original &lt;strong&gt;Friedman 1&lt;/strong&gt; problem, shows a slight drop in performance to 84% with the default &lt;code&gt;mtry&lt;/code&gt; value. After including an additional 100 noise variables, performance drops further to 56%. And if we add 500 instead of 100 additional noise variables, performance drops even further to 34% R2.&lt;/p&gt;
&lt;p&gt;So how to solve this? In this blog post, I will compare both RF &lt;strong&gt;hyperparameter tuning&lt;/strong&gt; and &lt;strong&gt;feature selection&lt;/strong&gt; in the presence of many irrelevant features.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tuning-rf-or-removing-the-irrelevant-features&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tuning RF or removing the irrelevant features?&lt;/h2&gt;
&lt;p&gt;It seems that most practical guidance to improve RF performance is on &lt;em&gt;tuning the algorithm hyperparameters&lt;/em&gt;, arguing that Random Forest as a tree-based method has built-in feature selection, alleviating the need to remove irrelevant features.&lt;/p&gt;
&lt;p&gt;This is demonstrated by the many guides on (RF/ML) algorithm tuning found online. For example, a currently popular book “Hands-On Machine Learning” &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-geron19&#34; role=&#34;doc-biblioref&#34;&gt;Géron 2019&lt;/a&gt;)&lt;/span&gt; contains a short paragraph on the importance of selecting / creating relevant features, but then goes on to discuss hyperparameter tuning at great length for the remainder of the book.&lt;/p&gt;
&lt;p&gt;Some evidence that RF tuning is sufficient to deal with irrelevant features is provided by Kuhn &amp;amp; Johnson &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kuhn_johnson19&#34; role=&#34;doc-biblioref&#34;&gt;Kuhn and Johnson 2019&lt;/a&gt;)&lt;/span&gt;.
In their book available online, they have a section called &lt;a href=&#34;http://www.feat.engineering/feature-selection-simulation.html&#34;&gt;Effect of Irrelevant Features&lt;/a&gt;. For simulated data with 20 informative predictors, they find that after RF tuning (which is not mentioned in the book but is clear from the &lt;a href=&#34;https://github.com/topepo/FES_Selection_Simulation&#34;&gt;R code provided on Github&lt;/a&gt;), the algorithm is (mostly) robust to up to 200 extra noise variables.&lt;/p&gt;
&lt;p&gt;So let’s start with RF tuning.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-effect-of-rf-tuning-demonstrated-on-openml-datasets&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The effect of RF tuning demonstrated on OpenML datasets&lt;/h2&gt;
&lt;p&gt;To experiment with RF tuning and compare it with RF feature selection, I needed datasets. Using simulated data is always an option, but with such data it is not always clear what the practical significance of our findings is.&lt;/p&gt;
&lt;p&gt;So I needed (regression) datasets that are not too big, nor too small, and where RF tuning has a substantial effect. Finding these was not easy: Surprisingly, online tutorials for RF hyperparameter tuning often only show small improvements in performance.&lt;/p&gt;
&lt;p&gt;Here the benchmarking study of Philipp Probst on RF tuning came to the rescue, as he identified three datasets where RF tuning has a significant effect. Probst created a suite of 29 regression datasets (&lt;code&gt;OpenML-Reg-19&lt;/code&gt;), where he compared tuned ranger with default ranger. The selection of the datasets is described &lt;a href=&#34;https://github.com/PhilippPro/OpenML-bench&#34;&gt;here&lt;/a&gt;. The datasets he used are all made available by &lt;a href=&#34;https://new.openml.org/&#34;&gt;OpenML.org&lt;/a&gt;. This is a website dedicated to reproducible, open ML, with a large collection of datasets, focused on benchmarking and performance comparison.&lt;/p&gt;
&lt;p&gt;Furthermore, For the RF tuning, he created an R package, aptly called &lt;code&gt;tuneRanger&lt;/code&gt;, available on CRAN as well as on &lt;a href=&#34;https://github.com/PhilippPro/tuneRanger&#34;&gt;Github&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Ignoring the red and green lines, and comparing the tuned vs default ranger, it is clear that on many datasets, tuning hardly improves things. Here we see the reputation of RF, that it works well straight out of the box, borne out in practice.&lt;/p&gt;
&lt;p&gt;However, a few did, and three stood out (blue line above dashed line).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2022-01-03-random_forest_rfe_vs_tuning_files/probst_tuning_ranger.png&#34; style=&#34;width:66.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure created by Philipp Probst and reproduced from his TuneRanger Github repository&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;As he made all his code available on Github, I could identify the three datasets as being &lt;a href=&#34;https://www.openml.org/search?type=data&amp;amp;sort=runs&amp;amp;id=560&amp;amp;status=active&#34;&gt;bodyfat&lt;/a&gt;, &lt;a href=&#34;https://www.openml.org/search?type=data&amp;amp;sort=runs&amp;amp;id=505&amp;amp;status=active&#34;&gt;tecator&lt;/a&gt; and &lt;a href=&#34;https://www.openml.org/search?type=data&amp;amp;sort=runs&amp;amp;id=308&amp;amp;status=active&#34;&gt;puma32H&lt;/a&gt;.
&lt;code&gt;Puma32H&lt;/code&gt; is noteworthy in that it is a classic ML dataset for a simulated PUMA 560 robotarm, that contains mostly irrelevant features (30 out of 32) &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-geurts_etal06&#34; role=&#34;doc-biblioref&#34;&gt;Geurts, Ernst, and Wehenkel 2006&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2022-01-03-random_forest_rfe_vs_tuning_files/puma560_schematic.png&#34; style=&#34;width:50.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For these three datasets, I reproduced the results of default &lt;code&gt;ranger()&lt;/code&gt; and tuned the &lt;code&gt;mtry&lt;/code&gt; parameter.&lt;/p&gt;
&lt;p&gt;mtry? what try?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rf-tuning-the-mtry-parameter&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;RF tuning: the mtry parameter&lt;/h2&gt;
&lt;p&gt;A great resource for tuning RF is a 2019 review paper by Probst &lt;em&gt;et al.&lt;/em&gt; called ‘Hyperparameters and Tuning Strategies for Random Forest’ &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-probst_etal19&#34; role=&#34;doc-biblioref&#34;&gt;Probst, Wright, and Boulesteix 2019&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;They conclude:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Out of these parameters, mtry is most influential both according to the literature and in our own experiments. The best value of mtry depends on the number of variables that are related to the outcome.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In this blog post, we use &lt;code&gt;mtry&lt;/code&gt; as the only tuning parameter of Random Forest. This is the number of randomly drawn features that is available to split on as the tree is grown. It can vary between 1 and the total number of features in the dataset. From the literature and my own experience, this is the hyperparameter that matters most. For an interesting discussion on the effect of &lt;code&gt;mtry&lt;/code&gt; on the complexity of the final model (the tree ensemble) see &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-goldstein_etal11&#34; role=&#34;doc-biblioref&#34;&gt;Goldstein, Polley, and Briggs 2011&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Reproducing the results using &lt;code&gt;ranger()&lt;/code&gt; myself, and playing around with the &lt;code&gt;mtry&lt;/code&gt; parameter, I discovered that the three datasets have something in common: they all contain only a few variables that are predictive of the outcome, in the presence of a lot of irrelevant variables. Furthermore, setting &lt;code&gt;mtry&lt;/code&gt; at its maximum value was sufficient to achieve the performance found by Probst after using &lt;code&gt;tuneRanger&lt;/code&gt; (blue line in the figure above).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_performance(data =fit_list %&amp;gt;% filter(ds_group == &amp;quot;openML&amp;quot; &amp;amp; algo %in% c(&amp;quot;ranger_mtry_default&amp;quot;, &amp;quot;ranger_mtry_max&amp;quot;)),
                 axis_label = &amp;quot;R-squared (%)&amp;quot;,
                 plot_title = &amp;quot;mtry tuning for optimal performance on OpenML datasets&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2022-07-09-random_forest_rfe_vs_tuning_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That tuning &lt;code&gt;mtry&lt;/code&gt; for a Random Forest is important in the presence of many irrelevant features was already shown by Hastie &lt;em&gt;et al.&lt;/em&gt; in their 2009 classic book “Elements of statistical Learning” (p615, Figure 15.7) &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-hastie_etal09&#34; role=&#34;doc-biblioref&#34;&gt;Hastie et al. 2009&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;They showed that if &lt;code&gt;mtry&lt;/code&gt; is kept at its default (square root of &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, the total number of features), as more irrelevant variables are added, the probability of the &lt;strong&gt;relevant&lt;/strong&gt; features being selected for splitting becomes too low, decreasing performance. So for datasets with a large proportion of irrelevant features, &lt;code&gt;mtry&lt;/code&gt; tuning (increasing its value) is crucially important.&lt;/p&gt;
&lt;p&gt;Before we move on to RF feature selection, let’s see what else we can tune in RF apart from &lt;code&gt;mtry&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rf-tuning-what-else-can-we-tune&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;RF tuning: what else can we tune?&lt;/h2&gt;
&lt;p&gt;With respect to the other RF parameters, a quick rundown:&lt;/p&gt;
&lt;p&gt;I left the &lt;code&gt;num.trees&lt;/code&gt; at its default (500), and chose “variance” as the &lt;code&gt;splitrule&lt;/code&gt; for regression problems, and “gini” for classification problems (alternative is “extratrees” which implements the “Extremely Randomized Trees” method &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-geurts_etal06&#34; role=&#34;doc-biblioref&#34;&gt;Geurts, Ernst, and Wehenkel 2006&lt;/a&gt;)&lt;/span&gt; but I have yet to see convincing results that demonstrate ERT performs substantially better). I checked a few key results at lower and higher &lt;code&gt;num.trees&lt;/code&gt; as well (100 and 1000 respectively): 100 is a bit low for the Out-of-bag predictions to stabilize, 500 appears to be a sweet spot, with no improvement in R-squared mean or a significant reduction in R2 variance between runs either.&lt;/p&gt;
&lt;p&gt;I played around a bit with the &lt;code&gt;min.node.size&lt;/code&gt; parameter, for which often the sequence 5,10,20 is mentioned to vary over. Setting this larger should reduce computation, since it leads to shorter trees, but for the datasets here, the effect is on the order of say 10% reduction, which does not warrant tuning it IMO. I left this at its default of 5 for regression and 1 for classification.&lt;/p&gt;
&lt;p&gt;Finally, Marvin Wright points to results from Probst &lt;em&gt;et al.&lt;/em&gt;&lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-probst_etal19&#34; role=&#34;doc-biblioref&#34;&gt;Probst, Wright, and Boulesteix 2019&lt;/a&gt;)&lt;/span&gt; that show &lt;code&gt;sample.fraction&lt;/code&gt; to be an important parameter as well. This determines the number of samples from the dataset to draw for each tree. I have not looked into this, instead I used the default setting from &lt;code&gt;ranger()&lt;/code&gt; which is to sample with replacement, and to use all samples for each tree, i.e &lt;code&gt;sample.fraction = 1&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To conclude: we focus on &lt;code&gt;mtry&lt;/code&gt; and leave the rest alone.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;from-rf-tuning-to-rf-feature-selection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;From RF tuning to RF feature selection&lt;/h2&gt;
&lt;p&gt;A natural question to ask is why not simply get rid of the irrelevant features? Why not perform feature selection?&lt;/p&gt;
&lt;p&gt;The classic book &lt;em&gt;Applied Predictive Modeling&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kuhn_johnson13&#34; role=&#34;doc-biblioref&#34;&gt;Kuhn and Johnson 2013&lt;/a&gt;)&lt;/span&gt; contains a similar simulation experiment (on the &lt;code&gt;solubility&lt;/code&gt; dataset, for RF I reproduce their results below) showing the negative effects of including many irrelevant features in a Random Forest model (chapter 18). And indeed, instead of tuning RF, they suggest &lt;strong&gt;removing the irrelevant features altogether&lt;/strong&gt;, i.e. to perform feature selection. Also their follow up book on Feature engineering and selection by Kuhn &amp;amp; Johnson from 2019 &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kuhn_johnson19&#34; role=&#34;doc-biblioref&#34;&gt;Kuhn and Johnson 2019&lt;/a&gt;)&lt;/span&gt; &lt;a href=&#34;http://www.feat.engineering/recursive-feature-elimination.html&#34;&gt;elaborates on this&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rf-feature-selection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;RF Feature selection&lt;/h2&gt;
&lt;p&gt;To perform feature selection, we use the recursive feature elimination (RFE) procedure, implemented for &lt;code&gt;ranger&lt;/code&gt; in &lt;code&gt;caret&lt;/code&gt; as the function &lt;code&gt;rfe()&lt;/code&gt;. This is a backward feature selection method, starting will all predictors and in stepwise manner dropping the least important features &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-guyon_etal02&#34; role=&#34;doc-biblioref&#34;&gt;Guyon et al. 2002&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;When the full model is created, a measure of variable importance is computed that ranks the predictors from most important to least. […] At each stage of the search, the least important predictors are iteratively eliminated prior to rebuilding the model.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;— Pages 494-495, Applied Predictive Modeling, 2013.&lt;/p&gt;
&lt;p&gt;(Computationally, I think it makes more sense to start with only the most relevant features and add more features in a stepwise fashion until performance no longer improves but reaches a plateau. But that would require writing my own “forward procedure”, which I save for another day.)&lt;/p&gt;
&lt;p&gt;As this is a procedure that drops predictors that do not correlate with the outcome, we have to be extremely careful that we end up with something that generalizes to unseen data. In &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kuhn_johnson13&#34; role=&#34;doc-biblioref&#34;&gt;Kuhn and Johnson 2013&lt;/a&gt;)&lt;/span&gt; they convincingly show that a special procedure is necessary, with two loops of cross validation first described by &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-ambroise_mclachlan02&#34; role=&#34;doc-biblioref&#34;&gt;Ambroise and McLachlan 2002&lt;/a&gt;)&lt;/span&gt;. The outer loop sets aside one fold that is not used for feature selection (and optionally model tuning), whereas the inner loop selects features and tunes the model. See &lt;a href=&#34;http://www.feat.engineering/selection-overfitting.html#selection-overfitting&#34;&gt;Chapter 10.4&lt;/a&gt; of their follow up book &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kuhn_johnson19&#34; role=&#34;doc-biblioref&#34;&gt;Kuhn and Johnson 2019&lt;/a&gt;)&lt;/span&gt; for detailed documentation and examples.&lt;/p&gt;
&lt;p&gt;Typically, as we start removing irrelevant features, performs either stays constant or even increases until we reach the point where performs drops. At this point features that are predictive of the outcome are getting removed.&lt;/p&gt;
&lt;p&gt;Note that we do not tune the &lt;code&gt;mtry&lt;/code&gt; variable of RF in this procedure. Empirically, it has been observed that this either has no effect, or only leads to marginal improvements &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kuhn_johnson19&#34; role=&#34;doc-biblioref&#34;&gt;Kuhn and Johnson 2019&lt;/a&gt;; &lt;a href=&#34;#ref-svetnik_etal04&#34; role=&#34;doc-biblioref&#34;&gt;Svetnik, Liaw, and Tong 2004&lt;/a&gt;)&lt;/span&gt;. Conceptually, tuning (increasing) &lt;code&gt;mtry&lt;/code&gt; is a way to reduce the effect of irrelevant features. Since we are applying a procedure to &lt;strong&gt;remove&lt;/strong&gt; the irrelevant features instead, it makes sense that tuning has little benefit here.&lt;/p&gt;
&lt;p&gt;(As I was curious, I nevertheless created a set of custom helper functions for &lt;code&gt;rfe()&lt;/code&gt; that tune &lt;code&gt;mtry&lt;/code&gt; during the feature selection procedure, see the &lt;code&gt;RangerTuneFuncs.R&lt;/code&gt; script, results not shown)&lt;/p&gt;
&lt;!-- While mtry is a tuning parameter for random forest models, the default value of mtry≈sqrt(p) tends to provide good overall performance. While tuning this parameter may have led to better performance, our experience is that the improvement tends to be marginal --&gt;
&lt;p&gt;So we expect that either tuning &lt;code&gt;mtry&lt;/code&gt; , OR performing feature selection solves the problem of irrelevant variables.
Indeed, this is also what we find on the three OpenML datasets. Both the RF tuning approach (Setting &lt;code&gt;mtry&lt;/code&gt; at its maximum value) as well as the RF feature selection using RFE result in optimal performance:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_performance(data =fit_list %&amp;gt;% filter(ds_group == &amp;quot;openML&amp;quot;),
                 axis_label = &amp;quot;R-squared (%)&amp;quot;,
                 plot_title = &amp;quot;mtry tuning vs feature selection for three OpenML datasets&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2022-07-09-random_forest_rfe_vs_tuning_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Can RF feature selection (“ranger-rfe” in the plot below) solve our problems with the “Friedman 1” simulated data as well?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_performance(data = fit_list %&amp;gt;% filter(ds_group == &amp;quot;df_friedman1&amp;quot;),
                 axis_label = &amp;quot;R-squared (%)&amp;quot;,
                 plot_title = &amp;quot;Feature selection vs tuning on simulated data with noise added&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2022-07-09-random_forest_rfe_vs_tuning_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Yes, it can! And here we see that RF tuning is not enough, we really need to identify and remove the irrelevant variables.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;N.b.&lt;/strong&gt; For &lt;code&gt;df_friedman1_N100&lt;/code&gt;, the RFE tuning grid is based on equidistant steps starting with &lt;span class=&#34;math inline&#34;&gt;\(p = 2\)&lt;/span&gt; and included as smallest values only &lt;span class=&#34;math inline&#34;&gt;\(p = 2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(p = 14\)&lt;/span&gt;, so it skipped the optimal value &lt;span class=&#34;math inline&#34;&gt;\(p = 5\)&lt;/span&gt;. This explains the sub optimal performance for RFE with 105 noise variables added. For &lt;code&gt;df_friedman1_N500&lt;/code&gt;, the tuning grid was exponential and included 2, 3, 6 and 12 (up to &lt;span class=&#34;math inline&#34;&gt;\(p = 510\)&lt;/span&gt;). The RFE procedure selected &lt;span class=&#34;math inline&#34;&gt;\(p = 6\)&lt;/span&gt; as optimal, this included as top five variables the five predictors that contained the signal.&lt;/p&gt;
&lt;p&gt;A similar pattern is seen for the &lt;code&gt;solubility&lt;/code&gt; dataset with noise added, an example taken from &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kuhn_johnson13&#34; role=&#34;doc-biblioref&#34;&gt;Kuhn and Johnson 2013&lt;/a&gt;)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_performance(data = fit_list %&amp;gt;% filter(ds_group == &amp;quot;solubility&amp;quot; &amp;amp; ds_name != &amp;quot;solubility_N500_perm&amp;quot;),
                 axis_label = &amp;quot;R-squared (%)&amp;quot;,
                 plot_title = &amp;quot;Feature selection vs tuning on Solubility data with noise added&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2022-07-09-random_forest_rfe_vs_tuning_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note that on the original &lt;code&gt;solubility&lt;/code&gt; dataset, neither tuning nor feature selection is needed, RF already performs well out of the box.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;doing-it-wrong-rf-tuning-after-rfe-feature-selection-on-the-same-dataset&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Doing it wrong: RF tuning after RFE feature selection on the same dataset&lt;/h2&gt;
&lt;p&gt;Finally, we echo others in stressing the importance of using a special nested cross-validation loop to perform the feature selection and performance assessment, especially when a test set is not available. “If the model is refit using only the important predictors, model performance almost certainly improves” according to Kuhn &amp;amp; Johnson (APM 2013). I also found a blog post [here] that references &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-hastie_etal09&#34; role=&#34;doc-biblioref&#34;&gt;Hastie et al. 2009&lt;/a&gt;)&lt;/span&gt; regarding the dangers when using both feature selection and cross validation.&lt;/p&gt;
&lt;p&gt;To drive the point home, I have taken the &lt;code&gt;solubility&lt;/code&gt; dataset with 500 noise predictors added (951 observation, with in total 228 + 500 = 728 predictors), and scrambled the &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; variable we wish to predict. With scrambling I mean shuffling the values of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, thereby removing any correlation with the predictors. This is an easy way to check our procedure for any data leakage from the training set to the test set where we evaluate performance.&lt;/p&gt;
&lt;p&gt;All three RF modeling approaches now correctly report an R-squared of approximately 0%.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_performance(fit_list %&amp;gt;% filter(ds_name == &amp;quot;solubility_N500_perm&amp;quot;),
                 axis_label = &amp;quot;R-squared (%)&amp;quot;,
                 plot_title = &amp;quot;Feature selection vs tuning on simulated data after scrambling y&amp;quot;) + expand_limits(y = 90)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2022-07-09-random_forest_rfe_vs_tuning_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;However, if we do RF tuning on this scrambled dataset, &lt;strong&gt;after&lt;/strong&gt; we performed RFE feature selection, we get cross-validated R-squared values of 5-10%, purely based on noise variables “dredged” from the hundreds of variables we supplied to the algorithm. For full code see the &lt;a href=&#34;https://github.com/gsverhoeven/hugo_source/blob/master/content/post/rf_rfe_post/prep_and_fit.Rmd&#34;&gt;R notebook on my Github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;(Note that I had to play around a bit with the RFE settings to not have it pick either the model with &lt;strong&gt;all&lt;/strong&gt; features or the model with only 1 feature: using &lt;code&gt;RMSE&lt;/code&gt; as a metric, and setting &lt;code&gt;pickSizeTolerance&lt;/code&gt; the procedure selected a model with 75 predictors.
Retraining this model using &lt;code&gt;caret::train&lt;/code&gt; gave me the result below)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trainObject &amp;lt;- readRDS(&amp;quot;rf_rfe_post/post_rfe_train.rds&amp;quot;)

trainObject&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Random Forest 
## 
## 951 samples
##  75 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 857, 855, 855, 856, 856, 858, ... 
## Resampling results across tuning parameters:
## 
##   mtry  RMSE      Rsquared    MAE     
##    1    2.001946  0.07577078  1.562253
##    2    1.997836  0.06977449  1.558831
##    3    1.993046  0.07166917  1.559323
##    5    1.983549  0.08801892  1.548190
##    9    1.988541  0.06977760  1.551304
##   15    1.987132  0.06717113  1.551576
##   25    1.989876  0.06165605  1.552562
##   44    1.985765  0.06707004  1.550177
##   75    1.984745  0.06737729  1.548531
## 
## Tuning parameter &amp;#39;splitrule&amp;#39; was held constant at a value of variance
## 
## Tuning parameter &amp;#39;min.node.size&amp;#39; was held constant at a value of 5
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were mtry = 5, splitrule = variance
##  and min.node.size = 5.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This illustrates the dangers of performing massive variable selection exercises without the proper safeguards.
Aydin Demircioğlu wrote a paper that identifies several radiomics studies that performed cross-validation as a separate step after feature selection, and thus got it wrong &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-demircioglu21&#34; role=&#34;doc-biblioref&#34;&gt;Demircioğlu 2021&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;To conclude: we have shown that for in the presence of (many) irrelevant variables, RF performance suffers and something needs to be done.
This can be either tuning the RF, most importantly increasing the &lt;code&gt;mtry&lt;/code&gt; parameter, or identifying and removing the irrelevant features using the RFE procedure &lt;code&gt;rfe()&lt;/code&gt; part of the &lt;code&gt;caret&lt;/code&gt; package in R. Selecting only relevant features has the added advantage of providing insight into which features contain the signal.&lt;/p&gt;
&lt;p&gt;Interestingly, on the “real” datasets (openML, the solubility QSAR data) both tuning and feature selection give the same result. Only when we use simulated data (Friedman1), or if we add noise to real datasets (iris, solubility)) we find that &lt;code&gt;mtry&lt;/code&gt; tuning is not enough, and removal of the irrelevant features is needed to obtain optimal performance.&lt;/p&gt;
&lt;p&gt;The fact that tuning and feature selection are rarely compared head to head might be that both procedures have different implicit use cases: ML tuning is often performed on datasets that are thought to contain mostly relevant predictors. In this setting feature selection does not improve performance, as it primarily works through the removal of noise variables. On the other hand, feature selection is often performed on high-dimensional datasets where prior information is available telling us that relatively few predictors are related to the outcome, and the many noise variables in the data can negatively influence RF performance.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;N.b.&lt;/strong&gt; As is often the case with simulation studies, an open question is how far we can generalize our results. &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-svetnik_etal03&#34; role=&#34;doc-biblioref&#34;&gt;Svetnik et al. 2003&lt;/a&gt;)&lt;/span&gt; identified a classification dataset &lt;strong&gt;Cox-2&lt;/strong&gt; that exhibits unexpected behavior: The dataset gives optimal performance with &lt;code&gt;mtry&lt;/code&gt; at its maximum setting, indicative of a many irrelevant predictors, so we expect feature selection to find a smaller model that gives the same performance at default &lt;code&gt;mtry&lt;/code&gt;. However, surprisingly, performance only degraded after performing feature selection using RFE. I wrote the authors (Vladimir Svetnik and Andy Liaw) to ask for the dataset, unfortunately they suffered a data loss some time ago. They obtained the data from Greg Kauffman and Peter Jurs &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kauffman_jurs01&#34; role=&#34;doc-biblioref&#34;&gt;Kauffman and Jurs 2001&lt;/a&gt;)&lt;/span&gt;, I reached out to them as well but did not receive a reply.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Acknowledgements&lt;/strong&gt;: I’d like to thank Philipp Probst and Anne-Laure Boulesteix for their constructive comments and suggestions on this blog post.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-ambroise_mclachlan02&#34; class=&#34;csl-entry&#34;&gt;
Ambroise, Christophe, and Geoffrey J. McLachlan. 2002. &lt;span&gt;“Selection Bias in Gene Extraction on the Basis of Microarray Gene-Expression Data.”&lt;/span&gt; &lt;em&gt;Proceedings of the National Academy of Sciences&lt;/em&gt; 99 (10): 6562–66.
&lt;/div&gt;
&lt;div id=&#34;ref-breiman96&#34; class=&#34;csl-entry&#34;&gt;
Breiman, Leo. 1996. &lt;span&gt;“Bagging Predictors.”&lt;/span&gt; &lt;em&gt;Machine Learning&lt;/em&gt; 24 (2): 123–40.
&lt;/div&gt;
&lt;div id=&#34;ref-breiman01&#34; class=&#34;csl-entry&#34;&gt;
———. 2001. &lt;span&gt;“Random Forests.”&lt;/span&gt; &lt;em&gt;Machine Learning&lt;/em&gt; 45 (1): 5–32.
&lt;/div&gt;
&lt;div id=&#34;ref-couronne_etal18&#34; class=&#34;csl-entry&#34;&gt;
Couronné, Raphael, Philipp Probst, and Anne-Laure Boulesteix. 2018. &lt;span&gt;“Random Forest Versus Logistic Regression: A Large-Scale Benchmark Experiment.”&lt;/span&gt; &lt;em&gt;BMC Bioinformatics&lt;/em&gt; 19 (1): 1–14.
&lt;/div&gt;
&lt;div id=&#34;ref-demircioglu21&#34; class=&#34;csl-entry&#34;&gt;
Demircioğlu, Aydin. 2021. &lt;span&gt;“Measuring the Bias of Incorrect Application of Feature Selection When Using Cross-Validation in Radiomics.”&lt;/span&gt; &lt;em&gt;Insights into Imaging&lt;/em&gt; 12 (1): 1–10.
&lt;/div&gt;
&lt;div id=&#34;ref-friedman91&#34; class=&#34;csl-entry&#34;&gt;
Friedman, Jerome H. 1991. &lt;span&gt;“Multivariate Adaptive Regression Splines.”&lt;/span&gt; &lt;em&gt;The Annals of Statistics&lt;/em&gt; 19 (1): 1–67.
&lt;/div&gt;
&lt;div id=&#34;ref-genuer_etal10&#34; class=&#34;csl-entry&#34;&gt;
Genuer, Robin, Jean-Michel Poggi, and Christine Tuleau-Malot. 2010. &lt;span&gt;“Variable Selection Using Random Forests.”&lt;/span&gt; &lt;em&gt;Pattern Recognition Letters&lt;/em&gt; 31 (14): 2225–36.
&lt;/div&gt;
&lt;div id=&#34;ref-geron19&#34; class=&#34;csl-entry&#34;&gt;
Géron, Aurélien. 2019. &lt;em&gt;Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems&lt;/em&gt;. O’Reilly Media, Inc.
&lt;/div&gt;
&lt;div id=&#34;ref-geurts_etal06&#34; class=&#34;csl-entry&#34;&gt;
Geurts, Pierre, Damien Ernst, and Louis Wehenkel. 2006. &lt;span&gt;“Extremely Randomized Trees.”&lt;/span&gt; &lt;em&gt;Machine Learning&lt;/em&gt; 63 (1): 3–42.
&lt;/div&gt;
&lt;div id=&#34;ref-goldstein_etal11&#34; class=&#34;csl-entry&#34;&gt;
Goldstein, Benjamin A., Eric C. Polley, and Farren B. S. Briggs. 2011. &lt;span&gt;“Random forests for genetic association studies.”&lt;/span&gt; &lt;em&gt;Statistical Applications in Genetics and Molecular Biology&lt;/em&gt; 10 (1): 32. &lt;a href=&#34;https://doi.org/10.2202/1544-6115.1691&#34;&gt;https://doi.org/10.2202/1544-6115.1691&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-guyon_etal02&#34; class=&#34;csl-entry&#34;&gt;
Guyon, Isabelle, Jason Weston, Stephen Barnhill, and Vladimir Vapnik. 2002. &lt;span&gt;“Gene Selection for Cancer Classification Using Support Vector Machines.”&lt;/span&gt; &lt;em&gt;Machine Learning&lt;/em&gt; 46 (1): 389–422.
&lt;/div&gt;
&lt;div id=&#34;ref-hastie_etal09&#34; class=&#34;csl-entry&#34;&gt;
Hastie, Trevor, Robert Tibshirani, Jerome H. Friedman, and Jerome H. Friedman. 2009. &lt;em&gt;The Elements of Statistical Learning: Data Mining, Inference, and Prediction&lt;/em&gt;. Vol. 2. Springer.
&lt;/div&gt;
&lt;div id=&#34;ref-kauffman_jurs01&#34; class=&#34;csl-entry&#34;&gt;
Kauffman, Gregory W., and Peter C. Jurs. 2001. &lt;span&gt;“QSAR and k-Nearest Neighbor Classification Analysis of Selective Cyclooxygenase-2 Inhibitors Using Topologically-Based Numerical Descriptors.”&lt;/span&gt; &lt;em&gt;Journal of Chemical Information and Computer Sciences&lt;/em&gt; 41 (6): 1553–60. &lt;a href=&#34;https://doi.org/10.1021/ci010073h&#34;&gt;https://doi.org/10.1021/ci010073h&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-kuhn_johnson13&#34; class=&#34;csl-entry&#34;&gt;
Kuhn, Max, and Kjell Johnson. 2013. &lt;em&gt;Applied Predictive Modeling&lt;/em&gt;. Vol. 26. Springer.
&lt;/div&gt;
&lt;div id=&#34;ref-kuhn_johnson19&#34; class=&#34;csl-entry&#34;&gt;
———. 2019. &lt;em&gt;Feature Engineering and Selection: A Practical Approach for Predictive Models&lt;/em&gt;. CRC Press.
&lt;/div&gt;
&lt;div id=&#34;ref-probst_etal19&#34; class=&#34;csl-entry&#34;&gt;
Probst, Philipp, Marvin N. Wright, and Anne-Laure Boulesteix. 2019. &lt;span&gt;“Hyperparameters and Tuning Strategies for Random Forest.”&lt;/span&gt; &lt;em&gt;Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery&lt;/em&gt; 9 (3): e1301.
&lt;/div&gt;
&lt;div id=&#34;ref-svetnik_etal04&#34; class=&#34;csl-entry&#34;&gt;
Svetnik, Vladimir, Andy Liaw, and Christopher Tong. 2004. &lt;span&gt;“Variable Selection in Random Forest with Application to Quantitative Structure-Activity Relationship.”&lt;/span&gt; &lt;em&gt;Proceedings of the 7th Course on Ensemble Methods for Learning Machines&lt;/em&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-svetnik_etal03&#34; class=&#34;csl-entry&#34;&gt;
Svetnik, Vladimir, Andy Liaw, Christopher Tong, J. Christopher Culberson, Robert P. Sheridan, and Bradley P. Feuston. 2003. &lt;span&gt;“Random Forest: A Classification and Regression Tool for Compound Classification and QSAR Modeling.”&lt;/span&gt; &lt;em&gt;Journal of Chemical Information and Computer Sciences&lt;/em&gt; 43 (6): 1947–58.
&lt;/div&gt;
&lt;div id=&#34;ref-svetnik_etal05&#34; class=&#34;csl-entry&#34;&gt;
Svetnik, Vladimir, Ting Wang, Christopher Tong, Andy Liaw, Robert P. Sheridan, and Qinghua Song. 2005. &lt;span&gt;“Boosting: An Ensemble Learning Tool for Compound Classification and QSAR Modeling.”&lt;/span&gt; &lt;em&gt;Journal of Chemical Information and Modeling&lt;/em&gt; 45 (3): 786–99.
&lt;/div&gt;
&lt;div id=&#34;ref-wright_ziegler17&#34; class=&#34;csl-entry&#34;&gt;
Wright, Marvin N., and Andreas Ziegler. 2017. &lt;span&gt;“Ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R.”&lt;/span&gt; &lt;em&gt;Journal of Statistical Software&lt;/em&gt; 77 (March): 1–17. &lt;a href=&#34;https://doi.org/10.18637/jss.v077.i01&#34;&gt;https://doi.org/10.18637/jss.v077.i01&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-xu_etal21&#34; class=&#34;csl-entry&#34;&gt;
Xu, Haoyin, Kaleab A. Kinfu, Will LeVine, Sambit Panda, Jayanta Dey, Michael Ainsworth, Yu-Chung Peng, Madi Kusmanov, Florian Engert, and Christopher M. White. 2021. &lt;span&gt;“When Are Deep Networks Really Better Than Decision Forests at Small Sample Sizes, and How?”&lt;/span&gt; &lt;em&gt;arXiv Preprint arXiv:2108.13637&lt;/em&gt;.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Using R to analyse the Roche Antigen Rapid Test: How accurate is it?</title>
      <link>/post/covid_antigen_test_reliability/</link>
      <pubDate>Sun, 06 Jun 2021 00:00:00 +0000</pubDate>
      <guid>/post/covid_antigen_test_reliability/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;em&gt;(Image is from a different antigen test)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Many people are suspicious about the reliability of rapid self-tests, so I decided to check it out.
For starters, I LOVE measurement. It is where learning from data starts, with technology and statistics involved.
With this post, I’d like to join the swelling ranks of amateur epidemiologists :) I have spent a few years in a molecular biology lab, that should count for something right?&lt;/p&gt;
&lt;p&gt;At home, we now have a box of the &lt;strong&gt;SARS-CoV-2 Rapid Antigen Test Nasal&lt;/strong&gt; kit.
The kit is distributed by Roche, and manufactured in South Korea by a company called SD Biosensor.&lt;/p&gt;
&lt;p&gt;So how reliable is it? A practical approach is to compare it to the golden standard, the PCR test, that public health test centers use to detect COVID-19. Well, the leaflet of the kit describes three experiments that do exactly that!
So I tracked down the data mentioned in the kit’s leaflet, and decided to check them out.&lt;/p&gt;
&lt;p&gt;But before we analyze the data, you want to know how they were generated, right? RIGHT?
For this we use cause-effect diagrams (a.k.a. DAGs), which we can quickly draw using &lt;a href=&#34;http://dagitty.net&#34;&gt;DAGitty&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;a-causal-model-of-the-measurement-process&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A causal model of the measurement process&lt;/h1&gt;
&lt;p&gt;The cool thing about DAGitty is that we can use a point-n-click interface to draw the diagram, and then export code that contains an exact description of the graph to include in R. (You can also view the &lt;a href=&#34;http://dagitty.net/dags.html?id=whqGBx&#34;&gt;graph for this blog post at DAGitty.net&lt;/a&gt;):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021_05_14_covid_rapid_test_reliability_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The graph is based on the following description of the various cause-effect pairs:&lt;/p&gt;
&lt;p&gt;It all starts with whether someone is infected. After infection, virus particles start to build up. These particles can be in the lungs, in the throat, nose etc.
These particles either do or do not cause symptoms. Whether there are symptoms will likely influence the decision to test, but there will also be people without symptoms that will be tested (i.e. if a family member was tested positive).&lt;/p&gt;
&lt;p&gt;In the experiments we analyze, two samples were taken, one for the PCR test and one for the antigen test. The way the samples were taken differed as well: “shallow” nose swabs for the rapid antigen test, and a combination of “deep” nose and throat swabs for the PCR test.&lt;/p&gt;
&lt;p&gt;Now that we now a bit about the measurement process, lets look at how the accuracy of the antigen test is quantified.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;quantifying-the-accuracy-of-an-covid-19-test&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Quantifying the accuracy of an COVID-19 test&lt;/h1&gt;
&lt;p&gt;The PCR test result serves as the ground truth, the standard to which the antigen test is compared.
Both tests are binary tests, either it detects the infection or it does not (to a first approximation).&lt;/p&gt;
&lt;p&gt;For this type of outcome, two concepts are key: the &lt;strong&gt;sensitivity&lt;/strong&gt; (does the antigen test detect COVID when the PCR test has detected it) and &lt;strong&gt;specificity&lt;/strong&gt; of the test (does the antigen test ONLY detect COVID, or also other flu types or even unrelated materials, for which the PCR test produces a negative result).&lt;/p&gt;
&lt;p&gt;The leaflet contains information on both.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sensitivity 83.3% (95%CI: 74.7% - 90.0%)&lt;/li&gt;
&lt;li&gt;Specificity 99.1% (95%CI: 97.7% - 99.7%)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;But what does this really tell us? And where do these numbers come from?&lt;/p&gt;
&lt;p&gt;Before we go to the data, we first need to know a bit more detail on what we are actually trying to measure, the viral load, and what factors influence this variable.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;viral-load-as-target-for-measurement&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Viral load as target for measurement&lt;/h1&gt;
&lt;p&gt;So, both tests work by detecting viral particles in a particular sample. The amount of virus particles present in the sample depends on, among others:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Time since infection&lt;/li&gt;
&lt;li&gt;How and where the sample is taken (throat, nose, lungs, using a swab etc)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I’ll discuss both.&lt;/p&gt;
&lt;div id=&#34;time-since-infection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Time since infection&lt;/h2&gt;
&lt;p&gt;When you have just been infected, your body will contain only a small amount of virus.
The &lt;strong&gt;viral load&lt;/strong&gt; is a function of time since infection, because it takes time for the virus to multiply itself. Even PCR cannot detect an infection on the first day, and even after 8 days, there are still some 20% of cases that go undetected by PCR (presumably because the amount of viral particle is too low) (Ref: Kucirka et al 2020).&lt;/p&gt;
&lt;p&gt;If you want to know more about the ability of PCR to detect COVID infections go check out &lt;a href=&#34;https://github.com/HopkinsIDD/covidRTPCR&#34;&gt;the covidRTPCR Github repository&lt;/a&gt;. It is completely awesome, with open data, open code, and Bayesian statistics using &lt;a href=&#34;https://mc-stan.org/&#34;&gt;Stan&lt;/a&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-and-where-the-sample-is-taken&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How and where the sample is taken&lt;/h2&gt;
&lt;p&gt;There are many ways to obtain a sample from a person.&lt;/p&gt;
&lt;p&gt;Here the golden standard is a so-called &lt;strong&gt;Nasopharyngeal swab&lt;/strong&gt;. This goes through your nose all the way (~ 5 cm) into the back of the throat, and is highly uncomfortable. Typically, only professional health workers perform &lt;strong&gt;nasopharyngeal&lt;/strong&gt; swabs.
In these experiments, this deep nose swab was combined with a swab from the throat (&lt;strong&gt;oroharyngeal&lt;/strong&gt;). This is also how test centers in the Netherlands operated during the last year.&lt;/p&gt;
&lt;p&gt;There are various alternatives: We have spit, saliva, we can cough up “sputum” (slime from the lungs) or we can take swab from the front part of the nose (“nasal”).&lt;/p&gt;
&lt;p&gt;The Roche antigen test is a &lt;strong&gt;nasal&lt;/strong&gt; test that only goes up to 2 cm in the nose and can be used by patients themselves (“self-collected”).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-dataset-results-from-the-three-berlin-studies&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The dataset: results from the three Berlin studies&lt;/h1&gt;
&lt;p&gt;Now that we have some background info, we are ready to check the data!&lt;/p&gt;
&lt;p&gt;As mentioned above, this data came from three experiments on samples from in total 547 persons.&lt;/p&gt;
&lt;p&gt;After googling a bit, I found out that the experiments were performed by independent researchers in a famous University hospital in Berlin, &lt;a href=&#34;https://de.wikipedia.org/wiki/Charit%C3%A9&#34;&gt;Charité&lt;/a&gt;. After googling a bit more and mailing with one of the researchers involved, Dr. Andreas Lindner, I received a list of papers that describe the research mentioned in the leaflet (References at the end of this post).&lt;/p&gt;
&lt;p&gt;The dataset for the blog post compares &lt;strong&gt;nasal&lt;/strong&gt; samples tested with the Roche Antigen test kit, to PCR-tested &lt;strong&gt;nasopharyngeal&lt;/strong&gt; plus &lt;strong&gt;oropharyngeal&lt;/strong&gt; samples taken by professionals.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;This blog post is possible because the three papers by Lindner and co-workers all contain the raw data as a table in the paper. Cool!&lt;/strong&gt;
Unfortunately, this means the data is not &lt;strong&gt;machine readable&lt;/strong&gt;. However, with a combination of manual tweaking / find-replace and some coding, I tidied the data of the three studies into a single &lt;code&gt;tibble&lt;/code&gt; data frame. You can grab the code and data from my &lt;a href=&#34;https://github.com/gsverhoeven/hugo_source/tree/master/content/post/sars_test&#34;&gt;Github&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;UPDATE&lt;/strong&gt;: Rein Halbersma showed me how to use web scraping to achieve the same result, with a mere 20 lines of code of either Python or R! Cool! I added his scripts to my &lt;a href=&#34;https://github.com/gsverhoeven/hugo_source/tree/master/content/post/sars_test&#34;&gt;Github&lt;/a&gt; as well, go check them out, I will definitely go this route next time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# creates df_pcr_pos
source(&amp;quot;sars_test/dataprep_roche_test.R&amp;quot;)

# creates df_leaflet
source(&amp;quot;sars_test/dataprep_roche_test_leaflet.R&amp;quot;)

# see below
source(&amp;quot;sars_test/bootstrap_conf_intervals.R&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The dataset &lt;code&gt;df_pcr_pos&lt;/code&gt; contains, for each &lt;strong&gt;PCR positive&lt;/strong&gt; patient:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ct_value&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;viral_load&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;days_of_symptoms&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mm_value&lt;/code&gt; (Result of a &lt;strong&gt;nasal&lt;/strong&gt; antigen test measurement, 1 is positive, 0 is negative)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To understand the PCR data, we need to know a bit more about the PCR method.&lt;/p&gt;
&lt;div id=&#34;the-pcr-method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The PCR method&lt;/h2&gt;
&lt;p&gt;The PCR method not only measures &lt;strong&gt;if&lt;/strong&gt; someone is infected, it also provides an estimate of the viral load in the sample.
How does this work? PCR can amplify, in so-called cycles, really low quantities of viral material in a biological sample. The amount of cycles of the PCR device needed to reach a threshold of signal is called the cycle threshold or &lt;strong&gt;Ct value&lt;/strong&gt;. The less material we have in our sample, the more cycles we need to amplify the signal to reach a certain threshold.&lt;/p&gt;
&lt;p&gt;Because the amplification is an exponential process, if we take the log of the number of virus particles, we get a linear inverse (negative) relationship between &lt;strong&gt;ct_value&lt;/strong&gt; and &lt;strong&gt;viral_load&lt;/strong&gt;. For example, &lt;span class=&#34;math inline&#34;&gt;\(10^6\)&lt;/span&gt; particles is a viral load of 6 on the log10 scale.&lt;/p&gt;
&lt;p&gt;So let’s plot the &lt;code&gt;ct_value&lt;/code&gt; of the PCR test vs the &lt;code&gt;viral_load&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)
ggplot(df_pcr_pos, aes(x = ct_value, y = viral_load, color = factor(pcr_assay_type))) + 
  geom_point() + ggtitle(&amp;quot;Calibration curves for viral load (log10 scale)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021_05_14_covid_rapid_test_reliability_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;
This plot shows that &lt;code&gt;viral_load&lt;/code&gt; is directly derived from the &lt;code&gt;ct_value&lt;/code&gt; through a calibration factor.
PCR Ct values of &amp;gt; 35 are considered as the threshold value for detecting a COVID infection using the PCR test, so the values in this plot make sense for COVID positive samples.&lt;/p&gt;
&lt;p&gt;Take some time to appreciate the huge range difference in the samples on display here.
From only 10.000 viral particles (&lt;span class=&#34;math inline&#34;&gt;\(log_{10}{(10^4)} = 4\)&lt;/span&gt; ) to almost 1 billion (&lt;span class=&#34;math inline&#34;&gt;\(log_{10}{(10^9)} = 9\)&lt;/span&gt; ) particles.&lt;/p&gt;
&lt;p&gt;We can also see that apparently, there were two separate PCR assays (test types), each with a separate conversion formula used to obtain the estimated viral load.&lt;/p&gt;
&lt;p&gt;(&lt;strong&gt;N.b.&lt;/strong&gt; The missings for &lt;code&gt;pcr_assay_type&lt;/code&gt; are because for two of three datasets, it was difficult to extract this information from the PDF file. From the plot, we can conclude that for these datasets, the same two assays were used since the values map onto the same two calibration lines)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sensitivity-of-the-antigen-test&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sensitivity of the Antigen test&lt;/h2&gt;
&lt;p&gt;The dataset contains all samples for which the PCR test was positive.
Let’s start by checking the raw percentage of antigen test measurements that are positive as well.
This is called the &lt;strong&gt;sensitivity&lt;/strong&gt; of a test.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res &amp;lt;- df_pcr_pos %&amp;gt;%
  summarize(sensitivity = mean(mm_value), 
            N = n())

res&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 2
##   sensitivity     N
##         &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;
## 1       0.792   120&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So for all PCR positive samples, 79.2 % is positive as well.
This means that, on average, if we would use the antigen test kit, we have a one in five (20%) probability of not detecting COVID-19, compared to when we would have used the method used by test centers operated by the public health agencies.&lt;/p&gt;
&lt;p&gt;This value is slightly lower, but close to what is mentioned in the Roche kit’s leaflet.&lt;/p&gt;
&lt;p&gt;Let’s postpone evaluation of this fact for a moment and look a bit closer at the data.
For example, we can example the relationship between &lt;code&gt;viral_load&lt;/code&gt; and a positive antigen test result (&lt;code&gt;mm_value&lt;/code&gt; = 1):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(df_pcr_pos$mm_value, df_pcr_pos_np$mm_value)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    
##      0  1
##   0 20  5
##   1  5 90&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)
ggplot(df_pcr_pos, aes(x = viral_load, y = mm_value)) + 
  geom_jitter(height = 0.1) +
  geom_smooth() + 
  geom_vline(xintercept = c(5.7, 7), col = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021_05_14_covid_rapid_test_reliability_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From this plot, we can see that the probability of obtaining a false negative result (&lt;code&gt;mm_value&lt;/code&gt; of 0) on the antigen test decreases as the viral load &lt;strong&gt;of the PCR sample&lt;/strong&gt; increases.&lt;/p&gt;
&lt;p&gt;From the data we also see that before the antigen test to work about half of the time (blue line at 0.5), the PCR sample needs to contain around &lt;span class=&#34;math inline&#34;&gt;\(5 \cdot 10^5\)&lt;/span&gt; viral particles (log10 scale 5.7), and for it to work reliably, we need around &lt;span class=&#34;math inline&#34;&gt;\(10^7\)&lt;/span&gt; particles (“high” viral load) in the PCR sample (which is a combination of &lt;strong&gt;oropharyngeal and nasopharyngeal swab&lt;/strong&gt;). This last bit is important: the researchers did not measure the viral load in the nasal swabs used for the antigen test, these are likely different.&lt;/p&gt;
&lt;p&gt;For really high viral loads, above &lt;span class=&#34;math inline&#34;&gt;\(10^7\)&lt;/span&gt; particles in the &lt;strong&gt;NP/OP sample&lt;/strong&gt;, the probability of a false negative result is only a few percent:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_pcr_pos %&amp;gt;% filter(viral_load &amp;gt;= 7) %&amp;gt;%
  summarize(sensitivity = mean(mm_value), 
            N = n())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 2
##   sensitivity     N
##         &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;
## 1       0.972    71&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;viral-loads-varies-with-days-of-symptoms&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Viral loads varies with days of symptoms&lt;/h1&gt;
&lt;p&gt;Above, we already discussed that the viral load varies with the time since infection.&lt;/p&gt;
&lt;p&gt;If we want to use the antigen test &lt;strong&gt;instead&lt;/strong&gt; of taking a PCR test, we don’t have information on the viral load. What we often do have is the days since symptoms, and we know that in the first few days of symptoms viral load is highest.&lt;/p&gt;
&lt;p&gt;We can check this by plotting the &lt;code&gt;days_of_symptoms&lt;/code&gt; versus &lt;code&gt;viral_load&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(df_pcr_pos, aes(x = days_of_symptoms, y = viral_load)) + 
  geom_smooth() + expand_limits(x = -4) + geom_vline(xintercept = 1, linetype = &amp;quot;dashed&amp;quot;) +
  geom_vline(xintercept = c(3, 7), col = &amp;quot;red&amp;quot;) + geom_hline(yintercept = 7, col = &amp;quot;grey&amp;quot;, linetype = &amp;quot;dashed&amp;quot;) +
  geom_jitter(height = 0, width = 0.2) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021_05_14_covid_rapid_test_reliability_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;
From this plot, we learn that the viral load is highest on the onset of symptoms day (typically 5 days after infection) and decreases afterwards.&lt;/p&gt;
&lt;p&gt;Above, we saw that the sensitivity in the whole sample was not equal to the sensitivity mentioned in the leaflet.
When evaluating rapid antigen tests, sometimes thresholds for days of symptoms are used, for example &amp;lt;= 3 days or &amp;lt;= 7 days (plotted in red).&lt;/p&gt;
&lt;p&gt;For the sensitivity in the leaflet, a threshold of &amp;lt;= 7 days was used on the days of symptoms.&lt;/p&gt;
&lt;p&gt;Let us see how sensitive the antigen test is for these subgroups:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res &amp;lt;- df_pcr_pos %&amp;gt;%
  filter(days_of_symptoms &amp;lt;= 3) %&amp;gt;%
  summarize(label = &amp;quot;&amp;lt; 3 days&amp;quot;,
            sensitivity = mean(mm_value), 
            N = n())

res2 &amp;lt;- df_pcr_pos %&amp;gt;%
  filter(days_of_symptoms &amp;lt;= 7) %&amp;gt;%
  summarize(label = &amp;quot;&amp;lt; 7 days&amp;quot;,
            sensitivity = mean(mm_value), 
            N = n())

bind_rows(res, res2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 3
##   label    sensitivity     N
##   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;
## 1 &amp;lt; 3 days       0.857    49
## 2 &amp;lt; 7 days       0.85    100&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The sensitivity in both subgroups is increased to 85.7 % and 85 %.
Now only 1 in 7 cases is missed by the antigen test.
This sensitivity is now very close to that in the leaflet. The dataset in the leaflet has N = 102, whereas here we have N = 100.
Given that the difference is very small, I decided to not look into this any further.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;is-there-a-swab-effect&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Is there a swab effect?&lt;/h1&gt;
&lt;p&gt;Ok, so the rapid antigen test is less sensitive than PCR.
What about the effect of using self-administered nasal swabs, versus professional health workers taking a nasopharyngeal swab (and often a swab in the back of the throat as well)?&lt;/p&gt;
&lt;p&gt;Interestingly, the three Berlin studies all contain a head-to-head comparison of &lt;strong&gt;nasal&lt;/strong&gt; versus &lt;strong&gt;nasopharygeal (NP)&lt;/strong&gt; swabs. Lets have a look, shall we?&lt;/p&gt;
&lt;p&gt;The dataset &lt;code&gt;df_pcr_pos_np&lt;/code&gt; is identical to &lt;code&gt;df_pcr_pos&lt;/code&gt;, but contains the measurement results for the &lt;strong&gt;nasopharygeal&lt;/strong&gt; swabs.&lt;/p&gt;
&lt;p&gt;To compare both measurement methods, we can plot the relationship between the probability of obtaining a positive result versus viral load. If one method gathers systematically more viral load from the patient, we expect that method to detect infection at lower patient viral loads, and the curves (nasal vs NP) would be shifted relative to each other.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)

ggplot(df_pcr_pos_np , aes(x = viral_load, y = mm_value)) + 
  geom_jitter(data = df_pcr_pos, height = 0.05, col = &amp;quot;blue&amp;quot;) +
  geom_jitter(height = 0.05, col = &amp;quot;orange&amp;quot;) +
  geom_smooth(data = df_pcr_pos , col = &amp;quot;blue&amp;quot;) + 
  geom_smooth(col = &amp;quot;orange&amp;quot;) +
  ggtitle(&amp;quot;nasal (blue) versus nasopharyngeal (orange) swabs&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021_05_14_covid_rapid_test_reliability_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;By fitting a smoother through the binary data, we obtain an estimate of the relationship between the probability of obtaining a positive result, and the viral load of the patient as measured by PCR on a combined NP/OP swab.&lt;/p&gt;
&lt;p&gt;From this plot, I conclude that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The sensitivity of a test is strongly dependent on the distribution of viral loads in the population the measurement was conducted in&lt;/li&gt;
&lt;li&gt;There is no evidence for any differences in sensitivity between nasal and nasopharyngeal swabs&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This last conclusion came as a surprise for me, as nasopharygeal swabs are long considered to be the golden standard for obtaining samples for PCR detection of respiratory viruses, such as influenza and COVID-19 (Seaman &lt;em&gt;et al.&lt;/em&gt; (2019), (Lee &lt;em&gt;et al.&lt;/em&gt; 2021) ). So let’s look a bit deeper still.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;double-check-rotterdam-vs-berlin&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Double-check: Rotterdam vs Berlin&lt;/h1&gt;
&lt;p&gt;We can compare the results from the three Berlin studies with a recent Dutch study that also used the Roche antigen test (ref: Igloi &lt;em&gt;et al.&lt;/em&gt; 2021). The study was conducted in Rotterdam, and used nasopharygeal swabs to obtain the sample for the antigen test.&lt;/p&gt;
&lt;p&gt;Cool! Lets try and create two comparable groups in both studies so we can compare the sensitivity.&lt;/p&gt;
&lt;p&gt;The Igloi et al. paper reports results for a particular subset that we can also create in the Berlin dataset.
They report that for the subset of samples with high viral load (viral load &lt;span class=&#34;math inline&#34;&gt;\(2.17 \cdot 10^5\)&lt;/span&gt; particles / ml = 5.35 on the log10 scale, ct_value &amp;lt;= 30) &lt;strong&gt;AND&lt;/strong&gt; who presented within 7 days of symptom onset, they found a sensitivity of 95.8% (CI95% 90.5-98.2). The percentage is based on N = 159 persons (or slightly less because of not subsetting on &amp;lt;= 7 days of symptoms, the paper is not very clear here).&lt;/p&gt;
&lt;p&gt;We can check what the sensitivity is for this subgroup in the Berlin dataset:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_pcr_pos %&amp;gt;% filter(viral_load &amp;gt;= 5.35 &amp;amp; days_of_symptoms &amp;lt;= 7) %&amp;gt;%
  summarize(sensitivity = mean(mm_value), 
            N = n())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 2
##   sensitivity     N
##         &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;
## 1       0.898    88&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the same subgroup of high viral load, sensitivity of the nasal swab test is 6% lower than the nasopharyngeal swab test, across the two studies. But how do we have to weigh this evidence? N = 88 is not so much data, and the studies are not identical in design.&lt;/p&gt;
&lt;p&gt;Importantly, since the threshold to be included in this comparison (ct value &amp;lt;= 30, viral_load &amp;gt; 5.35) contains a large part of the region where the probability of a positive result is between 0 and 1, we need to compare the distributions of viral loads for both studies to make an apples to apples comparison.&lt;/p&gt;
&lt;p&gt;The Igloi study reports their distribution of viral loads for PCR-positive samples (N=186) in five bins (Table 1 in their paper):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat &amp;lt;- c(&amp;quot;ct &amp;lt;= 20&amp;quot;, &amp;quot;ct 20-25&amp;quot;, &amp;quot;ct 25-30&amp;quot;, &amp;quot;ct 30-35&amp;quot;, &amp;quot;ct 35+&amp;quot;)
counts &amp;lt;- c(31, 82, 46, 27, 1)

ggplot(data.frame(cat, counts), aes(x = cat, y = counts)) +
  geom_col()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021_05_14_covid_rapid_test_reliability_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;
Lets create those same bins in the Berlin dataset as well:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_pcr_pos$ct_bin &amp;lt;- cut(df_pcr_pos$ct_value, breaks = c(-Inf,20,25,30,35,Inf))

ggplot(df_pcr_pos, aes(x = ct_bin)) +
  geom_histogram(stat = &amp;quot;count&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021_05_14_covid_rapid_test_reliability_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;
For the subset where we can compare the sensitivities (ct_value &amp;lt;= 30), the Berlin clinical population has a higher viral load than the Rotterdam clinical population! So that does not explain why the Rotterdam study reports a higher sensitivity.&lt;/p&gt;
&lt;p&gt;I use simulation to create distributions of plausible values for the sensitivity, assuming the observed values in both studies (89.7% for the Berlin studies, and 95.8% for the Rotterdam study) to be the true data generating values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)

# Berlin
sample_size = 88
prior_probability = 0.898

est_p &amp;lt;- rbinom(10000, sample_size, p=prior_probability)/sample_size

# Rotterdam
sample_size2 = 159 # derived from Table 1 (Ct value distribution of PCR+ samples, &amp;lt;= 30)
prior_probability2 = 0.958

est_p2 &amp;lt;- rbinom(10000, sample_size2, p=prior_probability2)/sample_size2

ggplot(data.frame(est_p), aes(x = est_p)) +
  geom_histogram(binwidth = 0.005) +
    geom_histogram(data = data.frame(est_p = est_p2), fill = &amp;quot;gray60&amp;quot;, alpha = 0.5, binwidth = 0.005) +
  geom_vline(xintercept = prior_probability, linetype = &amp;quot;dashed&amp;quot;, col= &amp;quot;red&amp;quot;) +
geom_vline(xintercept = prior_probability2, linetype = &amp;quot;dashed&amp;quot;, col= &amp;quot;blue&amp;quot;) +
  ggtitle(&amp;quot;Berlin (black bars) vs Rotterdam (grey bars) sensitivity for higher viral loads&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021_05_14_covid_rapid_test_reliability_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;
There is a region of overlap between the two distributions, so the difference between the studies could be (in part) attributed to statistical sampling variation for the same underlying process.&lt;/p&gt;
&lt;p&gt;I conclude that the Berlin study, who does a head to head comparison of NP versus nasal swabs, finds them to be highly comparable, and reports sensitivities that are close to those reported by the Rotterdam study.&lt;/p&gt;
&lt;p&gt;Surprisingly, nasal swabs appear to give results that are comparable to those of nasopharyngeal swabs, while having not having the disadvantages of them (unpleasant, can only be performed by professional health worker).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;that-other-metric-the-specificity&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;That other metric: the specificity&lt;/h1&gt;
&lt;p&gt;So far, the discussion centered around the &lt;strong&gt;sensitivity&lt;/strong&gt; of the test.
Equally important is the &lt;strong&gt;specificity&lt;/strong&gt; of the test. This quantifies if the test result of the antigen test is specific for COVID-19. It would be bad if the test would also show a result for other viruses, or even unrelated molecules.&lt;/p&gt;
&lt;p&gt;To examine this, we use the aggregated data supplied on the leaflet from the kit, &lt;code&gt;df_leaflet&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;N.b.&lt;/strong&gt; The aggregated data is a subset of all the data from the three studies, because the data was subsetted for cases with &lt;code&gt;&amp;lt;= 7 days_of_symptoms&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This dataset contains for each sample one of four possibilities:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Both tests are negative,&lt;/li&gt;
&lt;li&gt;both tests are positive,&lt;/li&gt;
&lt;li&gt;the PCR test is positive but the antigen test negative,&lt;/li&gt;
&lt;li&gt;the PCR test is negative but the antigen positive.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We use the &lt;code&gt;yardstick&lt;/code&gt; package of R’s &lt;code&gt;tidymodels&lt;/code&gt; family to create the 2x2 table and analyze the specificity.&lt;/p&gt;
&lt;p&gt;(&lt;strong&gt;Overthinking&lt;/strong&gt;: Note that the &lt;code&gt;yardstick&lt;/code&gt; package is used to quantify the performance of statistical prediction models by comparing the model predictions to the true values contained in the training data. This provides us with an analogy where the antigen test can be viewed as a model that is trying the predict the outcome of the PCR test.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;options(yardstick.event_first = FALSE)

conf_matrix &amp;lt;- yardstick::conf_mat(df_leaflet, pcr_result, ag_result)

autoplot(conf_matrix, 
         type = &amp;quot;heatmap&amp;quot;, 
         title = &amp;quot;Truth = PCR test, Prediction = Antigen test&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021_05_14_covid_rapid_test_reliability_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;288&#34; /&gt;
From the heatmap (confusingly called a confusion matrix among ML practioners), we see that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For most samples (N = 431), both tests are COVID-19 negative.&lt;/li&gt;
&lt;li&gt;85 + 17 = 102 samples tested COVID-19 positive using the PCR-test&lt;/li&gt;
&lt;li&gt;85 out of 102 samples that are PCR positive, are antigen test positive as well&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For the specificity, we have to look at the samples where the PCR test is negative, but the antigen test is positive, and compare these to all the samples that are PCR-test negative. These are the number of tests where the antigen test picked up a non-specific signal. One minus this percentage gives the specificity (1 - 4/435 = 431/435):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;yardstick::spec(df_leaflet, pcr_result, ag_result)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
## 1 spec    binary         0.991&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thus, we find that the antigen test is highly specific, with around 1% of false positives.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;uncertainty-in-the-estimated-specificity-and-sensitivity&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Uncertainty in the estimated specificity and sensitivity&lt;/h1&gt;
&lt;p&gt;So far, we did not discuss the sampling variability in the estimated specificity and sensitivity.&lt;/p&gt;
&lt;p&gt;The kit leaflet mentions the following confidence intervals:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sensitivity 83.3% (95%CI: 74.7% - 90.0%)&lt;/li&gt;
&lt;li&gt;Specificity 99.1% (95%CI: 97.7% - 99.7%)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The R-package &lt;code&gt;yardstick&lt;/code&gt; does not yet include confidence intervals, so I generated these using bootstrapping. I calculate both metrics for 10.000 samples sampled from the raw data. For brevity I omitted the code here, go check out my &lt;a href=&#34;https://github.com/gsverhoeven/hugo_source/tree/master/content/post/sars_test&#34;&gt;Github&lt;/a&gt; for the R script.&lt;/p&gt;
&lt;p&gt;The bootstrapping approach yields the following range of plausible values given the data (95% interval):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quantile(spec_vec, probs = c(0.025, 0.975))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      2.5%     97.5% 
## 0.9811765 0.9977477&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quantile(sens_vec, probs = c(0.025, 0.975))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      2.5%     97.5% 
## 0.7570030 0.9029126&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The amount of data (N = 537) prevents us from getting an exact match to the leaflet’s confidence intervals, that are based on theoretic formulas. But we do get pretty close.&lt;/p&gt;
&lt;p&gt;Especially for the sensitivity, there is quite some uncertainty, we see that plausible values range from 76% up to 90% &lt;em&gt;for this particular cohort of cases with this particular mix of viral loads that showed up during the last four months of 2020 in the University hospital in Berlin&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusions&lt;/h1&gt;
&lt;p&gt;To summarise: we found that the numbers of the kit’s leaflet are reliable, reproducible, and published in full detail in the scientific literature.
Hurray!&lt;/p&gt;
&lt;p&gt;We also found that even the gold standard PCR is not able to detect all infected persons, it all depends on how much virus is present, and how the sample is obtained.&lt;/p&gt;
&lt;p&gt;But all in all, the PCR test is clearly more accurate. Why would we want to use an antigen test then?
To do the PCR test you need a lab with skilled people, equipment such as PCR devices and pipets, and time, as the process takes at least a few hours to complete. The advantage of an antigen test is to have a low-tech, faster alternative that can be self-administered. But that comes at a cost, because the antigen tests are less sensitive.&lt;/p&gt;
&lt;p&gt;From the analysis, it is clear that the rapid Antigen tests need more virus present to reliably detect infection. It is ALSO clear that the test is highly specific, with less than 1% false positives. Note that a false positive rate of 1% still means that in a healthy population of 1000, 10 are falsely detected as having COVID-19.&lt;/p&gt;
&lt;p&gt;Surprisingly, nasal swabs appear to give results that are comparable to those of nasopharyngeal swabs, while having not having the disadvantages of them (unpleasant, can only be performed by professional health worker).&lt;/p&gt;
&lt;p&gt;So the antigen tests are less sensitive than PCR tests. But now comes the key insight: the persons that produce the largest amounts of virus get detected, irrespective of whether they have symptoms or not. To me, this seems like a “Unique Selling Point” of the rapid tests: the ability to rapidly detect the most contagious persons in a group, after which these persons can go into quarantine and help reduce spread.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Thanks to dr. Andreas Lindner for providing helpful feedback and pointing out flaws in my original blog post. This should not be seen as an endorsement of the conclusions of this post, and any remaining mistakes are all my own!&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Head-to-head comparison of SARS-CoV-2 antigen-detecting rapid test with self-collected nasal swab versus professional-collected nasopharyngeal swab&lt;/em&gt;:
Andreas K. Lindner, Olga Nikolai, Franka Kausch, Mia Wintel, Franziska Hommes, Maximilian Gertler, Lisa J. Krüger, Mary Gaeddert, Frank Tobian, Federica Lainati, Lisa Köppel, Joachim Seybold, Victor M. Corman, Christian Drosten, Jörg Hofmann, Jilian A. Sacks, Frank P. Mockenhaupt, Claudia M. Denkinger, &lt;a href=&#34;https://erj.ersjournals.com/content/57/4/2003961&#34;&gt;European Respiratory Journal 2021 57: 2003961&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Head-to-head comparison of SARS-CoV-2 antigen-detecting rapid test with professional-collected nasal versus nasopharyngeal swab&lt;/em&gt;:
Andreas K. Lindner, Olga Nikolai, Chiara Rohardt, Susen Burock, Claudia Hülso, Alisa Bölke, Maximilian Gertler, Lisa J. Krüger, Mary Gaeddert, Frank Tobian, Federica Lainati, Joachim Seybold, Terry C. Jones, Jörg Hofmann, Jilian A. Sacks, Frank P. Mockenhaupt, Claudia M. Denkinger &lt;a href=&#34;https://erj.ersjournals.com/content/57/5/2004430&#34;&gt;European Respiratory Journal 2021 57: 2004430&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;SARS-CoV-2 patient self-testing with an antigen-detecting rapid test: a head-to-head comparison with professional testing&lt;/em&gt;:
Andreas K. Lindner, Olga Nikolai, Chiara Rohardt, Franka Kausch, Mia Wintel, Maximilian Gertler, Susen Burock, Merle Hörig, Julian Bernhard, Frank Tobian, Mary Gaeddert, Federica Lainati, Victor M. Corman, Terry C. Jones, Jilian A. Sacks, Joachim Seybold, Claudia M. Denkinger, Frank P. Mockenhaupt, under review, &lt;a href=&#34;https://www.medrxiv.org/content/10.1101/2021.01.06.20249009v1&#34;&gt;preprint on medrxiv.org&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Variation in False-Negative Rate of Reverse Transcriptase Polymerase Chain Reaction–Based SARS-CoV-2 Tests by Time Since Exposure&lt;/em&gt;: Lauren M. Kucirka, Stephen A. Lauer, Oliver Laeyendecker, Denali Boon, Justin Lessler &lt;a href=&#34;https://www.acpjournals.org/doi/full/10.7326/M20-1495&#34;&gt;Link to paper&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Clinical evaluation of the Roche/SD Biosensor rapid antigen test with symptomatic, non-hospitalized patients in a municipal health service drive-through testing site&lt;/em&gt;:
Zsὁfia Iglὁi, Jans Velzing, Janko van Beek, David van de Vijver, Georgina Aron, Roel Ensing, KimberleyBenschop, Wanda Han, Timo Boelsums, Marion Koopmans, Corine Geurtsvankessel, Richard Molenkamp &lt;a href=&#34;https://www.medrxiv.org/content/10.1101/2020.11.18.20234104v1&#34;&gt;Link on medrxiv.org&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Performance of Saliva, Oropharyngeal Swabs, and Nasal Swabs for SARS-CoV-2 Molecular Detection: a Systematic Review and Meta-analysis&lt;/em&gt;:
Rose A. Lee, Joshua C. Herigona, Andrea Benedetti, Nira R. Pollock, Claudia M. Denkinger &lt;a href=&#34;https://journals.asm.org/doi/10.1128/JCM.02881-20&#34;&gt;Link to paper&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Self-collected compared with professional-collected swabbing in the diagnosis of influenza in symptomatic individuals: A meta-analysis and assessment of validity&lt;/em&gt;:
Seaman et al 2019 &lt;a href=&#34;https://pubmed.ncbi.nlm.nih.gov/31400670/&#34;&gt;Link to paper&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Using posterior predictive distributions to get the Average Treatment Effect (ATE) with uncertainty</title>
      <link>/post/posterior-distribution-average-treatment-effect/</link>
      <pubDate>Fri, 04 Sep 2020 00:00:00 +0000</pubDate>
      <guid>/post/posterior-distribution-average-treatment-effect/</guid>
      <description>
&lt;link href=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;gertjan-verhoeven-misja-mikkers&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Gertjan Verhoeven &amp;amp; Misja Mikkers&lt;/h2&gt;
&lt;p&gt;Here we show how to use &lt;a href=&#34;https://mc-stan.org&#34;&gt;Stan&lt;/a&gt; with the &lt;a href=&#34;https://github.com/paul-buerkner/brms&#34;&gt;brms&lt;/a&gt; R-package to calculate the posterior predictive distribution of a covariate-adjusted average treatment effect. We fit a model on simulated data that mimics a (very clean) experiment with random treatment assignment.&lt;/p&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;Suppose we have data from a Randomized Controlled Trial (RCT) and we want to estimate the average treatment effect (ATE). Patients get treated, or not, depending only on a coin flip. This is encoded in the &lt;code&gt;Treatment&lt;/code&gt; variable. The outcome is a count variable &lt;code&gt;Admissions&lt;/code&gt;, representing the number of times the patient gets admitted to the hospital. The treatment is expected to reduce the number of hospital admissions for patients.&lt;/p&gt;
&lt;p&gt;To complicate matters (a bit): As is often the case with patients, not all patients are identical. Suppose that older patients have on average more Admissions. So &lt;code&gt;Age&lt;/code&gt; is a covariate.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;average-treatment-effect-ate&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Average treatment effect (ATE)&lt;/h3&gt;
&lt;p&gt;Now, after we fitted a model to the data, we want to actually &lt;strong&gt;use&lt;/strong&gt; our model to answer &amp;quot;What-if&amp;quot; questions (counterfactuals). Here we answer the following question:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What would the average reduction in Admissions be if we had treated &lt;strong&gt;ALL&lt;/strong&gt; the patients in the sample, compared to a situation where &lt;strong&gt;NO&lt;/strong&gt; patient in the sample would have received treatment?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Well, that is easy, we just take the fitted model, change treatment from zero to one for each, and observe the (&amp;quot;marginal&amp;quot;) effect on the outcome, right?&lt;/p&gt;
&lt;p&gt;Yes, but the uncertainty is harder. We have uncertainty in the estimated coefficients of the intercept and covariate, as well as in the coefficient of the treatment variable. And these uncertainties can be correlated (for example between the coefficients of intercept and covariate).&lt;/p&gt;
&lt;p&gt;Here we show how to use &lt;code&gt;posterior_predict()&lt;/code&gt; to simulate outcomes of the model using the sampled parameters. If we do this for two counterfactuals, all patients treated, and all patients untreated, and subtract these, we can easily calculate the posterior predictive distribution of the average treatment effect.&lt;/p&gt;
&lt;p&gt;Let&#39;s do it!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;load-packages&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Load packages&lt;/h3&gt;
&lt;p&gt;This tutorial uses &lt;a href=&#34;https://github.com/paul-buerkner/brms&#34;&gt;brms&lt;/a&gt;, a user friendly interface to full Bayesian modelling with &lt;a href=&#34;https://mc-stan.org/&#34;&gt;Stan&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(rstan)
library(brms) &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data-simulation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data simulation&lt;/h3&gt;
&lt;p&gt;We generate fake data that matches our problem setup.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Admissions&lt;/code&gt; are determined by patient &lt;code&gt;Age&lt;/code&gt;, whether the patient has &lt;code&gt;Treatment&lt;/code&gt;, and some random &lt;code&gt;Noise&lt;/code&gt; to capture unobserved effects that influence &lt;code&gt;Admissions&lt;/code&gt;. We exponentiate them to always get a positive number, and plug it in the Poisson distribution using &lt;code&gt;rpois()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123) 

id &amp;lt;- 1:200   
n_obs &amp;lt;- length(id)
b_tr &amp;lt;- -0.7
b_age &amp;lt;- 0.1

df_sim &amp;lt;- as.data.frame(id) %&amp;gt;% 
mutate(Age = rgamma(n_obs, shape = 5, scale = 2)) %&amp;gt;% # positive cont predictor
mutate(Noise = rnorm(n_obs, mean = 0, sd = 0.5)) %&amp;gt;% # add noise
mutate(Treatment = ifelse(runif(n_obs) &amp;lt; 0.5, 0, 1)) %&amp;gt;% # Flip a coin for treatment
mutate(Lambda = exp(b_age * Age + b_tr * Treatment + Noise)) %&amp;gt;% # generate lambda for the poisson dist
mutate(Admissions = rpois(n_obs, lambda = Lambda))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;summarize-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Summarize data&lt;/h3&gt;
&lt;p&gt;Ok, so what does our dataset look like?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(df_sim)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        id              Age             Noise            Treatment    
##  Min.   :  1.00   Min.   : 1.794   Min.   :-1.32157   Min.   :0.000  
##  1st Qu.: 50.75   1st Qu.: 6.724   1st Qu.:-0.28614   1st Qu.:0.000  
##  Median :100.50   Median : 8.791   Median : 0.04713   Median :0.000  
##  Mean   :100.50   Mean   : 9.474   Mean   : 0.02427   Mean   :0.495  
##  3rd Qu.:150.25   3rd Qu.:11.713   3rd Qu.: 0.36025   3rd Qu.:1.000  
##  Max.   :200.00   Max.   :24.835   Max.   : 1.28573   Max.   :1.000  
##      Lambda          Admissions    
##  Min.   : 0.2479   Min.   : 0.000  
##  1st Qu.: 1.1431   1st Qu.: 1.000  
##  Median : 1.8104   Median : 2.000  
##  Mean   : 2.6528   Mean   : 2.485  
##  3rd Qu.: 3.0960   3rd Qu.: 3.000  
##  Max.   :37.1296   Max.   :38.000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Treatment variable should reduce admissions. Lets visualize the distribution of Admission values for both treated and untreated patients.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = df_sim, aes(x = Admissions)) +
  geom_histogram(stat=&amp;quot;count&amp;quot;) +
  facet_wrap(~ Treatment) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-09-04-brms_posterior_pred_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The effect of the treatment on reducing admissions is clearly visible.&lt;/p&gt;
&lt;p&gt;We can also visualize the relationship between &lt;code&gt;Admissions&lt;/code&gt; and &lt;code&gt;Age&lt;/code&gt;, for both treated and untreated patients. We use the &lt;code&gt;viridis&lt;/code&gt; scales to provide colour maps that are designed to be perceived by viewers with common forms of colour blindness.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = df_sim, aes(x = Age, y = Admissions, color = as.factor(Treatment))) +
  geom_point() +
  scale_color_viridis_d(labels = c(&amp;quot;No Treatment&amp;quot;, &amp;quot;Treatment&amp;quot;)) +
  labs(color = &amp;quot;Treatment&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-09-04-brms_posterior_pred_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now lets fit our Bayesian Poisson regression model to it.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fit model&lt;/h3&gt;
&lt;p&gt;We use &lt;code&gt;brms&lt;/code&gt; default priors for convenience here. For a real application we would of course put effort into into crafting priors that reflect our current knowledge of the problem at hand.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model1 &amp;lt;- brm(
  formula = as.integer(Admissions) ~  Age + Treatment,
   data = df_sim,
  family = poisson(),
  warmup = 2000, iter = 5000, 
  cores = 2, 
  chains = 4,
  seed = 123,
  silent = TRUE,
  refresh = 0,
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Compiling Stan program...&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Start sampling&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;check-model-fit&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Check model fit&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(model1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: poisson 
##   Links: mu = log 
## Formula: as.integer(Admissions) ~ Age + Treatment 
##    Data: df_sim (Number of observations: 200) 
## Samples: 4 chains, each with iter = 5000; warmup = 2000; thin = 1;
##          total post-warmup samples = 12000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept    -0.05      0.12    -0.28     0.18 1.00     7410     7333
## Age           0.12      0.01     0.10     0.14 1.00     8052     8226
## Treatment    -0.83      0.10    -1.02    -0.63 1.00     7794     7606
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the posterior dists for &lt;span class=&#34;math inline&#34;&gt;\(\beta_{Age}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_{Treatment}\)&lt;/span&gt; cover the true values, so looking good. To get a fuller glimpse into the (correlated) uncertainty of the model parameters we make a pairs plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pairs(model1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-09-04-brms_posterior_pred_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As expected, the coefficients &lt;span class=&#34;math inline&#34;&gt;\(\beta_{Intercept}\)&lt;/span&gt; (added by &lt;code&gt;brms&lt;/code&gt;) and &lt;span class=&#34;math inline&#34;&gt;\(\beta_{Age}\)&lt;/span&gt; are highly correlated.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;first-attempt-calculate-individual-treatment-effects-using-the-model-fit-object&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;First attempt: Calculate Individual Treatment effects using the model fit object&lt;/h3&gt;
&lt;p&gt;Conceptually, the simplest approach for prediction is to take the most likely values for all the model parameters, and use these to calculate for each patient an individual treatment effect. This is what plain OLS regression does when we call &lt;code&gt;predict.lm()&lt;/code&gt; on a fitted model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;est_intercept &amp;lt;- fixef(model1, pars = &amp;quot;Intercept&amp;quot;)[,1]
est_age_eff &amp;lt;- fixef(model1, pars = &amp;quot;Age&amp;quot;)[,1]
est_t &amp;lt;- fixef(model1, pars = &amp;quot;Treatment&amp;quot;)[,1]

# brm fit parameters (intercept plus treatment)
ites &amp;lt;- exp(est_intercept + (est_age_eff * df_sim$Age) +  est_t) - exp(est_intercept + (est_age_eff * df_sim$Age))

ggplot(data.frame(ites), aes(x = ites)) + 
  geom_histogram() +
  geom_vline(xintercept = mean(ites), col = &amp;quot;red&amp;quot;) +
  ggtitle(&amp;quot;Effect of treatment on Admissions for each observation&amp;quot;) +
   expand_limits(x = 0) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-09-04-brms_posterior_pred_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Averaging the ITEs gives us the ATE, displayed in red.&lt;/p&gt;
&lt;p&gt;Ok, so &lt;strong&gt;on average&lt;/strong&gt;, our treatment reduces the number of Admissions by -1.9.&lt;/p&gt;
&lt;p&gt;You may wonder: why do we even have a distribution of treatment effects here? Should it not be the same for each patient? Here a peculiarity of the Poisson regression model comes to surface: The effect of changing &lt;code&gt;Treatment&lt;/code&gt; from 0 to 1 on the outcome depends on the value of &lt;code&gt;Age&lt;/code&gt; of the patient. This is because we &lt;strong&gt;exponentiate&lt;/strong&gt; the linear model before we plug it into the Poisson distribution.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;next-the-uncertainty-in-the-ate&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Next, the uncertainty in the ATE&lt;/h3&gt;
&lt;p&gt;How to get all this underlying, correlated uncertainty in the model parameters, that have varying effects depending on the covariates of patients, and properly propagate that to the ATE? What is the range of plausible values of the ATE consistent with the data &amp;amp; model?&lt;/p&gt;
&lt;p&gt;At this point, using only the summary statistics of the model fit (i.e. the coefficients), we hit a wall. To make progress we have to work with the full posterior distribution of model parameters, and use this to make predictions. That is why it is often called &amp;quot;the posterior predictive distribution&amp;quot; (Check &lt;a href=&#34;http://www.stat.columbia.edu/~gelman/book/BDA3.pdf&#34;&gt;BDA3&lt;/a&gt; for the full story).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;posterior-predictive-distribution-ppd-two-tricks&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Posterior predictive distribution (PPD): two tricks&lt;/h3&gt;
&lt;p&gt;Ok, you say, a Posterior Predictive Distribution, let&#39;s have it! Where can I get one?&lt;/p&gt;
&lt;p&gt;Luckily for us, most of the work is already done, because we have fitted our model. And thus we have a large collection of parameter draws (or samples, to confuse things a bit). All the correlated uncertainty is contained in these draws.&lt;/p&gt;
&lt;p&gt;This is the first trick. Conceptually, we imagine that each separate draw of the posterior represents a particular version of our model.&lt;/p&gt;
&lt;p&gt;In our example model fit, we have 12.000 samples from the posterior. In our imagination, we now have 12.000 versions of our model, where unlikely parameter combinations are present less often compared to likely parameter combinations. The full uncertainty of our model parameters is contained in this &amp;quot;collection of models&amp;quot; .&lt;/p&gt;
&lt;p&gt;The second trick is that we simulate (generate) predictions for all observations, from each of these 12.000 models. Under the hood, this means computing for each model (we have 12.000), for each observation (we have 200) the predicted lambda value given the covariates, and drawing a single value from a Poisson distribution with that &lt;span class=&#34;math inline&#34;&gt;\(\Lambda\)&lt;/span&gt; value (e.g. running &lt;code&gt;rpois(n = 1, lambda)&lt;/code&gt; ).&lt;/p&gt;
&lt;p&gt;This gives us a 12.000 x 200 matrix, that we can compute with.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;computing-with-the-ppd-brmsposterior_predict&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Computing with the PPD: brms::posterior_predict()&lt;/h3&gt;
&lt;p&gt;To compute PPD&#39;s, we can use &lt;code&gt;brms::posterior_predict()&lt;/code&gt;. We can feed it any dataset using the &lt;code&gt;newdata&lt;/code&gt; argument, and have it generate a PPD.&lt;/p&gt;
&lt;p&gt;For our application, the computation can be broken down in two steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Step 1: use &lt;code&gt;posterior_predict()&lt;/code&gt; on our dataset with &lt;code&gt;Treatment&lt;/code&gt; set to zero, do the same for our dataset with &lt;code&gt;Treatment&lt;/code&gt; set to one, and subtract the two matrices. This gives us a matrix of outcome differences / treatment effects.&lt;/li&gt;
&lt;li&gt;Step 2: Averaging over all cols (the N=200 simulated outcomes for each draw) should give us the distribution of the ATE. This distribution now represents the variability (uncertainty) of the estimate.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ok, step 1:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# create two versions of our dataset, with all Tr= 0 and all Tr=1
df_sim_t0 &amp;lt;- df_sim %&amp;gt;% mutate(Treatment = 0)

df_sim_t1 &amp;lt;- df_sim %&amp;gt;% mutate(Treatment = 1)

# simulate the PPDs
pp_t0 &amp;lt;- posterior_predict(model1, newdata = df_sim_t0)

pp_t1 &amp;lt;- posterior_predict(model1, newdata = df_sim_t1)

diff &amp;lt;- pp_t1 - pp_t0

dim(diff)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 12000   200&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And step 2 (averaging by row over the cols):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ATE_per_draw &amp;lt;- apply(diff, 1, mean)

# equivalent expression for tidyverse fans
#ATE_per_draw &amp;lt;- data.frame(diff) %&amp;gt;% rowwise() %&amp;gt;% summarise(avg = mean(c_across(cols = everything())))

length(ATE_per_draw)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 12000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, a distribution of plausible ATE values. Oo, that is so nice. Lets visualize it!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data.frame(ATE_per_draw), aes(x = ATE_per_draw)) +
  geom_histogram() + 
  geom_vline(xintercept = mean(ites), col = &amp;quot;red&amp;quot;) +
  ggtitle(&amp;quot;Posterior distribution of the Average Treatment Effect (ATE)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-09-04-brms_posterior_pred_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can compare this distribution with the point estimate of the ATE we obtained above using the model coefficients. It sits right in the middle (red line), just as it should be!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;demonstrating-the-versatility-uncertainty-in-the-sum-of-treatment-effects&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Demonstrating the versatility: uncertainty in the sum of treatment effects&lt;/h3&gt;
&lt;p&gt;Now suppose we are a policy maker, and we want to estimate the total reduction in Admissions if all patients get the treatment. And we want to quantify the range of plausible values of this summary statistic.&lt;/p&gt;
&lt;p&gt;To do so, we can easily adjust our code to summing instead of averaging all the treatment effects within each draw (i.e. by row):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;TTE_per_draw &amp;lt;- apply(diff, 1, sum)

ggplot(data.frame(TTE_per_draw), aes(x = TTE_per_draw)) +
  geom_histogram() + 
  geom_vline(xintercept = sum(ites), col = &amp;quot;red&amp;quot;) +
  ggtitle(&amp;quot;Posterior distribution of the Total Treatment Effect (TTE)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-09-04-brms_posterior_pred_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So our model predicts for the aggregate reduction of patient Admissions a value in the range of -500 to -250.&lt;/p&gt;
&lt;p&gt;This distribution can then be used to answer questions such as &amp;quot;what is the probability that our treatment reduces Admissions by at least 400&amp;quot;?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;TTE &amp;lt;- data.frame(TTE_per_draw) %&amp;gt;%
  mutate(counter = ifelse(TTE_per_draw &amp;lt; -400, 1, 0)) 

mean(TTE$counter) * 100&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 38.1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;take-home-message-ppd-with-brms-is-easy-and-powerful&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Take home message: PPD with brms is easy and powerful&lt;/h3&gt;
&lt;p&gt;We hope to have demonstrated that when doing a full bayesian analysis with &lt;code&gt;brms&lt;/code&gt; and &lt;code&gt;Stan&lt;/code&gt;, it is very easy to create Posterior Predictive Distributions using &lt;code&gt;posterior_predict()&lt;/code&gt;. And that if we &lt;em&gt;have&lt;/em&gt; a posterior predictive distribution, incorporating uncertainty in various &amp;quot;marginal effects&amp;quot; type analyses becomes dead-easy. These analyses include what-if scenarios using the original data, or scenarios using new data with different covariate distributions (for example if we have an RCT that is enriched in young students, and we want to apply it to the general population). Ok, that it is for today, happy modelling!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
