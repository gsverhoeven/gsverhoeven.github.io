[
	{
		"id": "kuhn_johnson13",
		"type": "book",
		"publisher": "Springer",
		"source": "Google Scholar",
		"title": "Applied predictive modeling",
		"volume": "26",
		"author": [
			{
				"family": "Kuhn",
				"given": "Max"
			},
			{
				"family": "Johnson",
				"given": "Kjell"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2013"
				]
			]
		},
		"citation-key": "kuhn_johnson13"
	},
	{
		"id": "strobl_etal07",
		"type": "article-journal",
		"abstract": "Variable importance measures for random forests have been receiving increased attention as a means of variable selection in many classification tasks in bioinformatics and related scientific fields, for instance to select a subset of genetic markers relevant for the prediction of a certain disease. We show that random forest variable importance measures are a sensible means for variable selection in many applications, but are not reliable in situations where potential predictor variables vary in their scale of measurement or their number of categories. This is particularly important in genomics and computational biology, where predictors often include variables of different types, for example when predictors include both sequence data and continuous variables such as folding energy, or when amino acid sequence data show different numbers of categories.",
		"container-title": "BMC Bioinformatics",
		"DOI": "10.1186/1471-2105-8-25",
		"ISSN": "1471-2105",
		"issue": "1",
		"journalAbbreviation": "BMC Bioinformatics",
		"page": "25",
		"source": "BioMed Central",
		"title": "Bias in random forest variable importance measures: Illustrations, sources and a solution",
		"title-short": "Bias in random forest variable importance measures",
		"URL": "https://doi.org/10.1186/1471-2105-8-25",
		"volume": "8",
		"author": [
			{
				"family": "Strobl",
				"given": "Carolin"
			},
			{
				"family": "Boulesteix",
				"given": "Anne-Laure"
			},
			{
				"family": "Zeileis",
				"given": "Achim"
			},
			{
				"family": "Hothorn",
				"given": "Torsten"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					5,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2007",
					1,
					25
				]
			]
		},
		"citation-key": "strobl_etal07"
	},
	{
		"id": "segalmarkr_04",
		"type": "article-journal",
		"abstract": "Breiman (2001a,b) has recently developed an ensemble classification and regression approach\nthat displayed outstanding performance with regard prediction error on a suite of benchmark\ndatasets. As the base constituents of the ensemble are tree-structured predictors, and since\neach of these is constructed using an injection of randomness, the method is called ‘random\nforests’. That the exceptional performance is attained with seemingly only a single tuning pa-\nrameter, to which sensitivity is minimal, makes the methodology all the more remarkable. The\nindividual trees comprising the forest are all grown to maximal depth. While this helps with\nregard bias, there is the familiar tradeoff with variance. However, these variability concerns\nwere potentially obscured because of an interesting feature of those benchmarking datasets\nextracted from the UCI machine learning repository for testing: all these datasets are hard to\noverfit using tree-structured methods. This raises issues about the scope of the repository.\nWith this as motivation, and coupled with experience from boosting methods, we revisit the\nformulation of random forests and investigate prediction performance on real-world and simu-\nlated datasets for which maximally sized trees do overfit. These explorations reveal that gains\ncan be realized by additional tuning to regulate tree size via limiting the number of splits\nand/or the size of nodes for which splitting is allowed. Nonetheless, even in these settings,\ngood performance for random forests can be attained by using larger (than default) primary\ntuning parameter values",
		"title": "Machine Learning Benchmarks and Random Forest Regression",
		"author": [
			{
				"family": "Segal, Mark R.",
				"given": ""
			}
		],
		"issued": {
			"date-parts": [
				[
					"2004"
				]
			]
		},
		"citation-key": "segalmarkr_04"
	},
	{
		"id": "svetnik_etal04",
		"type": "article-journal",
		"container-title": "Proceedings of the 7th Course on Ensemble Methods for Learning Machines",
		"note": "publisher: Springer-Verlag: Berlin",
		"source": "Google Scholar",
		"title": "Variable selection in random forest with application to quantitative structure-activity relationship",
		"author": [
			{
				"family": "Svetnik",
				"given": "Vladimir"
			},
			{
				"family": "Liaw",
				"given": "Andy"
			},
			{
				"family": "Tong",
				"given": "Christopher"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2004"
				]
			]
		},
		"citation-key": "svetnik_etal04"
	},
	{
		"id": "svetnik_etal03",
		"type": "article-journal",
		"container-title": "Journal of chemical information and computer sciences",
		"issue": "6",
		"note": "publisher: ACS Publications",
		"page": "1947–1958",
		"source": "Google Scholar",
		"title": "Random forest: a classification and regression tool for compound classification and QSAR modeling",
		"title-short": "Random forest",
		"volume": "43",
		"author": [
			{
				"family": "Svetnik",
				"given": "Vladimir"
			},
			{
				"family": "Liaw",
				"given": "Andy"
			},
			{
				"family": "Tong",
				"given": "Christopher"
			},
			{
				"family": "Culberson",
				"given": "J. Christopher"
			},
			{
				"family": "Sheridan",
				"given": "Robert P."
			},
			{
				"family": "Feuston",
				"given": "Bradley P."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2003"
				]
			]
		},
		"citation-key": "svetnik_etal03"
	},
	{
		"id": "guyon_etal02",
		"type": "article-journal",
		"container-title": "Machine learning",
		"issue": "1",
		"note": "publisher: Springer",
		"page": "389–422",
		"source": "Google Scholar",
		"title": "Gene selection for cancer classification using support vector machines",
		"volume": "46",
		"author": [
			{
				"family": "Guyon",
				"given": "Isabelle"
			},
			{
				"family": "Weston",
				"given": "Jason"
			},
			{
				"family": "Barnhill",
				"given": "Stephen"
			},
			{
				"family": "Vapnik",
				"given": "Vladimir"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2002"
				]
			]
		},
		"citation-key": "guyon_etal02"
	},
	{
		"id": "breiman01",
		"type": "article-journal",
		"container-title": "Machine learning",
		"issue": "1",
		"note": "publisher: Springer",
		"page": "5–32",
		"source": "Google Scholar",
		"title": "Random forests",
		"volume": "45",
		"author": [
			{
				"family": "Breiman",
				"given": "Leo"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2001"
				]
			]
		},
		"citation-key": "breiman01"
	},
	{
		"id": "ambroise_mclachlan02",
		"type": "article-journal",
		"container-title": "Proceedings of the national academy of sciences",
		"issue": "10",
		"note": "publisher: National Acad Sciences",
		"page": "6562–6566",
		"source": "Google Scholar",
		"title": "Selection bias in gene extraction on the basis of microarray gene-expression data",
		"volume": "99",
		"author": [
			{
				"family": "Ambroise",
				"given": "Christophe"
			},
			{
				"family": "McLachlan",
				"given": "Geoffrey J."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2002"
				]
			]
		},
		"citation-key": "ambroise_mclachlan02"
	},
	{
		"id": "svetnik_etal04a",
		"type": "paper-conference",
		"container-title": "International workshop on multiple Classifier systems",
		"page": "334–343",
		"publisher": "Springer",
		"source": "Google Scholar",
		"title": "Application of Breiman’s random forest to modeling structure-activity relationships of pharmaceutical molecules",
		"author": [
			{
				"family": "Svetnik",
				"given": "Vladimir"
			},
			{
				"family": "Liaw",
				"given": "Andy"
			},
			{
				"family": "Tong",
				"given": "Christopher"
			},
			{
				"family": "Wang",
				"given": "Ting"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2004"
				]
			]
		},
		"citation-key": "svetnik_etal04a"
	},
	{
		"id": "probst_etal19",
		"type": "article-journal",
		"container-title": "Wiley Interdisciplinary Reviews: data mining and knowledge discovery",
		"issue": "3",
		"note": "publisher: Wiley Periodicals, Inc Hoboken, USA",
		"page": "e1301",
		"source": "Google Scholar",
		"title": "Hyperparameters and tuning strategies for random forest",
		"volume": "9",
		"author": [
			{
				"family": "Probst",
				"given": "Philipp"
			},
			{
				"family": "Wright",
				"given": "Marvin N."
			},
			{
				"family": "Boulesteix",
				"given": "Anne-Laure"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		},
		"citation-key": "probst_etal19"
	},
	{
		"id": "friedman91",
		"type": "article-journal",
		"container-title": "The annals of statistics",
		"issue": "1",
		"note": "publisher: Institute of Mathematical Statistics",
		"page": "1–67",
		"source": "Google Scholar",
		"title": "Multivariate adaptive regression splines",
		"volume": "19",
		"author": [
			{
				"family": "Friedman",
				"given": "Jerome H."
			}
		],
		"issued": {
			"date-parts": [
				[
					"1991"
				]
			]
		},
		"citation-key": "friedman91"
	},
	{
		"id": "breiman96",
		"type": "article-journal",
		"container-title": "Machine learning",
		"issue": "2",
		"note": "publisher: Springer",
		"page": "123–140",
		"source": "Google Scholar",
		"title": "Bagging predictors",
		"volume": "24",
		"author": [
			{
				"family": "Breiman",
				"given": "Leo"
			}
		],
		"issued": {
			"date-parts": [
				[
					"1996"
				]
			]
		},
		"citation-key": "breiman96"
	},
	{
		"id": "couronne_etal18",
		"type": "article-journal",
		"container-title": "BMC bioinformatics",
		"issue": "1",
		"note": "publisher: Springer",
		"page": "1–14",
		"source": "Google Scholar",
		"title": "Random forest versus logistic regression: a large-scale benchmark experiment",
		"title-short": "Random forest versus logistic regression",
		"volume": "19",
		"author": [
			{
				"family": "Couronné",
				"given": "Raphael"
			},
			{
				"family": "Probst",
				"given": "Philipp"
			},
			{
				"family": "Boulesteix",
				"given": "Anne-Laure"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		},
		"citation-key": "couronne_etal18"
	},
	{
		"id": "genuer_etal10",
		"type": "article-journal",
		"container-title": "Pattern recognition letters",
		"issue": "14",
		"note": "publisher: Elsevier",
		"page": "2225–2236",
		"source": "Google Scholar",
		"title": "Variable selection using random forests",
		"volume": "31",
		"author": [
			{
				"family": "Genuer",
				"given": "Robin"
			},
			{
				"family": "Poggi",
				"given": "Jean-Michel"
			},
			{
				"family": "Tuleau-Malot",
				"given": "Christine"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2010"
				]
			]
		},
		"citation-key": "genuer_etal10"
	},
	{
		"id": "kuhn_johnson19",
		"type": "book",
		"publisher": "CRC Press",
		"source": "Google Scholar",
		"title": "Feature engineering and selection: A practical approach for predictive models",
		"title-short": "Feature engineering and selection",
		"author": [
			{
				"family": "Kuhn",
				"given": "Max"
			},
			{
				"family": "Johnson",
				"given": "Kjell"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		},
		"citation-key": "kuhn_johnson19"
	},
	{
		"id": "demircioglu21",
		"type": "article-journal",
		"container-title": "Insights into Imaging",
		"issue": "1",
		"note": "publisher: Springer",
		"page": "1–10",
		"source": "Google Scholar",
		"title": "Measuring the bias of incorrect application of feature selection when using cross-validation in radiomics",
		"volume": "12",
		"author": [
			{
				"family": "Demircioğlu",
				"given": "Aydin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		},
		"citation-key": "demircioglu21"
	},
	{
		"id": "geurts_etal06",
		"type": "article-journal",
		"container-title": "Machine learning",
		"issue": "1",
		"note": "publisher: Springer",
		"page": "3–42",
		"source": "Google Scholar",
		"title": "Extremely randomized trees",
		"volume": "63",
		"author": [
			{
				"family": "Geurts",
				"given": "Pierre"
			},
			{
				"family": "Ernst",
				"given": "Damien"
			},
			{
				"family": "Wehenkel",
				"given": "Louis"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2006"
				]
			]
		},
		"citation-key": "geurts_etal06"
	},
	{
		"id": "svetnik_etal05",
		"type": "article-journal",
		"container-title": "Journal of chemical information and modeling",
		"issue": "3",
		"note": "publisher: ACS Publications",
		"page": "786–799",
		"source": "Google Scholar",
		"title": "Boosting: An ensemble learning tool for compound classification and QSAR modeling",
		"title-short": "Boosting",
		"volume": "45",
		"author": [
			{
				"family": "Svetnik",
				"given": "Vladimir"
			},
			{
				"family": "Wang",
				"given": "Ting"
			},
			{
				"family": "Tong",
				"given": "Christopher"
			},
			{
				"family": "Liaw",
				"given": "Andy"
			},
			{
				"family": "Sheridan",
				"given": "Robert P."
			},
			{
				"family": "Song",
				"given": "Qinghua"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2005"
				]
			]
		},
		"citation-key": "svetnik_etal05"
	},
	{
		"id": "wright_ziegler17",
		"type": "article-journal",
		"abstract": "We introduce the C++ application and R package ranger. The software is a fast implementation of random forests for high dimensional data. Ensembles of classification, regression and survival trees are supported. We describe the implementation, provide examples, validate the package with a reference implementation, and compare runtime and memory usage with other implementations. The new software proves to scale best with the number of features, samples, trees, and features tried for splitting. Finally, we show that ranger is the fastest and most memory efficient implementation of random forests to analyze data on the scale of a genome-wide association study.",
		"container-title": "Journal of Statistical Software",
		"DOI": "10.18637/jss.v077.i01",
		"ISSN": "1548-7660",
		"language": "en",
		"page": "1-17",
		"source": "www.jstatsoft.org",
		"title": "ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R",
		"title-short": "ranger",
		"URL": "https://doi.org/10.18637/jss.v077.i01",
		"volume": "77",
		"author": [
			{
				"family": "Wright",
				"given": "Marvin N."
			},
			{
				"family": "Ziegler",
				"given": "Andreas"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					4,
					15
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017",
					3,
					31
				]
			]
		},
		"citation-key": "wright_ziegler17"
	},
	{
		"id": "james_etal13",
		"type": "book",
		"publisher": "Springer",
		"source": "Google Scholar",
		"title": "An introduction to statistical learning",
		"volume": "112",
		"author": [
			{
				"family": "James",
				"given": "Gareth"
			},
			{
				"family": "Witten",
				"given": "Daniela"
			},
			{
				"family": "Hastie",
				"given": "Trevor"
			},
			{
				"family": "Tibshirani",
				"given": "Robert"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2013"
				]
			]
		},
		"citation-key": "james_etal13"
	},
	{
		"id": "hastie_etal09",
		"type": "book",
		"publisher": "Springer",
		"source": "Google Scholar",
		"title": "The elements of statistical learning: data mining, inference, and prediction",
		"title-short": "The elements of statistical learning",
		"volume": "2",
		"author": [
			{
				"family": "Hastie",
				"given": "Trevor"
			},
			{
				"family": "Tibshirani",
				"given": "Robert"
			},
			{
				"family": "Friedman",
				"given": "Jerome H."
			},
			{
				"family": "Friedman",
				"given": "Jerome H."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2009"
				]
			]
		},
		"citation-key": "hastie_etal09"
	},
	{
		"id": "kuhn08",
		"type": "article-journal",
		"abstract": "The caret package, short for classification and regression training, contains numerous tools for developing predictive models using the rich set of models available in R. The package focuses on simplifying model training and tuning across a wide variety of modeling techniques. It also includes methods for pre-processing training data, calculating variable importance, and model visualizations. An example from computational chemistry is used to illustrate the functionality on a real data set and to benchmark the benefits of parallel processing with several types of models.",
		"container-title": "Journal of Statistical Software",
		"DOI": "10.18637/jss.v028.i05",
		"ISSN": "1548-7660",
		"language": "en",
		"page": "1-26",
		"source": "www.jstatsoft.org",
		"title": "Building Predictive Models in R Using the caret Package",
		"URL": "https://doi.org/10.18637/jss.v028.i05",
		"volume": "28",
		"author": [
			{
				"family": "Kuhn",
				"given": "Max"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					4,
					15
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2008",
					11,
					10
				]
			]
		},
		"citation-key": "kuhn08"
	},
	{
		"id": "xu_etal21",
		"type": "article-journal",
		"container-title": "arXiv preprint arXiv:2108.13637",
		"source": "Google Scholar",
		"title": "When are Deep Networks really better than Decision Forests at small sample sizes, and how?",
		"author": [
			{
				"family": "Xu",
				"given": "Haoyin"
			},
			{
				"family": "Kinfu",
				"given": "Kaleab A."
			},
			{
				"family": "LeVine",
				"given": "Will"
			},
			{
				"family": "Panda",
				"given": "Sambit"
			},
			{
				"family": "Dey",
				"given": "Jayanta"
			},
			{
				"family": "Ainsworth",
				"given": "Michael"
			},
			{
				"family": "Peng",
				"given": "Yu-Chung"
			},
			{
				"family": "Kusmanov",
				"given": "Madi"
			},
			{
				"family": "Engert",
				"given": "Florian"
			},
			{
				"family": "White",
				"given": "Christopher M."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		},
		"citation-key": "xu_etal21"
	},
	{
		"id": "geron19",
		"type": "book",
		"publisher": " O'Reilly Media, Inc.",
		"source": "Google Scholar",
		"title": "Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow: Concepts, tools, and techniques to build intelligent systems",
		"title-short": "Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow",
		"author": [
			{
				"family": "Géron",
				"given": "Aurélien"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		},
		"citation-key": "geron19"
	},
	{
		"id": "goldstein_etal11",
		"type": "article-journal",
		"abstract": "The Random Forests (RF) algorithm has become a commonly used machine learning algorithm for genetic association studies. It is well suited for genetic applications since it is both computationally efficient and models genetic causal mechanisms well. With its growing ubiquity, there has been inconsistent and less than optimal use of RF in the literature. The purpose of this review is to breakdown the theoretical and statistical basis of RF so that practitioners are able to apply it in their work. An emphasis is placed on showing how the various components contribute to bias and variance, as well as discussing variable importance measures. Applications specific to genetic studies are highlighted. To provide context, RF is compared to other commonly used machine learning algorithms.",
		"container-title": "Statistical Applications in Genetics and Molecular Biology",
		"DOI": "10.2202/1544-6115.1691",
		"ISSN": "1544-6115",
		"issue": "1",
		"journalAbbreviation": "Stat Appl Genet Mol Biol",
		"language": "eng",
		"note": "PMID: 22889876\nPMCID: PMC3154091",
		"page": "32",
		"source": "PubMed",
		"title": "Random forests for genetic association studies",
		"volume": "10",
		"author": [
			{
				"family": "Goldstein",
				"given": "Benjamin A."
			},
			{
				"family": "Polley",
				"given": "Eric C."
			},
			{
				"family": "Briggs",
				"given": "Farren B. S."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2011"
				]
			]
		},
		"citation-key": "goldstein_etal11"
	},
	{
		"id": "kauffman_jurs01",
		"type": "article-journal",
		"abstract": "Experimental IC50 data for 314 selective cyclooxygenase-2 (COX-2) inhibitors are used to develop quantitation and classification models as a potential screening mechanism for larger libraries of target compounds. Experimental log(IC50) values ranged from 0.23 to ≥ 5.00. Numerical descriptors encoding solely topological information are calculated for all structures and are used as inputs for linear regression, computational neural network, and classification analysis routines. Evolutionary optimization algorithms are then used to search the descriptor space for information-rich subsets which minimize the rms error of a diverse training set of compounds. An eight-descriptor model was identified as a robust predictor of experimental log(IC50) values, producing a root-mean-square error of 0.625 log units for an external prediction set of inhibitors which took no part in model development. A k-nearest neighbor classification study of the data set discriminating between active and inactive members produced a nine-descriptor model able to accurately classify 83.3% of the prediction set compounds correctly.",
		"container-title": "Journal of Chemical Information and Computer Sciences",
		"DOI": "10.1021/ci010073h",
		"ISSN": "0095-2338",
		"issue": "6",
		"journalAbbreviation": "J. Chem. Inf. Comput. Sci.",
		"note": "publisher: American Chemical Society",
		"page": "1553-1560",
		"source": "ACS Publications",
		"title": "QSAR and k-Nearest Neighbor Classification Analysis of Selective Cyclooxygenase-2 Inhibitors Using Topologically-Based Numerical Descriptors",
		"URL": "https://doi.org/10.1021/ci010073h",
		"volume": "41",
		"author": [
			{
				"family": "Kauffman",
				"given": "Gregory W."
			},
			{
				"family": "Jurs",
				"given": "Peter C."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					6,
					10
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2001",
					11,
					1
				]
			]
		},
		"citation-key": "kauffman_jurs01"
	}
]