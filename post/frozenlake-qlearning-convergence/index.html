<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.7.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Gertjan Verhoeven">

  
  
  
    
  
  <meta name="description" content="This blog post concerns a famous toy problem in Reinforcement Learning, the [FrozenLake environment](https://gym.openai.com/envs/FrozenLake-v0/). We compare solving an environment with RL by reaching **maximum performance** versus obtaining the **true state-action values** $Q_{s,a}$.">

  
  <link rel="alternate" hreflang="en-us" href="/post/frozenlake-qlearning-convergence/">

  


  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  





<script async src="https://www.googletagmanager.com/gtag/js?id=G-GQH76F5Q5R"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           document.location = url;
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target);  
  }

  gtag('js', new Date());
  gtag('config', 'G-GQH76F5Q5R', {});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  


  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_2.png">

  <link rel="canonical" href="/post/frozenlake-qlearning-convergence/">

  
  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@GertjanVerhoev1">
  <meta property="twitter:creator" content="@GertjanVerhoev1">
  
  <meta property="og:site_name" content="Gertjan Verhoeven">
  <meta property="og:url" content="/post/frozenlake-qlearning-convergence/">
  <meta property="og:title" content="OpenAI Gym&#39;s FrozenLake: Converging on the true Q-values | Gertjan Verhoeven">
  <meta property="og:description" content="This blog post concerns a famous toy problem in Reinforcement Learning, the [FrozenLake environment](https://gym.openai.com/envs/FrozenLake-v0/). We compare solving an environment with RL by reaching **maximum performance** versus obtaining the **true state-action values** $Q_{s,a}$."><meta property="og:image" content="/img/headers/ryan-fishel-xm9NIbVD-8E-unsplash_ed.png">
  <meta property="twitter:image" content="/img/headers/ryan-fishel-xm9NIbVD-8E-unsplash_ed.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2021-03-07T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2021-03-07T00:00:00&#43;00:00">
  

  


    






  






<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "/post/frozenlake-qlearning-convergence/"
  },
  "headline": "OpenAI Gym's FrozenLake: Converging on the true Q-values",
  
  "datePublished": "2021-03-07T00:00:00Z",
  "dateModified": "2021-03-07T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Gertjan Verhoeven"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Gertjan Verhoeven",
    "logo": {
      "@type": "ImageObject",
      "url": "img//"
    }
  },
  "description": "This blog post concerns a famous toy problem in Reinforcement Learning, the [FrozenLake environment](https://gym.openai.com/envs/FrozenLake-v0/). We compare solving an environment with RL by reaching **maximum performance** versus obtaining the **true state-action values** $Q_{s,a}$."
}
</script>

  

  


  


  





  <title>OpenAI Gym&#39;s FrozenLake: Converging on the true Q-values | Gertjan Verhoeven</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  

<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Gertjan Verhoeven</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Gertjan Verhoeven</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      <li class="nav-item">
        <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
      </li>
      

      

    </ul>

  </div>
</nav>


  <article class="article">

  









<div class="article-header">
  
  
  <img src="/img/headers/ryan-fishel-xm9NIbVD-8E-unsplash_ed.png" class="article-banner" alt="">
  

  
</div>




  

  
  
  
<div class="article-container pt-3">
  <h1>OpenAI Gym&#39;s FrozenLake: Converging on the true Q-values</h1>

  

  
    


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Mar 7, 2021
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    21 min read
  </span>
  

  
  
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder mr-1"></i><a href="/categories/machine-learning/">Machine Learning</a></span>
  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style">
      


<p>(Photo by Ryan Fishel on Unsplash)</p>
<p>This blog post concerns a famous “toy” problem in Reinforcement Learning, the <a href="https://gym.openai.com/envs/FrozenLake-v0/">FrozenLake environment</a>. We compare solving an environment with RL by reaching <strong>maximum performance</strong> versus obtaining the <strong>true state-action values</strong> <span class="math inline">\(Q_{s,a}\)</span>. In doing so I learned a lot about RL as well as about Python (such as the existence of a <code>ggplot</code> clone for Python, <code>plotnine</code>, see this blog post for some cool examples).</p>
<p>FrozenLake was created by OpenAI in 2016 as part of their Gym python package for Reinforcement Learning. Nowadays, the interwebs is full of tutorials how to “solve” FrozenLake. Most of them focus on performance in terms of episodic reward. As soon as this maxes out the algorithm is often said to have converged. For example, in this <a href="https://stats.stackexchange.com/questions/206944/how-do-i-know-when-a-q-learning-algorithm-converges">question on Cross-Validated</a> about Convergence and Q-learning:</p>
<p><em>In practice, a reinforcement learning algorithm is considered to converge when the learning curve gets flat and no longer increases.</em></p>
<p>Now, for <strong>Q-learning</strong> it has been proven that, <em>under certain conditions</em>, the Q-values convergence to their true values. But does this happens when the performance maxes out? In this blog we’ll see that this is not generally the case.</p>
<p>We start with obtaining the true, exact state-action values. For this we use Dynamic Programming (DP). Having implemented Dynamic Programming (DP) for the FrozenLake environment as an exercise notebook already (created by Udacity, go check them out), this was a convenient starting point.</p>
<div id="loading-the-packages-and-modules" class="section level2">
<h2>Loading the packages and modules</h2>
<p>We need various Python packages and modules.</p>
<pre class="python"><code># data science packages
import numpy as np
import pandas as pd
import plotnine as p9
import matplotlib.pyplot as plt
%matplotlib inline

# RL algorithms
from qlearning import *
from dp import *

# utils
from helpers import *
from plot_utils import plot_values
import copy
import dill

from frozenlake import FrozenLakeEnv

env = FrozenLakeEnv(is_slippery = True)</code></pre>
</div>
<div id="frozen-lake-environment-description" class="section level2">
<h2>Frozen Lake Environment description</h2>
<p><em>Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake (G).
The water is mostly frozen (F), but there are a few holes (H) where the ice has melted.</em></p>
<p><img src="/post/2021-03-07-qlearning_frozenlake_convergence_files/Frozen-Lake.png" /></p>
<p><em>If you step into one of those holes, you’ll fall into the freezing water. At this time, there’s an international frisbee shortage, so it’s absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won’t always move in the direction you intend.</em></p>
<p><em>The episode ends when you reach the goal or fall in a hole. You receive a reward of 1 if you reach the goal, and zero otherwise.</em></p>
<p>The agent moves through a <span class="math inline">\(4 \times 4\)</span> gridworld, with states numbered as follows:</p>
<pre><code>[[ 0  1  2  3]
 [ 4  5  6  7]
 [ 8  9 10 11]
 [12 13 14 15]]</code></pre>
<p>and the agent has 4 potential actions:</p>
<pre><code>LEFT = 0
DOWN = 1
RIGHT = 2
UP = 3</code></pre>
<p>The FrozenLake Gym environment has been amended to make the one-step dynamics accessible to the agent. For example, if we are in State <code>s = 1</code> and we perform action <code>a = 0</code>, the probabilities of ending up in new states, including the associated rewards are contained in the <span class="math inline">\(P_{s,a}\)</span> array:</p>
<pre class="python"><code>env.P[1][0]</code></pre>
<pre><code>[(0.3333333333333333, 1, 0.0, False),
 (0.3333333333333333, 0, 0.0, False),
 (0.3333333333333333, 5, 0.0, True)]</code></pre>
<p>The FrozenLake environment is highly stochastic, with a very sparse reward: only when the agent reaches the goal, a reward of <code>+1</code> is obtained. This means that if we do not set a discount rate, the agent can keep on wandering around without receiving a learning “signal” that can be propagated back through the preceding state-actions (since falling into the holes does not result in a negative reward) and thus learning very slowly. We will focus on the discounting case (<code>gamma = 0.95</code>) for this reason (less computation needed for convergence), but compare also with the undiscounted case.</p>
</div>
<div id="solving-frozen-lake-using-dp" class="section level2">
<h2>Solving Frozen Lake using DP</h2>
<p>Let us solve FrozenLake first for the no discounting case (<code>gamma = 1</code>).
The Q-value for the first state will then tell us the average episodic reward, which for FrozenLake translates into the percentage of episodes in which the Agent succesfully reaches its goal.</p>
<pre class="python"><code>policy_pi, V_pi = policy_iteration(env, gamma = 1, theta=1e-9, \
                                   verbose = False)


plot_values(V_pi)</code></pre>
<p><img src="/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_9_1.png" /></p>
<p>The Q-value for state <code>s = 0</code> is <code>0.824</code>. This means that for <code>gamma = 1</code> and following an optimal policy <span class="math inline">\(\pi^*\)</span>, 82.4% of all episodes ends in succes.</p>
<p>As already mentioned above, for computational reasons we will apply Q-learning to the environment with <code>gamma = 0.95</code>. So… lets solve FrozenLake for <code>gamma = 0.95</code> as well:</p>
<pre class="python"><code># obtain the optimal policy and optimal state-value function
policy_pi_dc, V_pi_dc = policy_iteration(env, gamma = 0.95, theta=1e-9, \
                                   verbose = False)

Q_perfect = create_q_dict_from_v(env, V_pi_dc, gamma = 0.95)

df_true = convert_vals_to_pd(V_pi_dc)

plot_values(V_pi_dc)</code></pre>
<p><img src="/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_11_1.png" /></p>
<p>Now, comparing the optimal policies for <code>gamma = 0.95</code> and for <code>gamma = 1</code> (not shown here) we find that they are not the same. Therefore, 82.4% succes rate is likely an upper bound for <code>gamma = 0.95</code>, since introducing discounting in this stochastic environment can (intuitively) either have no effect on the optimal policy, or favor more risky (lower succes rate) but faster (less discounting) policies, leading to a lower overall succes rate.</p>
<p>For example, for the undiscounted case, the Agent is indifferent in choosing direction in the first state. If it ends up going <strong>right</strong> , it can choose UP and wander around at the top row at no cost until it reaches the starting state again. As soon as this wandering around gets a cost by discounting we see (not shown) that the Agent is no longer indifferent, and does NOT want to end up wandering in the top row, but chooses to play LEFT in state <code>s = 0</code> instead.</p>
</div>
<div id="checking-the-performance-of-an-optimal-greedy-policy-based-on-perfect-q-values" class="section level1">
<h1>Checking the performance of an optimal greedy policy based on perfect Q-values</h1>
<p>Now that we have the <span class="math inline">\(Q_{s,a}\)</span> values corresponding to the optimal policy given that <code>gamma = 0.95</code>, we can check its performance. To do so, we use brute force and simulate the average reward under the optimal policy for a large number of episodes.</p>
<p>To do so, I wrote a function <code>test_performance()</code> by taking the <code>q_learning()</code> function, removing the learning (Q-updating) part and setting epsilon to zero when selecting an action (i.e. a pure greedy policy based on a given Q-table).</p>
<p>Playing around with the binomial density in <code>R</code> (<code>summary(rbinom(1e5, 1e5, prob = 0.8)/1e5</code>) tells me that choosing 100.000 episodes gives a precision of around three decimals in estimating the probability of succes. This is good enough for me.</p>
<pre class="python"><code># Obtain Q for Gamma = 0.95 and convert to defaultdict 
Q_perfect = create_q_dict_from_v(env, V_pi_dc, gamma = 0.95)

fullrun = 0

if fullrun == 1:
    d = []
    runs = 100
    for i in range(runs):
        avg_scores = test_performance(Q_perfect, env, num_episodes = 1000, \
                                   plot_every = 1000, verbose = False)
        d.append({&#39;run&#39;: i,
                 &#39;avg_score&#39;: np.mean(avg_scores)})
        print(&quot;\ri {}/{}&quot;.format(i, runs), end=&quot;&quot;)
        sys.stdout.flush() 

    d_perfect = pd.DataFrame(d)
    d_perfect.to_pickle(&#39;cached/scores_perfect_0.95.pkl&#39;)
else:
    d_perfect = pd.read_pickle(&#39;cached/scores_perfect_0.95.pkl&#39;)

</code></pre>
<pre class="python"><code>round(np.mean(d_perfect.avg_score), 3)</code></pre>
<pre><code>0.781</code></pre>
<p>Thus, we find that with the true optimal policy for <code>gamma = 0.95</code>, in the long run 78% of all episodes is succesful. This is therefore the expected plateau value for the learning curve in Q-learning, provided that the exploration rate has become sufficiently small.</p>
<p>Let’s move on to Q-learning and convergence.</p>
</div>
<div id="solving-frozenlake-using-q-learning" class="section level1">
<h1>“Solving” FrozenLake using Q-learning</h1>
<p>The typical RL tutorial approach to solve a simple MDP as FrozenLake is to choose a constant learning rate, not too high, not too low, say <span class="math inline">\(\alpha = 0.1\)</span>. Then, the exploration parameter <span class="math inline">\(\epsilon\)</span> starts at 1 and is gradually reduced to a floor value of say <span class="math inline">\(\epsilon = 0.0001\)</span>.</p>
<p>Lets solve FrozenLake this way, monitoring the learning curve (average reward per episode) as it learns, and compare the learned Q-values with the true Q-values found using DP.</p>
<p>I wrote Python functions that generate a <strong>decay schedule</strong>, a 1D numpy array of <span class="math inline">\(\epsilon\)</span> values, with length equal to the total number of episode the Q-learning algorithm is to run. This array is passed on to the Q-learning algorithm, and used during learning.</p>
<p>It is helpful to visualize the decay schedule of <span class="math inline">\(\epsilon\)</span> to check that it is reasonable before we start to use them with our Q-learning algorithm. I played around with the decay rate until the “elbow” of the curve is around 20% of the number of episodes, and reaches the desired minimal end value (<span class="math inline">\(\epsilon = 0.0001\)</span>).</p>
<pre class="python"><code>def create_epsilon_schedule(num_episodes,  \
                       epsilon_start=1.0, epsilon_decay=.9999, epsilon_min=1e-4):
    x = np.arange(num_episodes)+1
    y = np.full(num_episodes, epsilon_start)
    y = np.maximum((epsilon_decay**x)*epsilon_start, epsilon_min)
    return y

epsilon_schedule = create_epsilon_schedule(100_000)

plot_schedule(epsilon_schedule, ylab = &#39;Epsilon&#39;)</code></pre>
<p><img src="/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_20_0.png" /></p>
<p>My version of the Q-learning algorithm has slowly evolved to include more and more lists of things to monitor during the execution of the algorithm. Every 100 episodes, a copy of <span class="math inline">\(Q^{*}_{s}\)</span> and of <span class="math inline">\(Q_{s, a}\)</span> is stored in a list, as well as the average reward over this episode, as well as the final <span class="math inline">\(Q_{s,a}\)</span> table, and <span class="math inline">\(N_{s,a}\)</span> that kept track of how often each state-action was chosen. The <code>dill</code> package is used to store these datastructures on disk to avoid the need to rerun the algorithm every time the notebook is run.</p>
<pre class="python"><code># K / N + K decay learning rate schedule
# fully random policy

plot_every = 100

n_episodes = len(epsilon_schedule)

fullrun = 0

random.seed(123)
np.random.seed(123)

if fullrun == 1:
    Q_sarsamax, N_sarsamax, avg_scores, Qlist, Qtable_list = q_learning(env, num_episodes = n_episodes, \
                                                eps_schedule = epsilon_schedule,\
                                                alpha = 0.1, \
                                                gamma =  0.95, \
                                                plot_every = plot_every, \
                                                verbose = True, log_full = True)
    
    with open(&#39;cached/es1_Q_sarsamax.pkl&#39;, &#39;wb&#39;) as f:
        dill.dump(Q_sarsamax, f)
    with open(&#39;cached/es1_avg_scores.pkl&#39;, &#39;wb&#39;) as f:
        dill.dump(avg_scores, f)
    with open(&#39;cached/es1_Qlist.pkl&#39;, &#39;wb&#39;) as f:
        dill.dump(Qlist, f)   
    with open(&#39;cached/es1_Qtable_list.pkl&#39;, &#39;wb&#39;) as f:
        dill.dump(Qtable_list, f)           
else:
    with open(&#39;cached/es1_Q_sarsamax.pkl&#39;, &#39;rb&#39;) as f:
        Q_sarsamax = dill.load(f)
    with open(&#39;cached/es1_avg_scores.pkl&#39;, &#39;rb&#39;) as f:
        avg_scores = dill.load(f)
    with open(&#39;cached/es1_Qlist.pkl&#39;, &#39;rb&#39;) as f:
        Qlist = dill.load(f)   
    with open(&#39;cached/es1_Qtable_list.pkl&#39;, &#39;rb&#39;) as f:
        Qtable_list = dill.load(f)            

Q_es1_lasttable = [np.max(Q_sarsamax[key]) if key in Q_sarsamax \
     else 0 for key in np.arange(env.nS)]</code></pre>
<p>Lets plot the learning curve. Here <code>plotnine</code>, a <code>ggplot</code> clone in Python comes in handy.</p>
<p>As we can see below, the “recipe” for solving FrozenLake has worked really well. The displayed red line gives the theoretical optimal performance for <code>gamma = 0.95</code>, I used a <code>loess</code> smoother so we can more easily compare the Q-learning results with the theoretical optimum.</p>
<p>We can see that the Q-learning algorithm has indeed found a policy that performs optimal. This appears to happen at around 60.000 episodes. We return to this later.</p>
<pre class="python"><code># plot performance
df_scores = pd.DataFrame(
    {&#39;episode_nr&#39;: np.linspace(0,n_episodes,len(avg_scores),endpoint=False),
     &#39;avg_score&#39;: np.asarray(avg_scores)})

(p9.ggplot(data = df_scores.loc[df_scores.episode_nr &gt; -1], mapping = p9.aes(x = &#39;episode_nr&#39;, y = &#39;avg_score&#39;))
    + p9.geom_point(colour = &#39;gray&#39;)
    + p9.geom_smooth(method = &#39;loess&#39;)
    + p9.geom_hline(yintercept = 0.781, colour = &quot;red&quot;)
    + p9.geom_vline(xintercept = 60_000, color = &quot;blue&quot;))
</code></pre>
<p><img src="/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_24_0.png" /></p>
<p>But now comes the big question: did the <span class="math inline">\(Q^{*}_{a}\)</span> estimates converge onto the true values as well?</p>
<pre class="python"><code>q_showdown = pd.DataFrame(
    {&#39;Q_es1_lasttable&#39;: Q_es1_lasttable,
     &#39;q_true&#39;: V_pi_dc})
q_showdown[&#39;state&#39;] = range(16)

q_showdown = pd.melt(q_showdown, [&#39;state&#39;])

(p9.ggplot(data = q_showdown, mapping = p9.aes(x = &#39;state&#39;, y = &#39;value&#39;, color = &#39;factor(variable)&#39;))
+ p9.geom_point(size = 5, shape = &#39;x&#39;))</code></pre>
<p><img src="/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_26_0.png" /></p>
<p>Nope, they did not. Ok, most learned Q-values are close to their true values, but they clearly did not converge exactly to their true value.</p>
</div>
<div id="plotting-the-learning-curves-for-all-the-state-action-values" class="section level1">
<h1>Plotting the learning curves for all the state-action values</h1>
<p>To really understand what is going on, I found it helpful to plot the learning curves for all the 16 x 4 = 64 state-action values at the same time. Here <code>plotnine</code> really shines. I leave out the actual estimates, and only plot a <code>loess</code> smoothed curve for each of the state-action values over time.</p>
<pre class="python"><code># convert to pandas df
dfm = list_of_tables_to_df(Qtable_list)</code></pre>
<pre class="python"><code>#10 s
(p9.ggplot(data = dfm.loc[(dfm.episode &gt; -1)], mapping = p9.aes(x = &#39;episode&#39;, y = &#39;value&#39;, group = &#39;action&#39;, color = &#39;factor(action)&#39;))
    #+ p9.geom_point(shape = &#39;x&#39;) 
    + p9.geom_smooth(method = &#39;loess&#39;)
    + p9.geom_hline(data = df_true, mapping = p9.aes(yintercept = &#39;value&#39;), color = &#39;red&#39;)
    + p9.geom_vline(xintercept = 600, color = &#39;blue&#39;)
    + p9.facet_wrap(&quot;variable&quot;, ncol = 4)
    + p9.theme(subplots_adjust={&#39;wspace&#39;:0.4}) # fix a displaying issue
)</code></pre>
<p><img src="/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_30_0.png" /></p>
<p>First the main question: what about convergence of <span class="math inline">\(Q^{*}_{a}\)</span>? For this we need only to look, for each state, at the action with the highest value, and compare that to the true value (red horizontal lines). Most of the values appear to have converged to a value close to the true value, but Q3 is clearly still way off. Note that we using smoothing here to average out the fluctuations around the true value.</p>
<p>We noted earlier that at around episode 60.000, the optimal policy emerges and the learning curve becomes flat. Now, the most obvious reason why performance increases is because the value of <span class="math inline">\(\epsilon\)</span> is decaying, so the deleterious effects of exploration should go down, and performance should go up.</p>
<p>Another reason that performance goes up could be that the greedy policy is improving. It is interesting to examine whether at this point, meaningfull changes in the greedy policy still occur. Meaningfull changes in policy are caused by changes in the estimated state-action values. For example, we might expect two or more state-action value lines crossing, with the “right” action becoming dominant over the “wrong” action. Is this indeed the case?</p>
<p>Indeed, from the plot above, with the change point at around episode 600 (x 100),a change occurs in Q6, where the actions 0 and 2 cross. However, from the true Q-value table (not shown) we see that both actions are equally rewarding, so the crossing has no effect on performance. Note that only one of the two converges to the true value, because the exploration rate becomes so low that learning for the other action almost completely stops in the end.</p>
<p>lets zoom in then at the states that have low expected reward, Q0, Q1, Q2, and Q3. These are difficult to examine in the plot above, and have actions with expected rewards that are similar and therefore more difficult to resolve (BUT: since the difference in expected reward is small, the effect of resolving them on the overall performance is small as well). For these we plot the actual state-actions values:</p>
<pre class="python"><code>(p9.ggplot(data = dfm[dfm.variable.isin([&#39;Q0&#39;, &#39;Q1&#39;,&#39;Q2&#39;, &#39;Q3&#39;])], mapping = p9.aes(x = &#39;episode&#39;, y = &#39;value&#39;, group = &#39;action&#39;, color = &#39;factor(action)&#39;))
    + p9.geom_point(shape = &#39;x&#39;) 
    #+ p9.geom_smooth(method = &#39;loess&#39;)
    + p9.geom_hline(data = df_true[df_true.variable.isin([&#39;Q0&#39;, &#39;Q1&#39;,&#39;Q2&#39;, &#39;Q3&#39;])], mapping = p9.aes(yintercept = &#39;value&#39;), color = &#39;red&#39;)
    + p9.geom_vline(xintercept = 600, color = &#39;blue&#39;)
    + p9.facet_wrap(&quot;variable&quot;, scales = &#39;free_y&#39;)
    + p9.theme(subplots_adjust={&#39;wspace&#39;:0.15})
)</code></pre>
<p><img src="/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_33_0.png" /></p>
<p>From Q1, Q2 and Q3, we can see that exploration really goes down at around episode 500 (x 100) (<span class="math inline">\(\epsilon\)</span> at this point is <code>0.007</code>), and with the optimal action standing out already long before reaching this point.</p>
<p>Only with Q2 there is a portion of the learning curve where action 1 has the highest value, and is chosen for quite some time before switching back to action 0 again at around episode 60.000. Let’s compare with the true values for Q2:</p>
<pre class="python"><code>Q_perfect[2]</code></pre>
<pre><code>array([0.15347714, 0.14684971, 0.14644451, 0.13958106])</code></pre>
<p>Indeed, the difference in expected reward between the actions in state 2 is really small, and because of the increasingly greedy action selection, only action 0 converges to its true values, with the other values “frozen” in place because of the low exploration rate.</p>
<p>Now, after analyzing what happens if we apply the “cookbook” approach to solving problems using RL, let’s change our attention to getting convergence for preferably ALL the state-action values.</p>
</div>
<div id="q-learning-theoretical-sufficient-conditions-for-convergence" class="section level1">
<h1>Q-learning: Theoretical sufficient conditions for convergence</h1>
<p>According to our RL bible (Sutton &amp; Barto), to obtain <strong>exact</strong> convergence, we need two conditions to hold.</p>
<ul>
<li>The first is that all states continue to be visited, and</li>
<li>the second is that the learning rate goes to zero.</li>
</ul>
<div id="continuous-exploration-visiting-all-the-states" class="section level2">
<h2>Continuous exploration: visiting all the states</h2>
<p>The nice thing of Q-learning is that it is an <strong>off-policy</strong> learning algorithm. This means that no matter what the actual policy is used to explore the states, the Q-values we learn correspond to the expected reward when following the <strong>optimal policy</strong>. Which is quite miraculous if you ask me.</p>
<p>Now, our (admittedly, a bit academic) goal is getting <strong>all</strong> of the learned state values <strong>as close as possible</strong> to the true values. An epsilon-greedy policy with a low <span class="math inline">\(epsilon\)</span> would spent a lot of time by choosing state-actions that are on the optimal path between start and goal state, and would only rarely visit low value states, or choose low value state-actions.</p>
<p>Because we can choose any policy we like, I chose a completely <strong>random</strong> policy. This way, the Agent is more likely to end up in low value states and estimate the Q-values of those state-actions accurately as well.</p>
<p>Note that since theFrozen Lake enviroment has a lot of inherent randomness already because of the ice being slippery, a policy with a low <span class="math inline">\(\epsilon\)</span> (most of the time exploiting and only rarely exploring) will still bring the agent in low values states, but this would require much more episodes.</p>
</div>
<div id="convergence-and-learning-rate-schedules" class="section level2">
<h2>Convergence and learning rate schedules</h2>
<p>For a particular constant learning rate, provided that it is not too high, the estimated Q-values will converge to a situation where they start to fluctuate around their true values.
If we subsequently lower the learning rate, the scale of the fluctuations (their <strong>variance</strong>) will decrease.
If the learning rate is gradually decayed to zero, the estimates will converge.
According to Sutton &amp; Barto (2018), the <strong>decay schedule</strong> of <span class="math inline">\(\alpha\)</span> must obey two constraints to assure convergence: (p 33).</p>
<ul>
<li>sum of increments must go to infinity</li>
<li>sum of the square of increments must go to zero</li>
</ul>
<p>On the interwebs, I found two formulas that are often used to decay hyperparameters of Reinforcement Learning algorithms:</p>
<p><span class="math inline">\(\alpha_n = K / (K + n)\)</span> (Eq. 1)</p>
<p><span class="math inline">\(\alpha_n = \Sigma^{\infty}_{n = 1} \delta^n \alpha_0\)</span> (Eq. 2)</p>
<p>Again, just as with the decay schedule for <span class="math inline">\(\epsilon\)</span>, it is helpful to visualize the decay schedules to check that they are reasonable before we start to use them with our Q-learning algorithm.</p>
<pre class="python"><code># Eq 1
def create_alpha_schedule(num_episodes, \
                     alpha_K = 100, alpha_min = 1e-3):
    x = np.arange(num_episodes)+1
    y = np.maximum(alpha_K/(x + alpha_K), alpha_min)
    return y

# Eq 2
def create_alpha_schedule2(num_episodes,  \
                       alpha_start=1.0, alpha_decay=.999, alpha_min=1e-3):
    x = np.arange(num_episodes)+1
    y = np.full(num_episodes, alpha_start)
    y = np.maximum((alpha_decay**x)*alpha_start, alpha_min)
    return y

</code></pre>
<p>I played a bit with the shape parameter to get a curve with the “elbow” around 20% of the episodes.</p>
<pre class="python"><code>alpha_schedule = create_alpha_schedule(num_episodes = 500_000, \
                                       alpha_K = 5_000)

plot_schedule(alpha_schedule, ylab = &#39;Alpha&#39;)
</code></pre>
<p><img src="/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_44_0.png" /></p>
<pre class="python"><code>round(min(alpha_schedule), 3)</code></pre>
<pre><code>0.01</code></pre>
<p>This curve decays <span class="math inline">\(\alpha\)</span> in 500.000 episodes to <code>0.01</code>.</p>
<p>The second formula (Equation 2) decays <span class="math inline">\(\alpha\)</span> even further, to <code>0.001</code>:</p>
<pre class="python"><code>alpha_schedule2 = create_alpha_schedule2(num_episodes = 500_000, \
                                        alpha_decay = 0.99997)

plot_schedule(alpha_schedule2, ylab = &#39;Alpha&#39;)</code></pre>
<p><img src="/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_47_0.png" /></p>
<pre class="python"><code>min(alpha_schedule2)</code></pre>
<pre><code>0.001</code></pre>
</div>
</div>
<div id="running-the-q-learning-algorithm-for-different-learning-rate-schedules" class="section level1">
<h1>Running the Q-learning algorithm for different learning rate schedules</h1>
<p>We start with the decay function that follows Equation 1. To get a full random policy, we set <span class="math inline">\(\epsilon = 1\)</span>. Note that this gives awful performance where the learning curve suggests it is hardly learning anything at all. However, wait until we try out a fully exploiting policy on the Q-value table learned during this run!</p>
<pre class="python"><code># K / N + K decay learning rate schedule
# fully random policy

plot_every = 100

alpha_schedule = create_alpha_schedule(num_episodes = 500_000, \
                                       alpha_K = 5_000)

n_episodes = len(alpha_schedule)

fullrun = 0

if fullrun == 1:
    Q_sarsamax, N_sarsamax, avg_scores, Qlist = q_learning(env, num_episodes = n_episodes, \
                                                eps = 1,\
                                                alpha_schedule = alpha_schedule, \
                                                gamma =  0.95, \
                                                plot_every = plot_every, \
                                                verbose = True)
    
    with open(&#39;cached/as1_Q_sarsamax.pkl&#39;, &#39;wb&#39;) as f:
        dill.dump(Q_sarsamax, f)
    with open(&#39;cached/as1_avg_scores.pkl&#39;, &#39;wb&#39;) as f:
        dill.dump(avg_scores, f)
    with open(&#39;cached/as1_Qlist.pkl&#39;, &#39;wb&#39;) as f:
        dill.dump(Qlist, f)        
else:
    with open(&#39;cached/as1_Q_sarsamax.pkl&#39;, &#39;rb&#39;) as f:
        Q_sarsamax = dill.load(f)
    with open(&#39;cached/as1_avg_scores.pkl&#39;, &#39;rb&#39;) as f:
        avg_scores = dill.load(f)
    with open(&#39;cached/as1_Qlist.pkl&#39;, &#39;rb&#39;) as f:
        Qlist = dill.load(f)      

Q_as1_lasttable = [np.max(Q_sarsamax[key]) if key in Q_sarsamax \
     else 0 for key in np.arange(env.nS)]</code></pre>
<p>Here is the learning curve for this run of Q-learning:</p>
<pre class="python"><code># plot performance
plt.plot(np.linspace(0,n_episodes,len(avg_scores),endpoint=False), np.asarray(avg_scores))
plt.xlabel(&#39;Episode Number&#39;)
plt.ylabel(&#39;Average Reward (Over Next %d Episodes)&#39; % plot_every)
plt.show()

print((&#39;Best Average Reward over %d Episodes: &#39; % plot_every), np.max(avg_scores))    
 </code></pre>
<p><img src="/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_53_0.png" /></p>
<pre><code>Best Average Reward over 100 Episodes:  0.08</code></pre>
<p>Pretty awfull huh? Now let us check out the performance of the learned Q-table:</p>
<pre class="python"><code>fullrun = 0
runs = 100

if fullrun == 1:
    d_q = []
    for i in range(runs):
        avg_scores = test_performance(Q_sarsamax, env, num_episodes = 1000, \
                                   plot_every = 1000, verbose = False)
        d_q.append({&#39;run&#39;: i,
                 &#39;avg_score&#39;: np.mean(avg_scores)})  
        print(&quot;\ri = {}/{}&quot;.format(i+1, runs), end=&quot;&quot;)
        sys.stdout.flush() 
        
    d_qlearn = pd.DataFrame(d_q)
    d_qlearn.to_pickle(&#39;cached/scores_qlearn_0.95.pkl&#39;)
else:
    d_qlearn = pd.read_pickle(&#39;cached/scores_qlearn_0.95.pkl&#39;)</code></pre>
<pre class="python"><code>round(np.mean(d_qlearn.avg_score), 3)</code></pre>
<pre><code>0.778</code></pre>
<p>Bam! Equal to the performance of the optimal policy found using Dynamic programming (sampling error (2x SD) is +/- 0.008). The random policy has actual learned Q-values that for a greedy policy result in optimal performance!</p>
<div id="but-what-about-convergence" class="section level2">
<h2>But what about convergence?</h2>
<p>Ok, so Q-learning found an optimal policy. But did it converge?
Our <code>q_learning()</code> function made a list of Q-tables while learning, adding a new table every 100 episodes. This gives us 5.000 datapoints for each Q-value, which we can plot to visually check for convergence.</p>
<p>As with the list of state-action tables above, It takes some datawrangling to get the list of Q-tables in a nice long pandas DataFrame suitable for plotting. This is hidden away in the <code>list_to_df()</code> function.</p>
<pre class="python"><code># 13s
dfm = list_to_df(Qlist)</code></pre>
<pre class="python"><code>#10 s
(p9.ggplot(data = dfm.loc[(dfm.episode &gt; -1)], mapping = p9.aes(x = &#39;episode&#39;, y = &#39;value&#39;))
    + p9.geom_point() 
    + p9.geom_smooth(method = &quot;loess&quot;, color = &quot;yellow&quot;)
    + p9.geom_hline(data = df_true, mapping = p9.aes(yintercept = &#39;value&#39;), color = &#39;red&#39;)
    + p9.facet_wrap(&quot;variable&quot;, scales = &#39;free_y&#39;)
    + p9.theme(subplots_adjust={&#39;wspace&#39;:0.4}) # fix plotting issue
)</code></pre>
<p><img src="/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_60_0.png" /></p>
<p>This looks good, lets zoom in at one of the more noisier Q-values, <code>Q14</code>.
In the learning schedule used, the lowest value for <span class="math inline">\(\alpha\)</span> is <code>0.01</code>.
At this value of the learning rate, there is still considerable variation around the true value.</p>
<pre class="python"><code>(p9.ggplot(data = dfm.loc[(dfm.variable == &#39;Q10&#39;) &amp; (dfm.episode &gt; -1)], mapping = p9.aes(x = &#39;episode&#39;, y = &#39;value&#39;))
    + p9.geom_point() 
    + p9.geom_smooth(method = &quot;loess&quot;, color = &quot;yellow&quot;)
    + p9.geom_hline(data = df_true[df_true.variable == &#39;Q10&#39;], mapping = p9.aes(yintercept = &#39;value&#39;), color = &#39;red&#39;)
)</code></pre>
<p><img src="/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_62_0.png" /></p>
<p>Suppose we did not know the true value of Q(S = 14), and wanted to estimate it using Q-learning. From the plot above, an obvious strategy is to average all the values in the tail of the learning rate schedule, say after episode 400.000.</p>
<pre class="python"><code># drop burn-in, then average Q-vals by variable
Q_est = (dfm.loc[dfm.episode &gt; 4000]
         .groupby([&#39;variable&#39;])
         .mean()
        )

# convert to a 1D array sorted by Q-value
Q_est[&#39;sort_order&#39;] = [int(str.replace(x, &#39;Q&#39;, &#39;&#39;)) \
                       for x in Q_est.index.values]

Q_est = Q_est.sort_values(by=[&#39;sort_order&#39;])

Q_est_as1 = Q_est[&#39;value&#39;].values
</code></pre>
<p>Lets compare these with the final estimated values, and with true values:</p>
<pre class="python"><code>Q_as1_lasttable = [np.max(Q_sarsamax[key]) if key in Q_sarsamax \
             else 0 for key in np.arange(env.nS)]

q_showdown = pd.DataFrame(
    {&#39;q_est_as1&#39;: Q_est_as1,
     &#39;q_est_as1_lasttable&#39;: Q_as1_lasttable,
     &#39;q_true&#39;: V_pi_dc})
q_showdown[&#39;state&#39;] = range(16)

q_showdown = pd.melt(q_showdown, [&#39;state&#39;])

(p9.ggplot(data = q_showdown, mapping = p9.aes(x = &#39;state&#39;, y = &#39;value&#39;, color = &#39;factor(variable)&#39;))
+ p9.geom_point(size = 5, shape = &#39;x&#39;))</code></pre>
<p><img src="/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_66_0.png" /></p>
<p>Pretty close! So here we see Q-learning finally delivering on its convergence promise.</p>
<p>And what about final values vs averaging? Averaging seems to have improved the estimates a bit. Note that we had to choose an averaging window based on eyeballing the learning curves for the separate Q-values.</p>
<p>Can we do even better? A learning rate schedule where alpha is lowered further would diminish the fluctuations around the true values, but at the risk of lowering it too fast and effectively freezing (or very slowly evolving) the Q-values at non-equilibrium values.</p>
</div>
</div>
<div id="decay-learning-rate-schedule-variant-ii" class="section level1">
<h1>decay learning rate schedule variant II</h1>
<p>The second formula decays <span class="math inline">\(\alpha\)</span> to <code>0.001</code>, ten times lower than the previous decay schedule:</p>
<pre class="python"><code># fully random policy

plot_every = 100

alpha_schedule2 = create_alpha_schedule2(num_episodes = 500_000, \
                                        alpha_decay = 0.99997)
n_episodes = len(alpha_schedule2)

fullrun = 0

if fullrun == 1:
    Q_sarsamax, N_sarsamax, avg_scores, Qlist = q_learning(env, num_episodes = n_episodes, \
                                                eps = 1,\
                                                alpha_schedule = alpha_schedule2, \
                                                gamma =  0.95, \
                                                plot_every = plot_every, \
                                                verbose = True)
    
    with open(&#39;cached/as2_Q_sarsamax.pkl&#39;, &#39;wb&#39;) as f:
        dill.dump(Q_sarsamax, f)
    with open(&#39;cached/as2_avg_scores.pkl&#39;, &#39;wb&#39;) as f:
        dill.dump(avg_scores, f)
    with open(&#39;cached/as2_Qlist.pkl&#39;, &#39;wb&#39;) as f:
        dill.dump(Qlist, f)        
else:
    with open(&#39;cached/as2_Q_sarsamax.pkl&#39;, &#39;rb&#39;) as f:
        Q_sarsamax = dill.load(f)
    with open(&#39;cached/as2_avg_scores.pkl&#39;, &#39;rb&#39;) as f:
        avg_scores = dill.load(f)
    with open(&#39;cached/as2_Qlist.pkl&#39;, &#39;rb&#39;) as f:
        Qlist = dill.load(f)     </code></pre>
<pre class="python"><code>dfm = list_to_df(Qlist)</code></pre>
<pre class="python"><code>(p9.ggplot(data = dfm.loc[(dfm.episode &gt; -1)], mapping = p9.aes(x = &#39;episode&#39;, y = &#39;value&#39;))
    + p9.geom_point() 
    + p9.geom_smooth(method = &quot;loess&quot;, color = &quot;yellow&quot;)
    + p9.geom_hline(data = df_true, mapping = p9.aes(yintercept = &#39;value&#39;), color = &#39;red&#39;)
    + p9.facet_wrap(&quot;variable&quot;, scales = &#39;free_y&#39;)
    + p9.theme(subplots_adjust={&#39;wspace&#39;:0.4})
)</code></pre>
<p><img src="/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_70_0.png" /></p>
<p>Cleary, the fluctuations are reduced compared to the previous schedule. AND all Q-values still fluctuate around their true values, so it seems that this schedule is better with respect to finding the true values.</p>
<p>Let’s see if the accuracy of the estimated Q-values is indeed higher:</p>
<pre class="python"><code># drop burn-in, then average Q-vals by variable
Q_est = (dfm.loc[dfm.episode &gt; 2000]
         .groupby([&#39;variable&#39;])
         .mean()
        )
# convert to 1d array sorted by state nr
Q_est[&#39;sort_order&#39;] = [int(str.replace(x, &#39;Q&#39;, &#39;&#39;)) \
                       for x in Q_est.index.values]

Q_est = Q_est.sort_values(by=[&#39;sort_order&#39;])

Q_est_as2 = Q_est[&#39;value&#39;].values

Q_as2_lasttable = [np.max(Q_sarsamax[key]) if key in Q_sarsamax \
             else 0 for key in np.arange(env.nS)]</code></pre>
<pre class="python"><code>q_showdown = pd.DataFrame(
    {&#39;Q_as2_lasttable&#39;: Q_as2_lasttable,
     &#39;q_est_as2&#39;: Q_est_as2,
     &#39;q_true&#39;: V_pi_dc})
q_showdown[&#39;state&#39;] = range(16)

q_showdown = pd.melt(q_showdown, [&#39;state&#39;])

(p9.ggplot(data = q_showdown, mapping = p9.aes(x = &#39;state&#39;, y = &#39;value&#39;, color = &#39;factor(variable)&#39;))
+ p9.geom_point(size = 5, shape = &#39;x&#39;))</code></pre>
<p><img src="/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_73_0.png" /></p>
<p>Boom! Now our <span class="math inline">\(Q^{*}_{a}\)</span> estimates are really getting close to the true values. Clearly, the second learning rate schedule is able to learn the true Q-values compared to the first rate schedule, given the fixed amount of computation, in this case 500.000 episodes each.</p>
<p>Averaging out does not do much anymore, except for states 10 and 14, where it improves the estimates a tiny bit.</p>
</div>
<div id="wrapping-up" class="section level1">
<h1>Wrapping up</h1>
<p>In conclusion, we have seen that the common approach of using Q-learning with a constant learning rate and gradually decreasing the exploration rate, given sensible values and rates, will indeed find the optimal policy. However, this approach does not necessary converge to the true state-values. We have to tune the algorithm exactly the other way around: keep the exploration rate constant and sufficiently high, and decay the learning rate. For sufficiently low learning rates, averaging out the fluctuations does not meaningfully increase accuracy of the learned Q-values.</p>
</div>

    </div>

    





<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/reinforcement-learning/">Reinforcement learning</a>
  
  <a class="badge badge-light" href="/tags/python/">Python</a>
  
  <a class="badge badge-light" href="/tags/openai-gym/">OpenAI Gym</a>
  
  <a class="badge badge-light" href="/tags/ai/">AI</a>
  
</div>



<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=/post/frozenlake-qlearning-convergence/&amp;text=OpenAI%20Gym&amp;#39;s%20FrozenLake:%20Converging%20on%20the%20true%20Q-values" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=/post/frozenlake-qlearning-convergence/&amp;t=OpenAI%20Gym&amp;#39;s%20FrozenLake:%20Converging%20on%20the%20true%20Q-values" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=OpenAI%20Gym&amp;#39;s%20FrozenLake:%20Converging%20on%20the%20true%20Q-values&amp;body=/post/frozenlake-qlearning-convergence/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=/post/frozenlake-qlearning-convergence/&amp;title=OpenAI%20Gym&amp;#39;s%20FrozenLake:%20Converging%20on%20the%20true%20Q-values" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=OpenAI%20Gym&amp;#39;s%20FrozenLake:%20Converging%20on%20the%20true%20Q-values%20/post/frozenlake-qlearning-convergence/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=/post/frozenlake-qlearning-convergence/&amp;title=OpenAI%20Gym&amp;#39;s%20FrozenLake:%20Converging%20on%20the%20true%20Q-values" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>












  






  
  
  
    
  
  
  <div class="media author-card content-widget-hr">
    
      
      <img class="portrait mr-3" src="/authors/admin/avatar_hudb6f5a71508eb5317c72df2dc43608d9_19467_250x250_fill_q90_lanczos_center.jpg" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title"><a href="/">Gertjan Verhoeven</a></h5>
      <h6 class="card-subtitle">Data Scientist / Policy Advisor</h6>
      <p class="card-text">Gertjan Verhoeven is a research scientist currently at the Dutch Healthcare Authority, working on health policy and statistical methods. Follow me on <a href="https://twitter.com/GertjanVerhoev1">Twitter</a> to receive updates on new blog posts. Statistics posts using R are featured on <a href="https://www.r-bloggers.com">R-Bloggers</a>.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="/#contact" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/GertjanVerhoev1" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.nl/citations?user=B-tEtToAAAAJ&amp;hl=nl" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/gsverhoeven" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://stackoverflow.com/users/10628316/gertjan-verhoeven" target="_blank" rel="noopener">
        <i class="fab fa-stack-overflow"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>









  
  
  <div class="article-widget content-widget-hr">
    <h3>Related</h3>
    <ul>
      
      <li><a href="/post/jacks-car-rental-gym/">Jacks Car Rental as a Gym Environment</a></li>
      
    </ul>
  </div>
  



  </div>
</article>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.4.3/mermaid.min.js" integrity="" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js" integrity="sha256-1zu+3BnLYV9LdiY85uXMzii3bdrkelyp37e0ZyTAQh0=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/r.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/python.min.js"></script>
        
      

    

    
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.a0d331bcd05dbe8b31e244f796710f08.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    © 2019-2022 &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
