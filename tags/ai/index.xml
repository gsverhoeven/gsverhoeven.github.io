<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI | Gertjan Verhoeven</title>
    <link>/tags/ai/</link>
      <atom:link href="/tags/ai/index.xml" rel="self" type="application/rss+xml" />
    <description>AI</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2019, 2020</copyright><lastBuildDate>Sun, 07 Mar 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>AI</title>
      <link>/tags/ai/</link>
    </image>
    
    <item>
      <title>OpenAI Gym&#39;s FrozenLake: Converging on the true Q-values</title>
      <link>/post/frozenlake-qlearning-convergence/</link>
      <pubDate>Sun, 07 Mar 2021 00:00:00 +0000</pubDate>
      <guid>/post/frozenlake-qlearning-convergence/</guid>
      <description>


&lt;p&gt;(Photo by Ryan Fishel on Unsplash)&lt;/p&gt;
&lt;p&gt;This blog post concerns a famous “toy” problem in Reinforcement Learning, the &lt;a href=&#34;https://gym.openai.com/envs/FrozenLake-v0/&#34;&gt;FrozenLake environment&lt;/a&gt;. We compare solving an environment with RL by reaching &lt;strong&gt;maximum performance&lt;/strong&gt; versus obtaining the &lt;strong&gt;true state-action values&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(Q_{s,a}\)&lt;/span&gt;. In doing so I learned a lot about RL as well as about Python (such as the existence of a &lt;code&gt;ggplot&lt;/code&gt; clone for Python, &lt;code&gt;plotnine&lt;/code&gt;, see this blog post for some cool examples).&lt;/p&gt;
&lt;p&gt;FrozenLake was created by OpenAI in 2016 as part of their Gym python package for Reinforcement Learning. Nowadays, the interwebs is full of tutorials how to “solve” FrozenLake. Most of them focus on performance in terms of episodic reward. As soon as this maxes out the algorithm is often said to have converged. For example, in this &lt;a href=&#34;https://stats.stackexchange.com/questions/206944/how-do-i-know-when-a-q-learning-algorithm-converges&#34;&gt;question on Cross-Validated&lt;/a&gt; about Convergence and Q-learning:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;In practice, a reinforcement learning algorithm is considered to converge when the learning curve gets flat and no longer increases.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Now, for &lt;strong&gt;Q-learning&lt;/strong&gt; it has been proven that, &lt;em&gt;under certain conditions&lt;/em&gt;, the Q-values convergence to their true values. But does this happens when the performance maxes out? In this blog we’ll see that this is not generally the case.&lt;/p&gt;
&lt;p&gt;We start with obtaining the true, exact state-action values. For this we use Dynamic Programming (DP). Having implemented Dynamic Programming (DP) for the FrozenLake environment as an exercise notebook already (created by Udacity, go check them out), this was a convenient starting point.&lt;/p&gt;
&lt;div id=&#34;loading-the-packages-and-modules&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Loading the packages and modules&lt;/h2&gt;
&lt;p&gt;We need various Python packages and modules.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# data science packages
import numpy as np
import pandas as pd
import plotnine as p9
import matplotlib.pyplot as plt
%matplotlib inline

# RL algorithms
from qlearning import *
from dp import *

# utils
from helpers import *
from plot_utils import plot_values
import copy
import dill

from frozenlake import FrozenLakeEnv

env = FrozenLakeEnv(is_slippery = True)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;frozen-lake-environment-description&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Frozen Lake Environment description&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake (G).
The water is mostly frozen (F), but there are a few holes (H) where the ice has melted.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-03-07-qlearning_frozenlake_convergence_files/Frozen-Lake.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;If you step into one of those holes, you’ll fall into the freezing water. At this time, there’s an international frisbee shortage, so it’s absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won’t always move in the direction you intend.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The episode ends when you reach the goal or fall in a hole. You receive a reward of 1 if you reach the goal, and zero otherwise.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The agent moves through a &lt;span class=&#34;math inline&#34;&gt;\(4 \times 4\)&lt;/span&gt; gridworld, with states numbered as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[[ 0  1  2  3]
 [ 4  5  6  7]
 [ 8  9 10 11]
 [12 13 14 15]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and the agent has 4 potential actions:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;LEFT = 0
DOWN = 1
RIGHT = 2
UP = 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The FrozenLake Gym environment has been amended to make the one-step dynamics accessible to the agent. For example, if we are in State &lt;code&gt;s = 1&lt;/code&gt; and we perform action &lt;code&gt;a = 0&lt;/code&gt;, the probabilities of ending up in new states, including the associated rewards are contained in the &lt;span class=&#34;math inline&#34;&gt;\(P_{s,a}\)&lt;/span&gt; array:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;env.P[1][0]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[(0.3333333333333333, 1, 0.0, False),
 (0.3333333333333333, 0, 0.0, False),
 (0.3333333333333333, 5, 0.0, True)]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The FrozenLake environment is highly stochastic, with a very sparse reward: only when the agent reaches the goal, a reward of &lt;code&gt;+1&lt;/code&gt; is obtained. This means that if we do not set a discount rate, the agent can keep on wandering around without receiving a learning “signal” that can be propagated back through the preceding state-actions (since falling into the holes does not result in a negative reward) and thus learning very slowly. We will focus on the discounting case (&lt;code&gt;gamma = 0.95&lt;/code&gt;) for this reason (less computation needed for convergence), but compare also with the undiscounted case.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;solving-frozen-lake-using-dp&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Solving Frozen Lake using DP&lt;/h2&gt;
&lt;p&gt;Let us solve FrozenLake first for the no discounting case (&lt;code&gt;gamma = 1&lt;/code&gt;).
The Q-value for the first state will then tell us the average episodic reward, which for FrozenLake translates into the percentage of episodes in which the Agent succesfully reaches its goal.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;policy_pi, V_pi = policy_iteration(env, gamma = 1, theta=1e-9, \
                                   verbose = False)


plot_values(V_pi)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_9_1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The Q-value for state &lt;code&gt;s = 0&lt;/code&gt; is &lt;code&gt;0.824&lt;/code&gt;. This means that for &lt;code&gt;gamma = 1&lt;/code&gt; and following an optimal policy &lt;span class=&#34;math inline&#34;&gt;\(\pi^*\)&lt;/span&gt;, 82.4% of all episodes ends in succes.&lt;/p&gt;
&lt;p&gt;As already mentioned above, for computational reasons we will apply Q-learning to the environment with &lt;code&gt;gamma = 0.95&lt;/code&gt;. So… lets solve FrozenLake for &lt;code&gt;gamma = 0.95&lt;/code&gt; as well:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# obtain the optimal policy and optimal state-value function
policy_pi_dc, V_pi_dc = policy_iteration(env, gamma = 0.95, theta=1e-9, \
                                   verbose = False)

Q_perfect = create_q_dict_from_v(env, V_pi_dc, gamma = 0.95)

df_true = convert_vals_to_pd(V_pi_dc)

plot_values(V_pi_dc)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_11_1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now, comparing the optimal policies for &lt;code&gt;gamma = 0.95&lt;/code&gt; and for &lt;code&gt;gamma = 1&lt;/code&gt; (not shown here) we find that they are not the same. Therefore, 82.4% succes rate is likely an upper bound for &lt;code&gt;gamma = 0.95&lt;/code&gt;, since introducing discounting in this stochastic environment can (intuitively) either have no effect on the optimal policy, or favor more risky (lower succes rate) but faster (less discounting) policies, leading to a lower overall succes rate.&lt;/p&gt;
&lt;p&gt;For example, for the undiscounted case, the Agent is indifferent in choosing direction in the first state. If it ends up going &lt;strong&gt;right&lt;/strong&gt; , it can choose UP and wander around at the top row at no cost until it reaches the starting state again. As soon as this wandering around gets a cost by discounting we see (not shown) that the Agent is no longer indifferent, and does NOT want to end up wandering in the top row, but chooses to play LEFT in state &lt;code&gt;s = 0&lt;/code&gt; instead.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;checking-the-performance-of-an-optimal-greedy-policy-based-on-perfect-q-values&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Checking the performance of an optimal greedy policy based on perfect Q-values&lt;/h1&gt;
&lt;p&gt;Now that we have the &lt;span class=&#34;math inline&#34;&gt;\(Q_{s,a}\)&lt;/span&gt; values corresponding to the optimal policy given that &lt;code&gt;gamma = 0.95&lt;/code&gt;, we can check its performance. To do so, we use brute force and simulate the average reward under the optimal policy for a large number of episodes.&lt;/p&gt;
&lt;p&gt;To do so, I wrote a function &lt;code&gt;test_performance()&lt;/code&gt; by taking the &lt;code&gt;q_learning()&lt;/code&gt; function, removing the learning (Q-updating) part and setting epsilon to zero when selecting an action (i.e. a pure greedy policy based on a given Q-table).&lt;/p&gt;
&lt;p&gt;Playing around with the binomial density in &lt;code&gt;R&lt;/code&gt; (&lt;code&gt;summary(rbinom(1e5, 1e5, prob = 0.8)/1e5&lt;/code&gt;) tells me that choosing 100.000 episodes gives a precision of around three decimals in estimating the probability of succes. This is good enough for me.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Obtain Q for Gamma = 0.95 and convert to defaultdict 
Q_perfect = create_q_dict_from_v(env, V_pi_dc, gamma = 0.95)

fullrun = 0

if fullrun == 1:
    d = []
    runs = 100
    for i in range(runs):
        avg_scores = test_performance(Q_perfect, env, num_episodes = 1000, \
                                   plot_every = 1000, verbose = False)
        d.append({&amp;#39;run&amp;#39;: i,
                 &amp;#39;avg_score&amp;#39;: np.mean(avg_scores)})
        print(&amp;quot;\ri {}/{}&amp;quot;.format(i, runs), end=&amp;quot;&amp;quot;)
        sys.stdout.flush() 

    d_perfect = pd.DataFrame(d)
    d_perfect.to_pickle(&amp;#39;cached/scores_perfect_0.95.pkl&amp;#39;)
else:
    d_perfect = pd.read_pickle(&amp;#39;cached/scores_perfect_0.95.pkl&amp;#39;)

&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;round(np.mean(d_perfect.avg_score), 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.781&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thus, we find that with the true optimal policy for &lt;code&gt;gamma = 0.95&lt;/code&gt;, in the long run 78% of all episodes is succesful. This is therefore the expected plateau value for the learning curve in Q-learning, provided that the exploration rate has become sufficiently small.&lt;/p&gt;
&lt;p&gt;Let’s move on to Q-learning and convergence.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;solving-frozenlake-using-q-learning&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;“Solving” FrozenLake using Q-learning&lt;/h1&gt;
&lt;p&gt;The typical RL tutorial approach to solve a simple MDP as FrozenLake is to choose a constant learning rate, not too high, not too low, say &lt;span class=&#34;math inline&#34;&gt;\(\alpha = 0.1\)&lt;/span&gt;. Then, the exploration parameter &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; starts at 1 and is gradually reduced to a floor value of say &lt;span class=&#34;math inline&#34;&gt;\(\epsilon = 0.0001\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Lets solve FrozenLake this way, monitoring the learning curve (average reward per episode) as it learns, and compare the learned Q-values with the true Q-values found using DP.&lt;/p&gt;
&lt;p&gt;I wrote Python functions that generate a &lt;strong&gt;decay schedule&lt;/strong&gt;, a 1D numpy array of &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; values, with length equal to the total number of episode the Q-learning algorithm is to run. This array is passed on to the Q-learning algorithm, and used during learning.&lt;/p&gt;
&lt;p&gt;It is helpful to visualize the decay schedule of &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; to check that it is reasonable before we start to use them with our Q-learning algorithm. I played around with the decay rate until the “elbow” of the curve is around 20% of the number of episodes, and reaches the desired minimal end value (&lt;span class=&#34;math inline&#34;&gt;\(\epsilon = 0.0001\)&lt;/span&gt;).&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def create_epsilon_schedule(num_episodes,  \
                       epsilon_start=1.0, epsilon_decay=.9999, epsilon_min=1e-4):
    x = np.arange(num_episodes)+1
    y = np.full(num_episodes, epsilon_start)
    y = np.maximum((epsilon_decay**x)*epsilon_start, epsilon_min)
    return y

epsilon_schedule = create_epsilon_schedule(100_000)

plot_schedule(epsilon_schedule, ylab = &amp;#39;Epsilon&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_20_0.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;My version of the Q-learning algorithm has slowly evolved to include more and more lists of things to monitor during the execution of the algorithm. Every 100 episodes, a copy of &lt;span class=&#34;math inline&#34;&gt;\(Q^{*}_{s}\)&lt;/span&gt; and of &lt;span class=&#34;math inline&#34;&gt;\(Q_{s, a}\)&lt;/span&gt; is stored in a list, as well as the average reward over this episode, as well as the final &lt;span class=&#34;math inline&#34;&gt;\(Q_{s,a}\)&lt;/span&gt; table, and &lt;span class=&#34;math inline&#34;&gt;\(N_{s,a}\)&lt;/span&gt; that kept track of how often each state-action was chosen. The &lt;code&gt;dill&lt;/code&gt; package is used to store these datastructures on disk to avoid the need to rerun the algorithm every time the notebook is run.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# K / N + K decay learning rate schedule
# fully random policy

plot_every = 100

n_episodes = len(epsilon_schedule)

fullrun = 0

random.seed(123)
np.random.seed(123)

if fullrun == 1:
    Q_sarsamax, N_sarsamax, avg_scores, Qlist, Qtable_list = q_learning(env, num_episodes = n_episodes, \
                                                eps_schedule = epsilon_schedule,\
                                                alpha = 0.1, \
                                                gamma =  0.95, \
                                                plot_every = plot_every, \
                                                verbose = True, log_full = True)
    
    with open(&amp;#39;cached/es1_Q_sarsamax.pkl&amp;#39;, &amp;#39;wb&amp;#39;) as f:
        dill.dump(Q_sarsamax, f)
    with open(&amp;#39;cached/es1_avg_scores.pkl&amp;#39;, &amp;#39;wb&amp;#39;) as f:
        dill.dump(avg_scores, f)
    with open(&amp;#39;cached/es1_Qlist.pkl&amp;#39;, &amp;#39;wb&amp;#39;) as f:
        dill.dump(Qlist, f)   
    with open(&amp;#39;cached/es1_Qtable_list.pkl&amp;#39;, &amp;#39;wb&amp;#39;) as f:
        dill.dump(Qtable_list, f)           
else:
    with open(&amp;#39;cached/es1_Q_sarsamax.pkl&amp;#39;, &amp;#39;rb&amp;#39;) as f:
        Q_sarsamax = dill.load(f)
    with open(&amp;#39;cached/es1_avg_scores.pkl&amp;#39;, &amp;#39;rb&amp;#39;) as f:
        avg_scores = dill.load(f)
    with open(&amp;#39;cached/es1_Qlist.pkl&amp;#39;, &amp;#39;rb&amp;#39;) as f:
        Qlist = dill.load(f)   
    with open(&amp;#39;cached/es1_Qtable_list.pkl&amp;#39;, &amp;#39;rb&amp;#39;) as f:
        Qtable_list = dill.load(f)            

Q_es1_lasttable = [np.max(Q_sarsamax[key]) if key in Q_sarsamax \
     else 0 for key in np.arange(env.nS)]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lets plot the learning curve. Here &lt;code&gt;plotnine&lt;/code&gt;, a &lt;code&gt;ggplot&lt;/code&gt; clone in Python comes in handy.&lt;/p&gt;
&lt;p&gt;As we can see below, the “recipe” for solving FrozenLake has worked really well. The displayed red line gives the theoretical optimal performance for &lt;code&gt;gamma = 0.95&lt;/code&gt;, I used a &lt;code&gt;loess&lt;/code&gt; smoother so we can more easily compare the Q-learning results with the theoretical optimum.&lt;/p&gt;
&lt;p&gt;We can see that the Q-learning algorithm has indeed found a policy that performs optimal. This appears to happen at around 60.000 episodes. We return to this later.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# plot performance
df_scores = pd.DataFrame(
    {&amp;#39;episode_nr&amp;#39;: np.linspace(0,n_episodes,len(avg_scores),endpoint=False),
     &amp;#39;avg_score&amp;#39;: np.asarray(avg_scores)})

(p9.ggplot(data = df_scores.loc[df_scores.episode_nr &amp;gt; -1], mapping = p9.aes(x = &amp;#39;episode_nr&amp;#39;, y = &amp;#39;avg_score&amp;#39;))
    + p9.geom_point(colour = &amp;#39;gray&amp;#39;)
    + p9.geom_smooth(method = &amp;#39;loess&amp;#39;)
    + p9.geom_hline(yintercept = 0.781, colour = &amp;quot;red&amp;quot;)
    + p9.geom_vline(xintercept = 60_000, color = &amp;quot;blue&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_24_0.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;But now comes the big question: did the &lt;span class=&#34;math inline&#34;&gt;\(Q^{*}_{a}\)&lt;/span&gt; estimates converge onto the true values as well?&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;q_showdown = pd.DataFrame(
    {&amp;#39;Q_es1_lasttable&amp;#39;: Q_es1_lasttable,
     &amp;#39;q_true&amp;#39;: V_pi_dc})
q_showdown[&amp;#39;state&amp;#39;] = range(16)

q_showdown = pd.melt(q_showdown, [&amp;#39;state&amp;#39;])

(p9.ggplot(data = q_showdown, mapping = p9.aes(x = &amp;#39;state&amp;#39;, y = &amp;#39;value&amp;#39;, color = &amp;#39;factor(variable)&amp;#39;))
+ p9.geom_point(size = 5, shape = &amp;#39;x&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_26_0.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Nope, they did not. Ok, most learned Q-values are close to their true values, but they clearly did not converge exactly to their true value.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;plotting-the-learning-curves-for-all-the-state-action-values&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Plotting the learning curves for all the state-action values&lt;/h1&gt;
&lt;p&gt;To really understand what is going on, I found it helpful to plot the learning curves for all the 16 x 4 = 64 state-action values at the same time. Here &lt;code&gt;plotnine&lt;/code&gt; really shines. I leave out the actual estimates, and only plot a &lt;code&gt;loess&lt;/code&gt; smoothed curve for each of the state-action values over time.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# convert to pandas df
dfm = list_of_tables_to_df(Qtable_list)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;#10 s
(p9.ggplot(data = dfm.loc[(dfm.episode &amp;gt; -1)], mapping = p9.aes(x = &amp;#39;episode&amp;#39;, y = &amp;#39;value&amp;#39;, group = &amp;#39;action&amp;#39;, color = &amp;#39;factor(action)&amp;#39;))
    #+ p9.geom_point(shape = &amp;#39;x&amp;#39;) 
    + p9.geom_smooth(method = &amp;#39;loess&amp;#39;)
    + p9.geom_hline(data = df_true, mapping = p9.aes(yintercept = &amp;#39;value&amp;#39;), color = &amp;#39;red&amp;#39;)
    + p9.geom_vline(xintercept = 600, color = &amp;#39;blue&amp;#39;)
    + p9.facet_wrap(&amp;quot;variable&amp;quot;, ncol = 4)
    + p9.theme(subplots_adjust={&amp;#39;wspace&amp;#39;:0.4}) # fix a displaying issue
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_30_0.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;First the main question: what about convergence of &lt;span class=&#34;math inline&#34;&gt;\(Q^{*}_{a}\)&lt;/span&gt;? For this we need only to look, for each state, at the action with the highest value, and compare that to the true value (red horizontal lines). Most of the values appear to have converged to a value close to the true value, but Q3 is clearly still way off. Note that we using smoothing here to average out the fluctuations around the true value.&lt;/p&gt;
&lt;p&gt;We noted earlier that at around episode 60.000, the optimal policy emerges and the learning curve becomes flat. Now, the most obvious reason why performance increases is because the value of &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; is decaying, so the deleterious effects of exploration should go down, and performance should go up.&lt;/p&gt;
&lt;p&gt;Another reason that performance goes up could be that the greedy policy is improving. It is interesting to examine whether at this point, meaningfull changes in the greedy policy still occur. Meaningfull changes in policy are caused by changes in the estimated state-action values. For example, we might expect two or more state-action value lines crossing, with the “right” action becoming dominant over the “wrong” action. Is this indeed the case?&lt;/p&gt;
&lt;p&gt;Indeed, from the plot above, with the change point at around episode 600 (x 100),a change occurs in Q6, where the actions 0 and 2 cross. However, from the true Q-value table (not shown) we see that both actions are equally rewarding, so the crossing has no effect on performance. Note that only one of the two converges to the true value, because the exploration rate becomes so low that learning for the other action almost completely stops in the end.&lt;/p&gt;
&lt;p&gt;lets zoom in then at the states that have low expected reward, Q0, Q1, Q2, and Q3. These are difficult to examine in the plot above, and have actions with expected rewards that are similar and therefore more difficult to resolve (BUT: since the difference in expected reward is small, the effect of resolving them on the overall performance is small as well). For these we plot the actual state-actions values:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;(p9.ggplot(data = dfm[dfm.variable.isin([&amp;#39;Q0&amp;#39;, &amp;#39;Q1&amp;#39;,&amp;#39;Q2&amp;#39;, &amp;#39;Q3&amp;#39;])], mapping = p9.aes(x = &amp;#39;episode&amp;#39;, y = &amp;#39;value&amp;#39;, group = &amp;#39;action&amp;#39;, color = &amp;#39;factor(action)&amp;#39;))
    + p9.geom_point(shape = &amp;#39;x&amp;#39;) 
    #+ p9.geom_smooth(method = &amp;#39;loess&amp;#39;)
    + p9.geom_hline(data = df_true[df_true.variable.isin([&amp;#39;Q0&amp;#39;, &amp;#39;Q1&amp;#39;,&amp;#39;Q2&amp;#39;, &amp;#39;Q3&amp;#39;])], mapping = p9.aes(yintercept = &amp;#39;value&amp;#39;), color = &amp;#39;red&amp;#39;)
    + p9.geom_vline(xintercept = 600, color = &amp;#39;blue&amp;#39;)
    + p9.facet_wrap(&amp;quot;variable&amp;quot;, scales = &amp;#39;free_y&amp;#39;)
    + p9.theme(subplots_adjust={&amp;#39;wspace&amp;#39;:0.15})
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_33_0.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From Q1, Q2 and Q3, we can see that exploration really goes down at around episode 500 (x 100) (&lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; at this point is &lt;code&gt;0.007&lt;/code&gt;), and with the optimal action standing out already long before reaching this point.&lt;/p&gt;
&lt;p&gt;Only with Q2 there is a portion of the learning curve where action 1 has the highest value, and is chosen for quite some time before switching back to action 0 again at around episode 60.000. Let’s compare with the true values for Q2:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;Q_perfect[2]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([0.15347714, 0.14684971, 0.14644451, 0.13958106])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, the difference in expected reward between the actions in state 2 is really small, and because of the increasingly greedy action selection, only action 0 converges to its true values, with the other values “frozen” in place because of the low exploration rate.&lt;/p&gt;
&lt;p&gt;Now, after analyzing what happens if we apply the “cookbook” approach to solving problems using RL, let’s change our attention to getting convergence for preferably ALL the state-action values.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;q-learning-theoretical-sufficient-conditions-for-convergence&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Q-learning: Theoretical sufficient conditions for convergence&lt;/h1&gt;
&lt;p&gt;According to our RL bible (Sutton &amp;amp; Barto), to obtain &lt;strong&gt;exact&lt;/strong&gt; convergence, we need two conditions to hold.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The first is that all states continue to be visited, and&lt;/li&gt;
&lt;li&gt;the second is that the learning rate goes to zero.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;continuous-exploration-visiting-all-the-states&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Continuous exploration: visiting all the states&lt;/h2&gt;
&lt;p&gt;The nice thing of Q-learning is that it is an &lt;strong&gt;off-policy&lt;/strong&gt; learning algorithm. This means that no matter what the actual policy is used to explore the states, the Q-values we learn correspond to the expected reward when following the &lt;strong&gt;optimal policy&lt;/strong&gt;. Which is quite miraculous if you ask me.&lt;/p&gt;
&lt;p&gt;Now, our (admittedly, a bit academic) goal is getting &lt;strong&gt;all&lt;/strong&gt; of the learned state values &lt;strong&gt;as close as possible&lt;/strong&gt; to the true values. An epsilon-greedy policy with a low &lt;span class=&#34;math inline&#34;&gt;\(epsilon\)&lt;/span&gt; would spent a lot of time by choosing state-actions that are on the optimal path between start and goal state, and would only rarely visit low value states, or choose low value state-actions.&lt;/p&gt;
&lt;p&gt;Because we can choose any policy we like, I chose a completely &lt;strong&gt;random&lt;/strong&gt; policy. This way, the Agent is more likely to end up in low value states and estimate the Q-values of those state-actions accurately as well.&lt;/p&gt;
&lt;p&gt;Note that since theFrozen Lake enviroment has a lot of inherent randomness already because of the ice being slippery, a policy with a low &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; (most of the time exploiting and only rarely exploring) will still bring the agent in low values states, but this would require much more episodes.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;convergence-and-learning-rate-schedules&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Convergence and learning rate schedules&lt;/h2&gt;
&lt;p&gt;For a particular constant learning rate, provided that it is not too high, the estimated Q-values will converge to a situation where they start to fluctuate around their true values.
If we subsequently lower the learning rate, the scale of the fluctuations (their &lt;strong&gt;variance&lt;/strong&gt;) will decrease.
If the learning rate is gradually decayed to zero, the estimates will converge.
According to Sutton &amp;amp; Barto (2018), the &lt;strong&gt;decay schedule&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; must obey two constraints to assure convergence: (p 33).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;sum of increments must go to infinity&lt;/li&gt;
&lt;li&gt;sum of the square of increments must go to zero&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;On the interwebs, I found two formulas that are often used to decay hyperparameters of Reinforcement Learning algorithms:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\alpha_n = K / (K + n)\)&lt;/span&gt; (Eq. 1)&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\alpha_n = \Sigma^{\infty}_{n = 1} \delta^n \alpha_0\)&lt;/span&gt; (Eq. 2)&lt;/p&gt;
&lt;p&gt;Again, just as with the decay schedule for &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;, it is helpful to visualize the decay schedules to check that they are reasonable before we start to use them with our Q-learning algorithm.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Eq 1
def create_alpha_schedule(num_episodes, \
                     alpha_K = 100, alpha_min = 1e-3):
    x = np.arange(num_episodes)+1
    y = np.maximum(alpha_K/(x + alpha_K), alpha_min)
    return y

# Eq 2
def create_alpha_schedule2(num_episodes,  \
                       alpha_start=1.0, alpha_decay=.999, alpha_min=1e-3):
    x = np.arange(num_episodes)+1
    y = np.full(num_episodes, alpha_start)
    y = np.maximum((alpha_decay**x)*alpha_start, alpha_min)
    return y

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I played a bit with the shape parameter to get a curve with the “elbow” around 20% of the episodes.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;alpha_schedule = create_alpha_schedule(num_episodes = 500_000, \
                                       alpha_K = 5_000)

plot_schedule(alpha_schedule, ylab = &amp;#39;Alpha&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_44_0.png&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;round(min(alpha_schedule), 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.01&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This curve decays &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; in 500.000 episodes to &lt;code&gt;0.01&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The second formula (Equation 2) decays &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; even further, to &lt;code&gt;0.001&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;alpha_schedule2 = create_alpha_schedule2(num_episodes = 500_000, \
                                        alpha_decay = 0.99997)

plot_schedule(alpha_schedule2, ylab = &amp;#39;Alpha&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_47_0.png&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;min(alpha_schedule2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.001&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;running-the-q-learning-algorithm-for-different-learning-rate-schedules&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Running the Q-learning algorithm for different learning rate schedules&lt;/h1&gt;
&lt;p&gt;We start with the decay function that follows Equation 1. To get a full random policy, we set &lt;span class=&#34;math inline&#34;&gt;\(\epsilon = 1\)&lt;/span&gt;. Note that this gives awful performance where the learning curve suggests it is hardly learning anything at all. However, wait until we try out a fully exploiting policy on the Q-value table learned during this run!&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# K / N + K decay learning rate schedule
# fully random policy

plot_every = 100

alpha_schedule = create_alpha_schedule(num_episodes = 500_000, \
                                       alpha_K = 5_000)

n_episodes = len(alpha_schedule)

fullrun = 0

if fullrun == 1:
    Q_sarsamax, N_sarsamax, avg_scores, Qlist = q_learning(env, num_episodes = n_episodes, \
                                                eps = 1,\
                                                alpha_schedule = alpha_schedule, \
                                                gamma =  0.95, \
                                                plot_every = plot_every, \
                                                verbose = True)
    
    with open(&amp;#39;cached/as1_Q_sarsamax.pkl&amp;#39;, &amp;#39;wb&amp;#39;) as f:
        dill.dump(Q_sarsamax, f)
    with open(&amp;#39;cached/as1_avg_scores.pkl&amp;#39;, &amp;#39;wb&amp;#39;) as f:
        dill.dump(avg_scores, f)
    with open(&amp;#39;cached/as1_Qlist.pkl&amp;#39;, &amp;#39;wb&amp;#39;) as f:
        dill.dump(Qlist, f)        
else:
    with open(&amp;#39;cached/as1_Q_sarsamax.pkl&amp;#39;, &amp;#39;rb&amp;#39;) as f:
        Q_sarsamax = dill.load(f)
    with open(&amp;#39;cached/as1_avg_scores.pkl&amp;#39;, &amp;#39;rb&amp;#39;) as f:
        avg_scores = dill.load(f)
    with open(&amp;#39;cached/as1_Qlist.pkl&amp;#39;, &amp;#39;rb&amp;#39;) as f:
        Qlist = dill.load(f)      

Q_as1_lasttable = [np.max(Q_sarsamax[key]) if key in Q_sarsamax \
     else 0 for key in np.arange(env.nS)]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is the learning curve for this run of Q-learning:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# plot performance
plt.plot(np.linspace(0,n_episodes,len(avg_scores),endpoint=False), np.asarray(avg_scores))
plt.xlabel(&amp;#39;Episode Number&amp;#39;)
plt.ylabel(&amp;#39;Average Reward (Over Next %d Episodes)&amp;#39; % plot_every)
plt.show()

print((&amp;#39;Best Average Reward over %d Episodes: &amp;#39; % plot_every), np.max(avg_scores))    
 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_53_0.png&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Best Average Reward over 100 Episodes:  0.08&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Pretty awfull huh? Now let us check out the performance of the learned Q-table:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fullrun = 0
runs = 100

if fullrun == 1:
    d_q = []
    for i in range(runs):
        avg_scores = test_performance(Q_sarsamax, env, num_episodes = 1000, \
                                   plot_every = 1000, verbose = False)
        d_q.append({&amp;#39;run&amp;#39;: i,
                 &amp;#39;avg_score&amp;#39;: np.mean(avg_scores)})  
        print(&amp;quot;\ri = {}/{}&amp;quot;.format(i+1, runs), end=&amp;quot;&amp;quot;)
        sys.stdout.flush() 
        
    d_qlearn = pd.DataFrame(d_q)
    d_qlearn.to_pickle(&amp;#39;cached/scores_qlearn_0.95.pkl&amp;#39;)
else:
    d_qlearn = pd.read_pickle(&amp;#39;cached/scores_qlearn_0.95.pkl&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;round(np.mean(d_qlearn.avg_score), 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.778&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Bam! Equal to the performance of the optimal policy found using Dynamic programming (sampling error (2x SD) is +/- 0.008). The random policy has actual learned Q-values that for a greedy policy result in optimal performance!&lt;/p&gt;
&lt;div id=&#34;but-what-about-convergence&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;But what about convergence?&lt;/h2&gt;
&lt;p&gt;Ok, so Q-learning found an optimal policy. But did it converge?
Our &lt;code&gt;q_learning()&lt;/code&gt; function made a list of Q-tables while learning, adding a new table every 100 episodes. This gives us 5.000 datapoints for each Q-value, which we can plot to visually check for convergence.&lt;/p&gt;
&lt;p&gt;As with the list of state-action tables above, It takes some datawrangling to get the list of Q-tables in a nice long pandas DataFrame suitable for plotting. This is hidden away in the &lt;code&gt;list_to_df()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# 13s
dfm = list_to_df(Qlist)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;#10 s
(p9.ggplot(data = dfm.loc[(dfm.episode &amp;gt; -1)], mapping = p9.aes(x = &amp;#39;episode&amp;#39;, y = &amp;#39;value&amp;#39;))
    + p9.geom_point() 
    + p9.geom_smooth(method = &amp;quot;loess&amp;quot;, color = &amp;quot;yellow&amp;quot;)
    + p9.geom_hline(data = df_true, mapping = p9.aes(yintercept = &amp;#39;value&amp;#39;), color = &amp;#39;red&amp;#39;)
    + p9.facet_wrap(&amp;quot;variable&amp;quot;, scales = &amp;#39;free_y&amp;#39;)
    + p9.theme(subplots_adjust={&amp;#39;wspace&amp;#39;:0.4}) # fix plotting issue
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_60_0.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This looks good, lets zoom in at one of the more noisier Q-values, &lt;code&gt;Q14&lt;/code&gt;.
In the learning schedule used, the lowest value for &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; is &lt;code&gt;0.01&lt;/code&gt;.
At this value of the learning rate, there is still considerable variation around the true value.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;(p9.ggplot(data = dfm.loc[(dfm.variable == &amp;#39;Q10&amp;#39;) &amp;amp; (dfm.episode &amp;gt; -1)], mapping = p9.aes(x = &amp;#39;episode&amp;#39;, y = &amp;#39;value&amp;#39;))
    + p9.geom_point() 
    + p9.geom_smooth(method = &amp;quot;loess&amp;quot;, color = &amp;quot;yellow&amp;quot;)
    + p9.geom_hline(data = df_true[df_true.variable == &amp;#39;Q10&amp;#39;], mapping = p9.aes(yintercept = &amp;#39;value&amp;#39;), color = &amp;#39;red&amp;#39;)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_62_0.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Suppose we did not know the true value of Q(S = 14), and wanted to estimate it using Q-learning. From the plot above, an obvious strategy is to average all the values in the tail of the learning rate schedule, say after episode 400.000.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# drop burn-in, then average Q-vals by variable
Q_est = (dfm.loc[dfm.episode &amp;gt; 4000]
         .groupby([&amp;#39;variable&amp;#39;])
         .mean()
        )

# convert to a 1D array sorted by Q-value
Q_est[&amp;#39;sort_order&amp;#39;] = [int(str.replace(x, &amp;#39;Q&amp;#39;, &amp;#39;&amp;#39;)) \
                       for x in Q_est.index.values]

Q_est = Q_est.sort_values(by=[&amp;#39;sort_order&amp;#39;])

Q_est_as1 = Q_est[&amp;#39;value&amp;#39;].values
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lets compare these with the final estimated values, and with true values:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;Q_as1_lasttable = [np.max(Q_sarsamax[key]) if key in Q_sarsamax \
             else 0 for key in np.arange(env.nS)]

q_showdown = pd.DataFrame(
    {&amp;#39;q_est_as1&amp;#39;: Q_est_as1,
     &amp;#39;q_est_as1_lasttable&amp;#39;: Q_as1_lasttable,
     &amp;#39;q_true&amp;#39;: V_pi_dc})
q_showdown[&amp;#39;state&amp;#39;] = range(16)

q_showdown = pd.melt(q_showdown, [&amp;#39;state&amp;#39;])

(p9.ggplot(data = q_showdown, mapping = p9.aes(x = &amp;#39;state&amp;#39;, y = &amp;#39;value&amp;#39;, color = &amp;#39;factor(variable)&amp;#39;))
+ p9.geom_point(size = 5, shape = &amp;#39;x&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_66_0.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Pretty close! So here we see Q-learning finally delivering on its convergence promise.&lt;/p&gt;
&lt;p&gt;And what about final values vs averaging? Averaging seems to have improved the estimates a bit. Note that we had to choose an averaging window based on eyeballing the learning curves for the separate Q-values.&lt;/p&gt;
&lt;p&gt;Can we do even better? A learning rate schedule where alpha is lowered further would diminish the fluctuations around the true values, but at the risk of lowering it too fast and effectively freezing (or very slowly evolving) the Q-values at non-equilibrium values.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;decay-learning-rate-schedule-variant-ii&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;decay learning rate schedule variant II&lt;/h1&gt;
&lt;p&gt;The second formula decays &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; to &lt;code&gt;0.001&lt;/code&gt;, ten times lower than the previous decay schedule:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# fully random policy

plot_every = 100

alpha_schedule2 = create_alpha_schedule2(num_episodes = 500_000, \
                                        alpha_decay = 0.99997)
n_episodes = len(alpha_schedule2)

fullrun = 0

if fullrun == 1:
    Q_sarsamax, N_sarsamax, avg_scores, Qlist = q_learning(env, num_episodes = n_episodes, \
                                                eps = 1,\
                                                alpha_schedule = alpha_schedule2, \
                                                gamma =  0.95, \
                                                plot_every = plot_every, \
                                                verbose = True)
    
    with open(&amp;#39;cached/as2_Q_sarsamax.pkl&amp;#39;, &amp;#39;wb&amp;#39;) as f:
        dill.dump(Q_sarsamax, f)
    with open(&amp;#39;cached/as2_avg_scores.pkl&amp;#39;, &amp;#39;wb&amp;#39;) as f:
        dill.dump(avg_scores, f)
    with open(&amp;#39;cached/as2_Qlist.pkl&amp;#39;, &amp;#39;wb&amp;#39;) as f:
        dill.dump(Qlist, f)        
else:
    with open(&amp;#39;cached/as2_Q_sarsamax.pkl&amp;#39;, &amp;#39;rb&amp;#39;) as f:
        Q_sarsamax = dill.load(f)
    with open(&amp;#39;cached/as2_avg_scores.pkl&amp;#39;, &amp;#39;rb&amp;#39;) as f:
        avg_scores = dill.load(f)
    with open(&amp;#39;cached/as2_Qlist.pkl&amp;#39;, &amp;#39;rb&amp;#39;) as f:
        Qlist = dill.load(f)     &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;dfm = list_to_df(Qlist)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;(p9.ggplot(data = dfm.loc[(dfm.episode &amp;gt; -1)], mapping = p9.aes(x = &amp;#39;episode&amp;#39;, y = &amp;#39;value&amp;#39;))
    + p9.geom_point() 
    + p9.geom_smooth(method = &amp;quot;loess&amp;quot;, color = &amp;quot;yellow&amp;quot;)
    + p9.geom_hline(data = df_true, mapping = p9.aes(yintercept = &amp;#39;value&amp;#39;), color = &amp;#39;red&amp;#39;)
    + p9.facet_wrap(&amp;quot;variable&amp;quot;, scales = &amp;#39;free_y&amp;#39;)
    + p9.theme(subplots_adjust={&amp;#39;wspace&amp;#39;:0.4})
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_70_0.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Cleary, the fluctuations are reduced compared to the previous schedule. AND all Q-values still fluctuate around their true values, so it seems that this schedule is better with respect to finding the true values.&lt;/p&gt;
&lt;p&gt;Let’s see if the accuracy of the estimated Q-values is indeed higher:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# drop burn-in, then average Q-vals by variable
Q_est = (dfm.loc[dfm.episode &amp;gt; 2000]
         .groupby([&amp;#39;variable&amp;#39;])
         .mean()
        )
# convert to 1d array sorted by state nr
Q_est[&amp;#39;sort_order&amp;#39;] = [int(str.replace(x, &amp;#39;Q&amp;#39;, &amp;#39;&amp;#39;)) \
                       for x in Q_est.index.values]

Q_est = Q_est.sort_values(by=[&amp;#39;sort_order&amp;#39;])

Q_est_as2 = Q_est[&amp;#39;value&amp;#39;].values

Q_as2_lasttable = [np.max(Q_sarsamax[key]) if key in Q_sarsamax \
             else 0 for key in np.arange(env.nS)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;q_showdown = pd.DataFrame(
    {&amp;#39;Q_as2_lasttable&amp;#39;: Q_as2_lasttable,
     &amp;#39;q_est_as2&amp;#39;: Q_est_as2,
     &amp;#39;q_true&amp;#39;: V_pi_dc})
q_showdown[&amp;#39;state&amp;#39;] = range(16)

q_showdown = pd.melt(q_showdown, [&amp;#39;state&amp;#39;])

(p9.ggplot(data = q_showdown, mapping = p9.aes(x = &amp;#39;state&amp;#39;, y = &amp;#39;value&amp;#39;, color = &amp;#39;factor(variable)&amp;#39;))
+ p9.geom_point(size = 5, shape = &amp;#39;x&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_73_0.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Boom! Now our &lt;span class=&#34;math inline&#34;&gt;\(Q^{*}_{a}\)&lt;/span&gt; estimates are really getting close to the true values. Clearly, the second learning rate schedule is able to learn the true Q-values compared to the first rate schedule, given the fixed amount of computation, in this case 500.000 episodes each.&lt;/p&gt;
&lt;p&gt;Averaging out does not do much anymore, except for states 10 and 14, where it improves the estimates a tiny bit.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;wrapping-up&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Wrapping up&lt;/h1&gt;
&lt;p&gt;In conclusion, we have seen that the common approach of using Q-learning with a constant learning rate and gradually decreasing the exploration rate, given sensible values and rates, will indeed find the optimal policy. However, this approach does not necessary converge to the true state-values. We have to tune the algorithm exactly the other way around: keep the exploration rate constant and sufficiently high, and decay the learning rate. For sufficiently low learning rates, averaging out the fluctuations does not meaningfully increase accuracy of the learned Q-values.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Jacks Car Rental as a Gym Environment</title>
      <link>/post/jacks-car-rental-gym/</link>
      <pubDate>Wed, 30 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/post/jacks-car-rental-gym/</guid>
      <description>
&lt;link href=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;This blogpost is about &lt;a href=&#34;https://en.wikipedia.org/wiki/Reinforcement_learning&#34;&gt;reinforcement learning&lt;/a&gt;, part of the Machine Learning (ML) / AI family of computer algorithms and techniques.
Reinforcement learning is all about agents taking decisions in complex environments. The decisions (&lt;strong&gt;actions&lt;/strong&gt;) take the agent from a current &lt;strong&gt;state&lt;/strong&gt; or situation, to a new &lt;strong&gt;state&lt;/strong&gt;. When the probability of ending up in a new state is only dependent on the current state and the action the agent takes in that state, we are facing a so-called &lt;strong&gt;Markov Decision Problem&lt;/strong&gt;, or &lt;strong&gt;MDP&lt;/strong&gt; for short.&lt;/p&gt;
&lt;p&gt;Back in 2016, people at OpenAI, a startup company that specializes in AI/ML, created a Python library called &lt;strong&gt;Gym&lt;/strong&gt; that provides standardized access to a range of MDP environments. Using Gym means keeping a sharp separation between the RL algorithm (“The agent”) and the environment (or task) it tries to solve / optimize / control / achieve. Gym allows us to easily benchmark RL algorithms on a range of different environments. It also allows us to more easily build on others work, and share our own work (i.e. on Github). Because when I implement something as a Gym Environment, others can then immediately apply their algorithms on it, and vice versa.&lt;/p&gt;
&lt;p&gt;In this blogpost, we solve a famous decision problem called “Jack’s Car Rental” by first turning it into a Gym environment and then use a RL algorithm called “Policy Iteration” (a form of “Dynamic Programming”) to solve for the optimal decisions to take in this environment.&lt;/p&gt;
&lt;p&gt;The Gym environment for Jack’s Car Rental is called &lt;code&gt;gym_jcr&lt;/code&gt; and can be installed from &lt;a href=&#34;https://github.com/gsverhoeven/gym_jcr&#34;&gt;https://github.com/gsverhoeven/gym_jcr&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;jacks-car-rental-problem&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Jack’s Car Rental problem&lt;/h1&gt;
&lt;p&gt;Learning Reinforcement learning (RL) as a student, means working through the famous &lt;a href=&#34;http://incompleteideas.net/book/the-book.html&#34;&gt;book on RL by Sutton and Barto&lt;/a&gt;. In chapter 4, Example 4.2 (2018 edition), Jack’s Car Rental problem is presented:&lt;/p&gt;
&lt;pre class=&#34;plaintext&#34;&gt;&lt;code&gt;Jack’s Car Rental 

Jack manages two locations for a nationwide car rental company. 
Each day, some number of customers arrive at each location to rent cars. 
If Jack has a car available, he rents it out and is credited $10 by 
the national company. If he is out of cars at that location, then the 
business is lost. Cars become available for renting the day after they 
are returned. To help ensure that cars are available where they are 
needed, Jack can move them between the two locations overnight, at a cost 
of $2 per car moved. We assume that the number of cars requested and 
returned at each location are Poisson random variables. Suppose Lambda is
3 and 4 for rental requests at the first and second locations and 
3 and 2 for returns. 

To simplify the problem slightly, we assume that there can be no more than
20 cars at each location (any additional cars are returned to the 
nationwide company, and thus disappear from the problem) and a maximum 
of five cars can be moved from one location to the other in one night. 

We take the discount rate to be gamma = 0.9 and formulate this as a 
continuing finite MDP, where the time steps are days, the state is the 
number of cars at each location at the end of the day, and the actions 
are the net numbers of cars moved between the two locations overnight.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In order to implement this MDP in Gym and solving it using DP (Dynamic Programming), we need to calculate for each state - action combination the probability of transitioning to all other states. Here a state is defined as the number of cars at the two locations A and B. Since there can be between 0 and 20 cars at each location, we have in total 21 x 21 = 441 states. We have 11 actions, moving up to five cars from A to B, moving up to five cars from B to A, or moving no cars at all. We also need the rewards &lt;strong&gt;R&lt;/strong&gt; for taking action &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; in state &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Luckily for us, Christian Herta and Patrick Baumann, as part of their project &lt;a href=&#34;https://www.deep-teaching.org/&#34;&gt;“Deep.Teaching”&lt;/a&gt;, created a Jupyter Notebook containing a well explained Python code solution for calculating &lt;strong&gt;P&lt;/strong&gt;, and &lt;strong&gt;R&lt;/strong&gt;, and published it as open source under the MIT license. I extracted their functions and put them in &lt;code&gt;jcr_mdp.py&lt;/code&gt;, containing two top level functions &lt;code&gt;create_P_matrix()&lt;/code&gt; and &lt;code&gt;create_R_matrix()&lt;/code&gt;, these are used when the Gym environment is initialized.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;jackscarrentalenv&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;JacksCarRentalEnv&lt;/h1&gt;
&lt;p&gt;My approach to creating the Gym environment for Jack’s Car Rental was to take the &lt;a href=&#34;https://gym.openai.com/envs/FrozenLake-v0/&#34;&gt;Frozen Lake Gym environment&lt;/a&gt;, and rework it to become JacksCarRentalEnv. I chose this environment because it has a similar structure as JCR, having discrete states and discrete actions. In addition, it is one of the few environments that create and expose the complete transition matrix &lt;strong&gt;P&lt;/strong&gt; needed for the DP algorithm.&lt;/p&gt;
&lt;p&gt;There is actually not much to it at this point, as most functionality is provided by the &lt;code&gt;DiscreteEnv&lt;/code&gt; class that our environment builds on. We need only to specify four objects:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;nS: number of states&lt;/li&gt;
&lt;li&gt;nA: number of actions&lt;/li&gt;
&lt;li&gt;P: transitions&lt;/li&gt;
&lt;li&gt;isd: initial state distribution (list or array of length nS)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;nS&lt;/code&gt; and &lt;code&gt;nA&lt;/code&gt; were already discussed above, there are 441 and 11 respectively.
For the &lt;code&gt;isd&lt;/code&gt; we simply choose an equal probability to start in any of the 441 states.&lt;/p&gt;
&lt;p&gt;This leaves us with the transitions &lt;strong&gt;P&lt;/strong&gt;. This needs to be in a particular format, a &lt;code&gt;dictionary dict of dicts of lists, where P[s][a] == [(probability, nextstate, reward, done), ...]&lt;/code&gt; according to the help of this class. So we take the &lt;strong&gt;P&lt;/strong&gt; and &lt;strong&gt;R&lt;/strong&gt; arrays created by the python code in &lt;code&gt;jcr_mdp.py&lt;/code&gt; and use these to fill the dictionary in the proper way (drawing inspiration from the Frozen Lake &lt;strong&gt;P&lt;/strong&gt; object :)).&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;P = {s : {a : [] for a in range(nA)} for s in range(nS)}

# prob, next_state, reward, done
for s in range(nS):
    # need a state vec to extract correct probs from Ptrans
    state_vec = np.zeros(nS)
    state_vec[s] = 1
    for a in range(nA):
        prob_vec = np.dot(Ptrans[:,:,a], state_vec)
        li = P[s][a]
        # add rewards for all transitions
        for ns in range(nS):
            li.append((prob_vec[ns], ns, R[s][a], False))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And were done! Let’s try it out.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import matplotlib.pyplot as plt
import numpy as np
import pickle

# Gym environment
import gym
import gym_jcr
# RL algorithm
from dp import *&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# n.b. can take up to 15 s
env = gym.make(&amp;quot;JacksCarRentalEnv-v0&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So what we have?&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# print the state space and action space
print(env.observation_space)
print(env.action_space)

# print the total number of states and actions
print(env.nS)
print(env.nA)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Discrete(441)
Discrete(11)
441
11&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let us check for state &lt;code&gt;s= 0&lt;/code&gt;, for each action &lt;code&gt;a&lt;/code&gt;, if the probabilities of transitioning to a new state &lt;code&gt;new_state&lt;/code&gt; sum to one (we need to end up somewhere right?).&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# from state 0, for each action the probs for going to new state
s = 0

for a in range(env.nA):
    prob = 0.0
    for new_state in range(env.nS):
        prob += env.P[s][a][new_state][0]
    print(prob, end = &amp;#39; &amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.9999999999999992 0.9999999999999992 0.9999999999999992 0.9999999999999992 0.9999999999999992 0.9999999999999992 0.9999999999999992 0.9999999999999992 0.9999999999999992 0.9999999999999992 0.9999999999999992 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Close enough. Let’s run our Dynamic Programming algorithm on it!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;policy-iteration-on-jcr&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Policy iteration on JCR&lt;/h1&gt;
&lt;p&gt;The &lt;code&gt;policy_iteration()&lt;/code&gt; function used below is from &lt;a href=&#34;https://github.com/gsverhoeven/hugo_source/blob/master/content/post/dp.py&#34;&gt;dp.py&lt;/a&gt;. This exact same code was used in a Jupyter tutorial notebook to solve the &lt;a href=&#34;https://gym.openai.com/envs/FrozenLake-v0/&#34;&gt;Frozen-Lake Gym environment&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We reproduce the results from the Sutton &amp;amp; Barto book (p81), where the algorithm converges after four iterations. This takes about 30 min on my computer.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fullrun = False

if fullrun == True:
    policy, V = policy_iteration(env, gamma = 0.9)
    with open(&amp;#39;policy.bin&amp;#39;, &amp;#39;wb&amp;#39;) as f:
        pickle.dump(policy, f)
    with open(&amp;#39;values.bin&amp;#39;, &amp;#39;wb&amp;#39;) as f:
        pickle.dump(V, f)
else:
    with open(&amp;#39;policy.bin&amp;#39;, &amp;#39;rb&amp;#39;) as f:
        policy = pickle.load(f)
    with open(&amp;#39;values.bin&amp;#39;, &amp;#39;rb&amp;#39;) as f:
        V = pickle.load(f)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-optimal-policy-as-a-contour-map&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Plot optimal policy as a contour map&lt;/h1&gt;
&lt;p&gt;For easy plotting, we need to transform the policy from a 2d state-action matrix to a 2d state-A, state-B matrix with the action values in the cells.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;MAX_CARS = 20

def get_state_vector(a, b):
    s = np.zeros((MAX_CARS+1)**2)
    s[a*(MAX_CARS+1)+b] = 1
    return s

policy_map = np.zeros([MAX_CARS+1, MAX_CARS+1])

for a in range(MAX_CARS+1):
    for b in range(MAX_CARS+1):
        state = get_state_vector(a, b)
        s = state.argmax()
        policy_map[a, b] = np.argmax(policy[s,:]) - 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We visualize the optimal policy as a 2d heatmap using &lt;code&gt;matplotlib.pyplot.imshow()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;plt.figure(figsize=(7,6))
hmap = plt.imshow(policy_map, cmap=&amp;#39;viridis&amp;#39;, origin=&amp;#39;lower&amp;#39;)
cbar = plt.colorbar(hmap)
cbar.ax.set_ylabel(&amp;#39;actions&amp;#39;)
plt.title(&amp;#39;Policy&amp;#39;)
plt.xlabel(&amp;quot;cars at B&amp;quot;)
plt.ylabel(&amp;quot;cars at A&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/post/2020-12-30-jacks_car_rental_gym_files/2020-12-30-jacks_car_rental_gym_13_1.png&#34; alt=&#34;Optimal policy for all states of Jack’s Car Rental&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Optimal policy for all states of Jack’s Car Rental&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion-and-outlook&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion and outlook&lt;/h1&gt;
&lt;p&gt;Conclusion: yes we can turn JCR into a Gym environment and solve it using the exact same (policy iteration) code that I had earlier used to solve the Frozen-Lake Gym environment!&lt;/p&gt;
&lt;p&gt;So now what? One obvious area of improvement is speed: It takes too long to load the environment. Also the DP algorithm is slow, because it uses for loops instead of matrix operations.&lt;/p&gt;
&lt;p&gt;Another thing is that currently the rewards that the environment returns are &lt;strong&gt;average expected rewards&lt;/strong&gt; that are received when taking action &lt;em&gt;a&lt;/em&gt; in state &lt;em&gt;s&lt;/em&gt; . However, they do not match the actual amount of cars rented when transitioning from a particular state &lt;em&gt;s&lt;/em&gt; to a new state &lt;em&gt;s’&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Finally, adding the modifications to the problem from Exercise 4.7 in Sutton &amp;amp; Barto could also be implemented, but this complicates the calculation of &lt;strong&gt;P&lt;/strong&gt; and &lt;strong&gt;R&lt;/strong&gt; even further.
For me, this is the real takeaway from this exercise: it is really hard to (correctly) compute the complete set of transition probabilities and rewards for an MDP, but it is much easier if we just need to simulate single transitions according to the MDP specification. Wikipedia has a nice paragraph on it under &lt;a href=&#34;https://en.wikipedia.org/wiki/Markov_decision_process#Simulator_models&#34;&gt;simulator models for MDPs&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
