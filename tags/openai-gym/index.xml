<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>OpenAI Gym | Gertjan Verhoeven</title>
    <link>https://gsverhoeven.github.io/tags/openai-gym/</link>
      <atom:link href="https://gsverhoeven.github.io/tags/openai-gym/index.xml" rel="self" type="application/rss+xml" />
    <description>OpenAI Gym</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2019, 2020</copyright><lastBuildDate>Wed, 30 Dec 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>OpenAI Gym</title>
      <link>https://gsverhoeven.github.io/tags/openai-gym/</link>
    </image>
    
    <item>
      <title>Jacks Car Rental as a Gym Environment</title>
      <link>https://gsverhoeven.github.io/post/jacks-car-rental-gym/</link>
      <pubDate>Wed, 30 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://gsverhoeven.github.io/post/jacks-car-rental-gym/</guid>
      <description>
&lt;link href=&#34;https://gsverhoeven.github.io/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://gsverhoeven.github.io/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;This blogpost is about &lt;a href=&#34;https://en.wikipedia.org/wiki/Reinforcement_learning&#34;&gt;reinforcement learning&lt;/a&gt;, part of the Machine Learning (ML) / AI family of computer algorithms and techniques.
Reinforcement learning is all about agents taking decisions in complex environments. The decisions (&lt;strong&gt;actions&lt;/strong&gt;) take the agent from a current &lt;strong&gt;state&lt;/strong&gt; or situation, to a new &lt;strong&gt;state&lt;/strong&gt;. When the probability of ending up in a new state is only dependent on the current state and the action the agent takes in that state, we are facing a so-called &lt;strong&gt;Markov Decision Problem&lt;/strong&gt;, or &lt;strong&gt;MDP&lt;/strong&gt; for short.&lt;/p&gt;
&lt;p&gt;Back in 2016, people at OpenAI, a startup company that specializes in AI/ML, created a Python library called &lt;strong&gt;Gym&lt;/strong&gt; that provides standardized access to a range of MDP environments. Using Gym means keeping a sharp separation between the RL algorithm (“The agent”) and the environment (or task) it tries to solve / optimize / control / achieve. Gym allows us to easily benchmark RL algorithms on a range of different environments. It also allows us to more easily build on others work, and share our own work (i.e. on Github). Because when I implement something as a Gym Environment, others can then immediately apply their algorithms on it, and vice versa.&lt;/p&gt;
&lt;p&gt;In this blogpost, we solve a famous decision problem called “Jack’s Car Rental” by first turning it into a Gym environment and then use a RL algorithm called “Policy Iteration” (a form of “Dynamic Programming”) to solve for the optimal decisions to take in this environment.&lt;/p&gt;
&lt;p&gt;The Gym environment for Jack’s Car Rental is called &lt;code&gt;gym_jcr&lt;/code&gt; and can be installed from &lt;a href=&#34;https://github.com/gsverhoeven/gym_jcr&#34;&gt;https://github.com/gsverhoeven/gym_jcr&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;jacks-car-rental-problem&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Jack’s Car Rental problem&lt;/h1&gt;
&lt;p&gt;Learning Reinforcement learning (RL) as a student, means working through the famous &lt;a href=&#34;http://incompleteideas.net/book/the-book.html&#34;&gt;book on RL by Sutton and Barto&lt;/a&gt;. In chapter 4, Example 4.2 (2018 edition), Jack’s Car Rental problem is presented:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Jack’s Car Rental 

Jack manages two locations for a nationwide car rental company. 
Each day, some number of customers arrive at each location to rent cars. 
If Jack has a car available, he rents it out and is credited $10 by 
the national company. If he is out of cars at that location, then the 
business is lost. Cars become available for renting the day after they 
are returned. To help ensure that cars are available where they are 
needed, Jack can move them between the two locations overnight, at a cost 
of $2 per car moved. We assume that the number of cars requested and 
returned at each location are Poisson random variables. Suppose Lambda is
3 and 4 for rental requests at the first and second locations and 
3 and 2 for returns. 

To simplify the problem slightly, we assume that there can be no more than
20 cars at each location (any additional cars are returned to the 
nationwide company, and thus disappear from the problem) and a maximum 
of five cars can be moved from one location to the other in one night. 

We take the discount rate to be gamma = 0.9 and formulate this as a 
continuing finite MDP, where the time steps are days, the state is the 
number of cars at each location at the end of the day, and the actions 
are the net numbers of cars moved between the two locations overnight.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In order to implement this MDP in Gym and solving it using DP (Dynamic Programming), we need to calculate for each state - action combination the probability of transitioning to all other states. Here a state is defined as the number of cars at the two locations A and B. Since there can be between 0 and 20 cars at each location, we have in total 21 x 21 = 441 states. We have 11 actions, moving up to five cars from A to B, moving up to five cars from B to A, or moving no cars at all. We also need the rewards &lt;strong&gt;R&lt;/strong&gt; for taking action &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; in state &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Luckily for us, Christian Herta and Patrick Baumann, as part of their project &lt;a href=&#34;https://www.deep-teaching.org/&#34;&gt;“Deep.Teaching”&lt;/a&gt;, created a Jupyter Notebook containing a well explained Python code solution for calculating &lt;strong&gt;P&lt;/strong&gt;, and &lt;strong&gt;R&lt;/strong&gt;, and published it as open source under the MIT license. I extracted their functions and put them in &lt;code&gt;jcr_mdp.py&lt;/code&gt;, containing two top level functions &lt;code&gt;create_P_matrix()&lt;/code&gt; and &lt;code&gt;create_R_matrix()&lt;/code&gt;, these are used when the Gym environment is initialized.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;jackscarrentalenv&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;JacksCarRentalEnv&lt;/h1&gt;
&lt;p&gt;My approach to creating the Gym environment for Jack’s Car Rental was to take the &lt;a href=&#34;https://gym.openai.com/envs/FrozenLake-v0/&#34;&gt;Frozen Lake Gym environment&lt;/a&gt;, and rework it to become JacksCarRentalEnv. I chose this environment because it has a similar structure as JCR, having discrete states and discrete actions. In addition, it is one of the few environments that create and expose the complete transition matrix &lt;strong&gt;P&lt;/strong&gt; needed for the DP algorithm.&lt;/p&gt;
&lt;p&gt;There is actually not much to it at this point, as most functionality is provided by the &lt;code&gt;DiscreteEnv&lt;/code&gt; class that our environment builds on. We need only to specify four objects:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;nS: number of states&lt;/li&gt;
&lt;li&gt;nA: number of actions&lt;/li&gt;
&lt;li&gt;P: transitions&lt;/li&gt;
&lt;li&gt;isd: initial state distribution (list or array of length nS)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;nS&lt;/code&gt; and &lt;code&gt;nA&lt;/code&gt; were already discussed above, there are 441 and 11 respectively.
For the &lt;code&gt;isd&lt;/code&gt; we simply choose an equal probability to start in any of the 441 states.&lt;/p&gt;
&lt;p&gt;This leaves us with the transitions &lt;strong&gt;P&lt;/strong&gt;. This needs to be in a particular format, a &lt;code&gt;dictionary dict of dicts of lists, where P[s][a] == [(probability, nextstate, reward, done), ...]&lt;/code&gt; according to the help of this class. So we take the &lt;strong&gt;P&lt;/strong&gt; and &lt;strong&gt;R&lt;/strong&gt; arrays created by the python code in &lt;code&gt;jcr_mdp.py&lt;/code&gt; and use these to fill the dictionary in the proper way (drawing inspiration from the Frozen Lake &lt;strong&gt;P&lt;/strong&gt; object :)).&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;P = {s : {a : [] for a in range(nA)} for s in range(nS)}

# prob, next_state, reward, done
for s in range(nS):
    # need a state vec to extract correct probs from Ptrans
    state_vec = np.zeros(nS)
    state_vec[s] = 1
    for a in range(nA):
        prob_vec = np.dot(Ptrans[:,:,a], state_vec)
        li = P[s][a]
        # add rewards for all transitions
        for ns in range(nS):
            li.append((prob_vec[ns], ns, R[s][a], False))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And were done! Let’s try it out.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import matplotlib.pyplot as plt
import numpy as np
import pickle

# Gym environment
import gym
import gym_jcr
# RL algorithm
from dp import *&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# n.b. can take up to 15 s
env = gym.make(&amp;quot;JacksCarRentalEnv-v0&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So what we have?&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# print the state space and action space
print(env.observation_space)
print(env.action_space)

# print the total number of states and actions
print(env.nS)
print(env.nA)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Discrete(441)
Discrete(11)
441
11&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let us check for state &lt;code&gt;s= 0&lt;/code&gt;, for each action &lt;code&gt;a&lt;/code&gt;, if the probabilities of transitioning to a new state &lt;code&gt;new_state&lt;/code&gt; sum to one (we need to end up somewhere right?).&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# from state 0, for each action the probs for going to new state
s = 0

for a in range(env.nA):
    prob = 0.0
    for new_state in range(env.nS):
        prob += env.P[s][a][new_state][0]
    print(prob, end = &amp;#39; &amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.9999999999999992 0.9999999999999992 0.9999999999999992 0.9999999999999992 0.9999999999999992 0.9999999999999992 0.9999999999999992 0.9999999999999992 0.9999999999999992 0.9999999999999992 0.9999999999999992 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Close enough. Let’s run our Dynamic Programming algorithm on it!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;policy-iteration-on-jcr&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Policy iteration on JCR&lt;/h1&gt;
&lt;p&gt;The &lt;code&gt;policy_iteration()&lt;/code&gt; function used below is from &lt;a href=&#34;https://github.com/gsverhoeven/hugo_source/blob/master/content/post/dp.py&#34;&gt;dp.py&lt;/a&gt;. This exact same code was used in a Jupyter tutorial notebook to solve the &lt;a href=&#34;https://gym.openai.com/envs/FrozenLake-v0/&#34;&gt;Frozen-Lake Gym environment&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We reproduce the results from the Sutton &amp;amp; Barto book (p81), where the algorithm converges after four iterations. This takes about 30 min on my computer.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fullrun = False

if fullrun == True:
    policy, V = policy_iteration(env, gamma = 0.9)
    with open(&amp;#39;policy.bin&amp;#39;, &amp;#39;wb&amp;#39;) as f:
        pickle.dump(policy, f)
    with open(&amp;#39;values.bin&amp;#39;, &amp;#39;wb&amp;#39;) as f:
        pickle.dump(V, f)
else:
    with open(&amp;#39;policy.bin&amp;#39;, &amp;#39;rb&amp;#39;) as f:
        policy = pickle.load(f)
    with open(&amp;#39;values.bin&amp;#39;, &amp;#39;rb&amp;#39;) as f:
        V = pickle.load(f)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-optimal-policy-as-a-contour-map&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Plot optimal policy as a contour map&lt;/h1&gt;
&lt;p&gt;For easy plotting, we need to transform the policy from a 2d state-action matrix to a 2d state-A, state-B matrix with the action values in the cells.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;MAX_CARS = 20

def get_state_vector(a, b):
    s = np.zeros((MAX_CARS+1)**2)
    s[a*(MAX_CARS+1)+b] = 1
    return s

policy_map = np.zeros([MAX_CARS+1, MAX_CARS+1])

for a in range(MAX_CARS+1):
    for b in range(MAX_CARS+1):
        state = get_state_vector(a, b)
        s = state.argmax()
        policy_map[a, b] = np.argmax(policy[s,:]) - 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We visualize the optimal policy as a 2d heatmap using &lt;code&gt;matplotlib.pyplot.imshow()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;plt.figure(figsize=(7,6))
hmap = plt.imshow(policy_map, cmap=&amp;#39;viridis&amp;#39;, origin=&amp;#39;lower&amp;#39;)
cbar = plt.colorbar(hmap)
cbar.ax.set_ylabel(&amp;#39;actions&amp;#39;)
plt.title(&amp;#39;Policy&amp;#39;)
plt.xlabel(&amp;quot;cars at B&amp;quot;)
plt.ylabel(&amp;quot;cars at A&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://gsverhoeven.github.io/post/2020-12-30-jacks_car_rental_gym_files/2020-12-30-jacks_car_rental_gym_13_1.png&#34; alt=&#34;png&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;png&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion-and-outlook&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion and outlook&lt;/h1&gt;
&lt;p&gt;Conclusion: yes we can turn JCR into a Gym environment and solve it using the exact same code we used to solve the Frozen Lake environment!&lt;/p&gt;
&lt;p&gt;So now what? One obvious area of improvement is speed: It takes too long to load the environment. Also the DP algorithm is slow, because it uses for loops instead of matrix operations.&lt;/p&gt;
&lt;p&gt;Another thing is that currently the rewards that the environment returns are &lt;strong&gt;average expected rewards&lt;/strong&gt; that are received when taking action &lt;em&gt;a&lt;/em&gt; in state &lt;em&gt;s&lt;/em&gt; . However, they do not match the actual amount of cars rented when transitioning from a particular state &lt;em&gt;s&lt;/em&gt; to a new state &lt;em&gt;s’&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Finally, adding the modifications to the problem from Exercise 4.7 could also be implemented, but this complicates the calculation of &lt;strong&gt;P&lt;/strong&gt; and &lt;strong&gt;R&lt;/strong&gt; even further.
For me, this is the real takeaway from this exercise: it is really hard to compute the complete set of proper transition probabilities and rewards for an MDP, but it is much easier if we just need to simulate single transitions according to the MDP specification. Wikipedia has a nice paragraph on it under &lt;a href=&#34;https://en.wikipedia.org/wiki/Markov_decision_process#Simulator_models&#34;&gt;simulator models for MDPs&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
