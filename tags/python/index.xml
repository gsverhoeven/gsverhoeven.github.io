<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Python | Gertjan Verhoeven</title>
    <link>/tags/python/</link>
      <atom:link href="/tags/python/index.xml" rel="self" type="application/rss+xml" />
    <description>Python</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2019-2022</copyright><lastBuildDate>Sun, 20 Mar 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Python</title>
      <link>/tags/python/</link>
    </image>
    
    <item>
      <title>Nufflytics: Analyzing Blood Bowl matches from FUMBBL using Python</title>
      <link>/post/blood-bowl-nufflytics/</link>
      <pubDate>Sun, 20 Mar 2022 00:00:00 +0000</pubDate>
      <guid>/post/blood-bowl-nufflytics/</guid>
      <description>


&lt;p&gt;This blogpost is about &lt;strong&gt;Blood Bowl&lt;/strong&gt;, a strategic boardgame invented in the late 80’s, that I finally started playing last year. Blood bowl is a game of Fantasy Football, where fantasy team races (think “Orcs”, or “Elves”) are pitted against each other. Interestingly, the various teams (there are over 20 different ones) require different play styles, and not all team races are equally strong. On tournaments, this gives rise to various compensation schemes to make all teams “viable” for competition. There exists a lively tournament scene, with thousands of matches played each year.&lt;/p&gt;
&lt;p&gt;The idea of this blog post is to showcase some possible analyses that can be done on the &lt;a href=&#34;https://gsverhoeven.github.io/post/blood-bowl-fumbbl-dataset/&#34;&gt;FUMBBL match data I’ve compiled&lt;/a&gt;. The idea is to make Blood Bowl data analysis (also know as &lt;a href=&#34;https://nufflytics.com&#34;&gt;Nufflytics&lt;/a&gt;, a term coined by Blood Bowler “Schlice” in reference to Nuffle, the god of Blood Bowl) easier and more accessible to others. I took inspiration from various sources, detailed at the end of this post. So lets dive in the world of Blood Bowl stats nerdery.&lt;/p&gt;
&lt;div id=&#34;getting-started-with-the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Getting started with the data&lt;/h1&gt;
&lt;p&gt;Since the previous blog post on FUMBBL data, I decided to make a separate Github repository &lt;a href=&#34;https://github.com/gsverhoeven/fumbbl_datasets&#34;&gt;fumbbl_datasets&lt;/a&gt; that contains the Python code to fetch and construct the FUMBBL datasets. You can either download the latest datasets manually, or clone the entire repo to your local drive, depending on your expertise and preferences.&lt;/p&gt;
&lt;p&gt;The datasets are available both in CSV and HDF5. CSV would be the format of choice for Excel analysis, whereas the HDF5 format is suitable for scripted languages such as Python or R. Here we use Python, with the libraries &lt;code&gt;Pandas&lt;/code&gt; and &lt;code&gt;plotnine&lt;/code&gt; for data analysis and visualization. The code below assumes the datasets are locally stored at the location contained in the &lt;code&gt;path_to_datasets&lt;/code&gt; variable:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import pandas as pd
import numpy as np
import plotnine as p9

# point this to the location of the HDF5 datasets
path_to_datasets = &amp;#39;../../../../fumbbl_datasets/&amp;#39;

# FUMBBL matches
target = &amp;#39;datasets/v0.2/df_matches.h5&amp;#39;
df_matches = pd.read_hdf(path_to_datasets + target) 

# FUMBBL matches by team
target = &amp;#39;datasets/v0.2/df_mbt.h5&amp;#39;
df_mbt = pd.read_hdf(path_to_datasets + target) 

# FUMBBL inducements
target = &amp;#39;datasets/v0.2/inducements.h5&amp;#39;
inducements = pd.read_hdf(path_to_datasets + target) &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;what-data-do-we-have-weekly-game-volumes&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What data do we have? Weekly game volumes&lt;/h1&gt;
&lt;p&gt;Let’s see what we’ve got! The pandas DataFrame &lt;code&gt;df_matches&lt;/code&gt; contains records for all matches played on FUMBBL between august 2020 and march 2022.&lt;/p&gt;
&lt;p&gt;Since we have a proper &lt;code&gt;datetime&lt;/code&gt; type variable for each week (&lt;code&gt;week_date&lt;/code&gt;), we can use &lt;code&gt;pandas&lt;/code&gt; and &lt;code&gt;plotnine&lt;/code&gt; to plot the weekly game volume as a time series.&lt;/p&gt;
&lt;p&gt;The introduction of the new &lt;strong&gt;Competitive division&lt;/strong&gt; with BB2020 rules is marked by a vertical red line. I labeled the larger leagues as well a recent tournament I took part in myself.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;res = (df_matches
    .loc[(df_matches[&amp;#39;week_date&amp;#39;] &amp;gt;= &amp;#39;2020-08-01&amp;#39; ) &amp;amp; (df_matches[&amp;#39;week_date&amp;#39;] &amp;lt; &amp;#39;2022-11-25&amp;#39;)]
    .groupby([&amp;#39;week_date&amp;#39;, &amp;#39;week_number&amp;#39;, &amp;#39;division_name&amp;#39;])
    .agg(        
        n_games = (&amp;#39;match_id&amp;#39;, &amp;quot;count&amp;quot;) 
    )
    .reset_index()) # this adds the &amp;quot;group by&amp;quot; variables back as columns of res

(p9.ggplot(data = res, mapping = p9.aes(x = &amp;#39;week_date&amp;#39;, y = &amp;#39;n_games&amp;#39;, color = &amp;#39;division_name&amp;#39;))
+ p9.geom_point() 
+ p9.geom_line()
+ p9.expand_limits(y=[0,2000])
+ p9.geom_vline(xintercept = &amp;#39;2021-09-01&amp;#39;, color = &amp;quot;red&amp;quot;)
+ p9.theme(figure_size = (10, 5))
+ p9.ggtitle(&amp;quot;Weekly game volume on FUMBBL august 2020 - march 2022&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2022-03-20_nufflytics_blog_post_files/nufflytics_blog_post_4_0.png&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;ggplot: (-9223363302253724542)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To check the dataset, I compared this plot with the plot of weekly game volumes that FUMBBL itself provides at &lt;a href=&#34;https://fumbbl.com/p/stats&#34; class=&#34;uri&#34;&gt;https://fumbbl.com/p/stats&lt;/a&gt;.
Both plots looked identical at the time of writing, so it seems that we have a complete dataset for the given period.&lt;/p&gt;
&lt;p&gt;The effect of starting the new BB2020 Competitive division is clearly visible, with the weekly game volume almost doubling in september 2021.
The first online NAF tournament using BB2020 rules is also visible, running for 6 weeks in October / November 2021.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;star-player-usage-on-fumbbl&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Star player usage on FUMBBL&lt;/h1&gt;
&lt;p&gt;We can also look at the percentage of matches that involve star players.
I used the various plot aesthetics like symbol shape and size to encode the game volume and ruleset (BB2016 or BB2020 based).&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;divisions = [&amp;#39;Blackbox&amp;#39;, &amp;#39;Competitive&amp;#39;, &amp;#39;Online NAF Tournaments&amp;#39;,  &amp;#39;Ranked&amp;#39;, &amp;#39;Regular_league&amp;#39;]

res = (df_matches
.query(&amp;quot;division_name in @divisions&amp;quot;)
.groupby([&amp;#39;division_name&amp;#39;, &amp;#39;league&amp;#39;, &amp;#39;ruleset&amp;#39;, &amp;#39;ruleset_version&amp;#39;, &amp;#39;week_date&amp;#39;])
.agg(
    n_games = (&amp;#39;match_id&amp;#39;, &amp;#39;count&amp;#39;),
    perc_sp = (&amp;#39;has_sp&amp;#39;, &amp;#39;mean&amp;#39;)
)
.reset_index()
.sort_values(&amp;quot;n_games&amp;quot;, ascending=False)
)

(p9.ggplot(data = res.query(&amp;quot;n_games &amp;gt; 30&amp;quot;), mapping = p9.aes(x = &amp;#39;week_date&amp;#39;, y = &amp;#39;perc_sp*100&amp;#39;, 
group = &amp;#39;factor(division_name)&amp;#39;, color = &amp;#39;factor(division_name)&amp;#39;))
    + p9.geom_point(p9.aes(shape = &amp;#39;factor(ruleset_version)&amp;#39;, size = &amp;#39;n_games&amp;#39;)) 
    + p9.expand_limits(y=[0,1])
    + p9.scale_size_area()
    + p9.geom_vline(xintercept = &amp;#39;2021-09-01&amp;#39;, color = &amp;quot;red&amp;quot;)
    + p9.ggtitle(&amp;quot;Star player usage over time, by division/league&amp;quot;)
    + p9.theme(figure_size = (10, 6))
    + p9.ylab(&amp;quot;% matches with at least one Star Player&amp;quot;))
    &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2022-03-20_nufflytics_blog_post_files/nufflytics_blog_post_7_0.png&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;ggplot: (-9223363302246313763)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In above graph, the various online NAF Tournaments are clearly distinguished. &lt;strong&gt;Amorical Cup 2020&lt;/strong&gt; in summer 2020, &lt;strong&gt;Eur’Open Online&lt;/strong&gt; in Nov/dec 2020, &lt;strong&gt;SteelBowl&lt;/strong&gt; in Feb 2021, and &lt;strong&gt;LitBowl&lt;/strong&gt; in May 2021 were all using BB2016 rules.&lt;/p&gt;
&lt;p&gt;Through Googling and using the Wayback Machine, I was able to find the rulepacks of these tournaments. LitBowl featured “big budgets” (up to 1440K) and a requirement of only 10 regular players before inducement, this likely explains the large amount of Star Players in that tournament.&lt;/p&gt;
&lt;p&gt;In contrast, in the GBFU tournament, the first online NAF tournament using the BB2020 rules, only some 15% of matches involved at least one star player.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;are-coach-ratings-predictive-of-match-outcomes&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Are coach ratings predictive of match outcomes?&lt;/h1&gt;
&lt;p&gt;For the main divisions on FUMBBL, ELO style coach ratings are available that are updated after each game.
The coach rankings are explained on &lt;a href=&#34;https://fumbbl.com/help:Ranking&#34;&gt;this help page&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;According to the ELO ranking system, a coach rating difference of 40 should result in 85% wins for the higher ranked coach.
Coaches of equal rating should have a win rate of 0.5 (with draws weighted at half point).&lt;/p&gt;
&lt;p&gt;The range of coach rankings observed for a particular game tells us something about the relationship between skill and luck.
If a game is pure luck, we will never observe large differences in coach rating, since the outcome will be determined by a coin flip, independent of coach skill.&lt;/p&gt;
&lt;p&gt;On FUMBBL, coach ratings vary roughly between 125 and 175. What do we expect if a coach with a rating of 175 plays a coach of rating 145? Well, the rating difference is 30. According to the formula (assuming equal team strength and equal races), the expected win probability is 1/(1 + 10^0.75) = 85%, and the probability of loss is 15%.&lt;/p&gt;
&lt;p&gt;Since our CR we obtained from the FUMBBL match result page is an overall coach rating (i.e. it ignores division), we can simply pool all matches from divisions where coach rating is tracked.&lt;/p&gt;
&lt;p&gt;The match data contains a &lt;strong&gt;Coach Ranking Difference&lt;/strong&gt; bin (category) that we can each to calculate the Win/draw/loss percentages for each category.&lt;/p&gt;
&lt;p&gt;Let’s see what the actual percentages are:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;main_divisions = [&amp;#39;Blackbox&amp;#39;, &amp;#39;Ranked&amp;#39;, &amp;#39;Competitive&amp;#39;]

res = (df_matches[df_matches[&amp;#39;division_name&amp;#39;].isin(main_divisions)]
    .groupby([&amp;#39;cr_bin&amp;#39;, &amp;#39;team1_win&amp;#39;])
    .agg(        
        n_games = (&amp;#39;cr_bin&amp;#39;, &amp;quot;count&amp;quot;),
    )
    .reset_index()) # this adds the group by variable (now index) as a column

# add total games played within each bin
res[&amp;#39;n_games_bin&amp;#39;] = res.groupby(&amp;#39;cr_bin&amp;#39;).n_games.transform(&amp;#39;sum&amp;#39;)

res[&amp;#39;perc&amp;#39;] = res[&amp;#39;n_games&amp;#39;]/res[&amp;#39;n_games_bin&amp;#39;]

(p9.ggplot(res, p9.aes(x = &amp;#39;factor(cr_bin)&amp;#39;, y = &amp;#39;perc&amp;#39;, fill = &amp;#39;factor(team1_win)&amp;#39;)) 
    + p9.geom_bar(position = &amp;quot;fill&amp;quot;, stat = &amp;quot;identity&amp;quot;) 
    + p9.theme(axis_text_x= p9.element_text(rotation=90, hjust=1))
    + p9.ggtitle(&amp;#39;probability of win/draw/loss as a function of Coach Rating difference&amp;#39;)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2022-03-20_nufflytics_blog_post_files/nufflytics_blog_post_10_0.png&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;ggplot: (-9223363302350291624)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(Note that I made the bins for large CR differences (greater than 10) wider to get more games per bin.)&lt;/p&gt;
&lt;p&gt;From above graphs, we can conclude that the coach ratings work as expected, with large coach rating differences indeed showing high win rates for the higher ranked coach. From this we can infer that a highly skilled coach will win 9 times out of ten agains a below average coach. We call Blood Bowl a Strategy game for a reason!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-about-the-passing-game-in-bb2020&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What about the passing game in BB2020?&lt;/h1&gt;
&lt;p&gt;With Blood Bowl 2020 also came a large change to passing the ball. Passing is no longer linked to the &lt;strong&gt;Agility&lt;/strong&gt; statistics, but now has its own &lt;strong&gt;Passing&lt;/strong&gt; (PA) stat. Overall, passing became riskier, and high agility teams do not automatically have good passing stats. For example, only a High Elf thrower has a PA of 2+, whereas the rest of the players have a PA of 4+ or higher. On the Dark Elf team, the player with the best PA stat is the runner, with a PA of 3+, without a built in re-roll. So we can expect quite some changes in the number of completions per match. For more detail I refer to a nice post by king_ghidra at &lt;a href=&#34;https://bloodbowlstrategies.com/en/tactics-blood-bowl-second-season/&#34;&gt;Blood Bowl Strategies&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let’s have a look!&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;divisions = [&amp;#39;Ranked&amp;#39;, &amp;#39;Blackbox&amp;#39;, &amp;#39;Competitive&amp;#39;]

tv_bins = [&amp;#39;1.1M&amp;#39;, &amp;#39;1.4M&amp;#39;, &amp;#39;1.7M&amp;#39;]

res = (df_mbt[df_mbt[&amp;#39;division_name&amp;#39;].isin(divisions)]
    .loc[df_mbt[&amp;#39;tv_bin&amp;#39;].isin(tv_bins)]
    .query(&amp;quot;mirror_match == 0 &amp;amp; has_sp == 0 &amp;amp; tv_bin in @tv_bins &amp;amp; division_name in @divisions&amp;quot;)
    .groupby([&amp;#39;division_name&amp;#39;, &amp;#39;ruleset_version&amp;#39;, &amp;#39;week_date&amp;#39;, &amp;#39;tv_bin&amp;#39;])
    .agg(        
        avg_comp = (&amp;#39;home_comp&amp;#39;, &amp;quot;mean&amp;quot;),
        avg_pass = (&amp;#39;home_pass&amp;#39;, &amp;quot;mean&amp;quot;),
        avg_foul = (&amp;#39;home_foul&amp;#39;, &amp;quot;mean&amp;quot;),
        avg_block = (&amp;#39;home_block&amp;#39;, &amp;quot;mean&amp;quot;),    
        avg_cas = (&amp;#39;home_cas&amp;#39;, &amp;quot;mean&amp;quot;),  
        avg_rcv_cas = (&amp;#39;away_cas&amp;#39;, &amp;quot;mean&amp;quot;),
        n_games = (&amp;#39;race_name&amp;#39;, &amp;quot;count&amp;quot;)
    )
    .sort_values( &amp;#39;n_games&amp;#39;, ascending = False)
    .reset_index()) # this adds the group by variables (now index) as a column

res = res.dropna()

(p9.ggplot(data = res.query(&amp;#39;n_games &amp;gt; 10&amp;#39;), 
            mapping = p9.aes(x = &amp;#39;week_date&amp;#39;, y = &amp;#39;avg_comp&amp;#39;, 
                            size = &amp;#39;n_games&amp;#39;, color = &amp;#39;factor(division_name)&amp;#39;, shape = &amp;#39;factor(tv_bin)&amp;#39;))
    + p9.geom_point()
    + p9.scale_size_area() 
    + p9.geom_vline(xintercept = &amp;#39;2021-09-01&amp;#39;, color = &amp;quot;red&amp;quot;)    
    + p9.theme(figure_size = (10, 5))
    + p9.ggtitle(&amp;quot;average completions per game BB2020 vs BB2016&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;/home/gertjan/venvs/requests_env/lib/python3.6/site-packages/plotnine/scales/scale_shape.py:85: PlotnineWarning: Using shapes for an ordinal variable is not advised.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2022-03-20_nufflytics_blog_post_files/nufflytics_blog_post_13_1.png&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;ggplot: (-9223363302350371093)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;divisions = [&amp;#39;Ranked&amp;#39;, &amp;#39;Blackbox&amp;#39;, &amp;#39;Competitive&amp;#39;]

tv_bins = [&amp;#39;1.1M&amp;#39;, &amp;#39;1.4M&amp;#39;, &amp;#39;1.7M&amp;#39;]

res = (df_mbt[df_mbt[&amp;#39;division_name&amp;#39;].isin(divisions)]
    .loc[df_mbt[&amp;#39;tv_bin&amp;#39;].isin(tv_bins)]
    .query(&amp;quot;mirror_match == 0 &amp;amp; has_sp == 0 &amp;amp; tv_bin in @tv_bins &amp;amp; division_name in @divisions&amp;quot;)
    .groupby([&amp;#39;ruleset_version&amp;#39;, &amp;#39;race_name&amp;#39;, &amp;#39;tv_bin&amp;#39;])
    .agg(        
        avg_comp = (&amp;#39;home_comp&amp;#39;, &amp;quot;mean&amp;quot;),
        avg_pass = (&amp;#39;home_pass&amp;#39;, &amp;quot;mean&amp;quot;),
        avg_foul = (&amp;#39;home_foul&amp;#39;, &amp;quot;mean&amp;quot;),
        n_games = (&amp;#39;race_name&amp;#39;, &amp;quot;count&amp;quot;)
    )
    .sort_values( &amp;#39;n_games&amp;#39;, ascending = False)
    .reset_index()) # this adds the group by variables (now index) as a column

res = res.dropna()

(p9.ggplot(data = res.query(&amp;#39;n_games &amp;gt; 10 &amp;amp; tv_bin == &amp;quot;1.1M&amp;quot;&amp;#39;), 
            mapping = p9.aes(y = &amp;#39;reorder(race_name, avg_comp)&amp;#39;, x = &amp;#39;avg_comp&amp;#39;, 
                            size = &amp;#39;n_games&amp;#39;, group = &amp;#39;factor(ruleset_version)&amp;#39;, 
                            color = &amp;#39;factor(ruleset_version)&amp;#39;))
    + p9.geom_point()
    + p9.scale_size_area() 
    + p9.ggtitle(&amp;quot;average completions per game BB2016 vs BB2020 at 1.1M TV&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2022-03-20_nufflytics_blog_post_files/nufflytics_blog_post_14_0.png&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;ggplot: (-9223363302350232432)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Across the board we see a decrease in average completions per match. Note that this is for low team values, at around 1.1M, between 950K and 1250K.&lt;/p&gt;
&lt;p&gt;Observations that stand out:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;High agility teams such as Elven Union, Wood Elf and Skaven show large drops,&lt;/li&gt;
&lt;li&gt;Dark elves show the largest relative drop (more than halving in completions),&lt;/li&gt;
&lt;li&gt;High Elves are hardly affected, as well as Humans,&lt;/li&gt;
&lt;li&gt;Halflings show a large increase.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;and-what-about-fouling-in-bb2020&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;And what about fouling in BB2020?&lt;/h1&gt;
&lt;p&gt;Also for fouling Blood Bowl 2020 brought some changes to the rules. The &lt;strong&gt;Sneaky Git&lt;/strong&gt; skill became better, allowing a player to continue moving after the foul has been committed. The &lt;strong&gt;Black Orcs&lt;/strong&gt; were added as a new team, that show fouling potential: they have access to cheap bribes, the &lt;strong&gt;Grab&lt;/strong&gt; skill to set up a foul, and cheap goblin bruisers to quickly move around the pitch. And there was of course the &lt;strong&gt;swarming&lt;/strong&gt; for the Underworld and Snotling teams, that provides a continuous supply of disposable players to foul with. For more detail I refer to a nice post by king_ghidra at &lt;a href=&#34;https://bloodbowlstrategies.com/en/tactics-blood-bowl-second-season/&#34;&gt;Blood Bowl Strategies&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let’s see how the stats were affected!&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;divisions = [&amp;#39;Ranked&amp;#39;, &amp;#39;Blackbox&amp;#39;, &amp;#39;Competitive&amp;#39;]

tv_bins = [&amp;#39;1.1M&amp;#39;, &amp;#39;1.2M&amp;#39;, &amp;#39;1.3M&amp;#39;, &amp;#39;1.4M&amp;#39;]

res = (df_mbt[df_mbt[&amp;#39;division_name&amp;#39;].isin(divisions)]
    .loc[df_mbt[&amp;#39;tv_bin2&amp;#39;].isin(tv_bins)]
    .query(&amp;quot;mirror_match == 0 &amp;amp; has_sp == 0 &amp;amp; tv_bin2 in @tv_bins &amp;amp; division_name in @divisions&amp;quot;)
    .groupby([&amp;#39;ruleset_version&amp;#39;, &amp;#39;race_name&amp;#39;, &amp;#39;tv_bin2&amp;#39;])
    .agg(        
        avg_comp = (&amp;#39;home_comp&amp;#39;, &amp;quot;mean&amp;quot;),
        avg_pass = (&amp;#39;home_pass&amp;#39;, &amp;quot;mean&amp;quot;),
        avg_foul = (&amp;#39;home_foul&amp;#39;, &amp;quot;mean&amp;quot;),
        n_games = (&amp;#39;race_name&amp;#39;, &amp;quot;count&amp;quot;)
    )
    .sort_values( &amp;#39;n_games&amp;#39;, ascending = False)
    .reset_index()) # this adds the group by variables (now index) as a column

res = res.dropna()

(p9.ggplot(data = res.query(&amp;#39;n_games &amp;gt; 10&amp;#39;), 
            mapping = p9.aes(y = &amp;#39;reorder(race_name, avg_foul)&amp;#39;, x = &amp;#39;avg_foul&amp;#39;, 
                            size = &amp;#39;n_games&amp;#39;, color = &amp;#39;factor(ruleset_version)&amp;#39;, 
                            shape = &amp;#39;factor(tv_bin2)&amp;#39;))
    + p9.geom_point()
    + p9.scale_size_area() 
    + p9.ggtitle(&amp;quot;average number of fouls per game BB2016 vs BB2020&amp;quot;)
    + p9.ylab(&amp;quot;&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2022-03-20_nufflytics_blog_post_files/nufflytics_blog_post_17_1.png&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;ggplot: (8734504437775)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From this plot, we can see that BB2020 indeed shows increased fouling across the board. As expected, the Black Orcs are high up in the fouling charts, and we see large increases in fouling for Underworld, Goblins, Halflings and Snotlings. We can also see that as teams develop, fouling typically increases, possible related to developing a specialized fouling player with the sneaky git skill. For humans access to cheap agile halfling hopefulls with access to sneaky git increased fouling opportunity. In Orc teams goblins can forfill this role.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;competitive-division-win-rates-and-malta-eurobowl-2022-tiers&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Competitive division win rates and Malta Eurobowl 2022 tiers&lt;/h1&gt;
&lt;p&gt;Last but not least, a win rate analysis. In a tournament setting, elaborate tiering systems are in place to compensate for differences in race strength, skills are selected from skill packs, and player casualties are forgotten with each match played with a “resurrected” fresh team. Furthermore, opponents are randomly assigned and must be played. Contrast this with the FUMBBL Competitive division: Here teams start with 1M gold and without any extra skills. Teams must be developed, like in a league, and opponents can be strategically chosen based on which race they play, their coach rating etc.&lt;/p&gt;
&lt;p&gt;With all this in mind, I tried the impossible: to compare relative team strength as expected by the tournament tiers, with the observed win rates in the Competitive Division. With the Eurobowl 2022 in Malta coming up, I decided to approximate the conditions of that rulepack. Teams are created using 1.15M gold, as well as roughly 36 SPP worth of skills. This translates to 6 primary skills worth 20K, giving us a total team value of 1270K, say around 1.3M.&lt;/p&gt;
&lt;p&gt;To correct for differences in coaching ability, I restricted the match selection for matches where coach ratings are not too different (&amp;lt; 10), and above 150. I excluded matches involving Star players, and mirror matches (I.e. Orcs vs Orcs).
To distinguish the relatively small % differences in win rate, we need to have a bandwidth around 1.3M to get sufficient statistics for each team.
I included confidence intervals to visualize the statistical uncertainty for the win rates.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Function for computing confidence intervals
from statsmodels.stats.proportion import proportion_confint   

divisions = [&amp;#39;Competitive&amp;#39;]

tv_bins = [&amp;#39;1.2M&amp;#39;, &amp;#39;1.3M&amp;#39;, &amp;#39;1.4M&amp;#39;]

res = (df_mbt[df_mbt[&amp;#39;division_name&amp;#39;].isin(divisions)]
    .loc[df_mbt[&amp;#39;tv_bin2&amp;#39;].isin(tv_bins)]
    .query(&amp;#39;mirror_match == 0 &amp;amp; has_sp == 0 &amp;amp; CR_diff &amp;lt; 10 &amp;amp; coach_CR &amp;gt; 150&amp;#39;)
    .groupby([&amp;#39;race_name&amp;#39;, &amp;#39;ruleset_version&amp;#39;, &amp;#39;Malta_2022&amp;#39;])
    .agg(        
        perc_win = (&amp;#39;wins&amp;#39;, &amp;quot;mean&amp;quot;),
        n_wins = (&amp;#39;wins&amp;#39;, &amp;quot;sum&amp;quot;),
        n_games = (&amp;#39;race_name&amp;#39;, &amp;quot;count&amp;quot;)
    )
    .query(&amp;#39;n_games &amp;gt; 0&amp;#39;)
    .reset_index()) # this adds the group by variable (now index) as a column

res[&amp;#39;lower_CI&amp;#39;], res[&amp;#39;upper_CI&amp;#39;] =  proportion_confint(
                                      count = round(res[&amp;#39;n_wins&amp;#39;]).astype(int),
                                      nobs = res[&amp;#39;n_games&amp;#39;],
                                      alpha = 0.05
                                  )

(p9.ggplot(data = res.query(&amp;#39;n_games &amp;gt; 30&amp;#39;), 
            mapping = p9.aes(x = &amp;#39;reorder(race_name, -Malta_2022)&amp;#39;, y = &amp;#39;perc_win&amp;#39;, 
            size = &amp;#39;n_games&amp;#39;, color = &amp;#39;factor(Malta_2022)&amp;#39;))
    + p9.geom_linerange(p9.aes(ymin = &amp;#39;lower_CI&amp;#39;, ymax = &amp;#39;upper_CI&amp;#39;), size = 1)
    + p9.geom_point()
    + p9.scale_size_area() 
    + p9.coord_flip()
    + p9.geom_hline(yintercept = 0.5)
    + p9.ggtitle(&amp;quot;FUMBBL BB2020 win rates around 1.3M&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2022-03-20_nufflytics_blog_post_files/nufflytics_blog_post_20_0.png&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;ggplot: (-9223363302334269892)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First off, I think the most important lesson here is that it is really difficult to compare win rates across such different settings.&lt;/p&gt;
&lt;p&gt;But what info can we squeeze from this plot nevertheless:&lt;/p&gt;
&lt;p&gt;It seems that Amazon and Underworld have higher FUMBBL win rates than expected based on their Malta 2022 tier.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For Amazon, this might be due to being able to avoid particular opponents on FUMBBL, such as Dwarves and Chaos Dwarves, with a lot of Tackle.&lt;/li&gt;
&lt;li&gt;For Underworld, this is likely related to their improvements in BB2020, leading the charts at NAF tournaments, that resulted in recent rule changes that weakened them with the November 2021 Games Workshop ruling.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What else do we got:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Nurgle has a relatively low win rate compared to other teams with the same tier.&lt;/li&gt;
&lt;li&gt;High Elf has a low tier, but shows an above average win rate, and appears to perform well at NAF tournaments. Curious to see how this race will do this year.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;concluding-remarks-and-acknowledgements&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Concluding Remarks and acknowledgements&lt;/h1&gt;
&lt;p&gt;The analyses above hopefully give you some idea what can be with the rich FUMMBL data available.&lt;/p&gt;
&lt;p&gt;One last application: The data can also be used to search for matches based on highly particular search criteria: for example, if you are interested in Snotling matches that induce Morg N Thorg and play against a skilled Elf coach. The &lt;code&gt;match_id&lt;/code&gt; can then be used to watch the replay on FUMBBL.&lt;/p&gt;
&lt;p&gt;Finally, some acknowledgements. While writing this blog, I drew inspiration from several sources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;most notable the &lt;a href=&#34;https://www.fumbbl.com&#34;&gt;FUMBBL website itself&lt;/a&gt; that has a wealth of statistics available,&lt;/li&gt;
&lt;li&gt;the &lt;a href=&#34;https://fumbbldata.azurewebsites.net/stats.html&#34;&gt;website of FUMBBL coach Koadah&lt;/a&gt; with aggregated FUMBBL stats,&lt;/li&gt;
&lt;li&gt;the &lt;a href=&#34;https://public.tableau.com/app/profile/mike.sann0638.davies/viz/TheNAFReport/Games&#34;&gt;NAF monthly reports&lt;/a&gt; by Mike Davies,&lt;/li&gt;
&lt;li&gt;a &lt;a href=&#34;https://bloodbowlstrategies.com/en/relative-strength-of-teams/&#34;&gt;blog post on team strength&lt;/a&gt; by Taureau Amiral ,&lt;/li&gt;
&lt;li&gt;the &lt;a href=&#34;https://nufflytics.com&#34;&gt;Nufflytics blog&lt;/a&gt; by Blood Bowl 2 coach Schlice,&lt;/li&gt;
&lt;li&gt;the various technical posts of &lt;a href=&#34;https://fumbbl.com/~SzieberthAdam&#34;&gt;FUMBBL coach Adam Szieberth&lt;/a&gt; who followed a similar approach using Python API and web scraping FUMBBL data.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Construction of a public dataset of Blood Bowl matches played on FUMBBL.com</title>
      <link>/post/blood-bowl-fumbbl-dataset/</link>
      <pubDate>Thu, 30 Dec 2021 00:00:00 +0000</pubDate>
      <guid>/post/blood-bowl-fumbbl-dataset/</guid>
      <description>


&lt;p&gt;This blogpost is about &lt;strong&gt;Blood Bowl&lt;/strong&gt;, a boardgame of fantasy football that can also be played online at &lt;a href=&#34;https://fumbbl.com&#34;&gt;FUMBBL.com&lt;/a&gt;. The goal of this blog post is to use Python API and HTML scraping to fetch online Blood Bowl match outcome data, and to create a publicly available dataset ready for analysis and visualization. In a &lt;a href=&#34;https://gsverhoeven.github.io/post/blood-bowl-nufflytics/&#34;&gt;separate blog post&lt;/a&gt; I’ll showcase some analyses on this dataset (also known as &lt;a href=&#34;https://www.nufflytics.com&#34;&gt;Nufflytics&lt;/a&gt; in reference to Nuffle, the god of Blood Bowl). The dataset is primarily constructed to analyze rule changes that were introduced on FUMBBL.com last year, and how these affected the relative strengths of the different teams coaches can pick to field against each other.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;N.b. an updated version of this post is available as a Jupyter Notebook in a separate Github Repository &lt;a href=&#34;https://github.com/gsverhoeven/fumbbl_datasets/blob/main/fumbbl_dataset.ipynb&#34;&gt;fumbbl_datasets&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In writing this blog post, I took inspiration from a relatively new development in Open Science, that of the &lt;strong&gt;Data paper&lt;/strong&gt; (Chavan &amp;amp; Penev, 2011). A data paper is a (ideally peer reviewed) publication of a dataset as a stand alone research output. A Data paper can be thought of similar to the “Methods” section of a traditional research article, though with greater detail. A data paper describes the contents of the dataset, the data acquisition process, and includes a discussion of the motivation and considerations regarding experimental design (if applicable). Data papers do not provide any analysis nor results / conclusions. The dataset itself should be online available at a data repository such as &lt;strong&gt;Zenodo&lt;/strong&gt;, &lt;strong&gt;Figshare&lt;/strong&gt; or &lt;strong&gt;Dryad&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;To give an impression of how this works out in practice, here are two examples: The first data paper, &lt;a href=&#34;https://www.nature.com/articles/s41597-019-0247-7&#34;&gt;&lt;em&gt;A public data set of spatio-temporal match events in soccer competitions&lt;/em&gt;&lt;/a&gt;, was published in “Scientific data” with the dataset hosted at &lt;a href=&#34;https://doi.org/10.6084/m9.figshare.c.4415000.v5&#34;&gt;Figshare&lt;/a&gt;. The second is collected from Twitter: &lt;a href=&#34;https://arxiv.org/abs/2004.03688&#34;&gt;&lt;em&gt;A large-scale COVID-19 Twitter chatter dataset for open scientific research – an international collaboration&lt;/em&gt;&lt;/a&gt;, with the open dataset published on &lt;a href=&#34;https://doi.org/10.5281/zenodo.5775023&#34;&gt;Zenodo&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;*Chavan, V., and Penev, L. (2011). The data paper: a mechanism to incentivize data publishing in biodiversity science. BMC Bioinformatics 12(Suppl. 15):S2. &lt;a href=&#34;doi:10.1186/1471-2105-12-S15-S2&#34; class=&#34;uri&#34;&gt;doi:10.1186/1471-2105-12-S15-S2&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;software-needed-to-reproduce-this-blog-post&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Software needed to reproduce this blog post&lt;/h1&gt;
&lt;p&gt;This blogpost is written as a Jupyter notebook containing Python code, and is fully reproducible. The idea is to make Blood Bowl data analysis accessible to others. Using open source tooling reduces the barriers for others to build on other people’s work.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import random
import time

import requests # API library

import numpy as np
import pandas as pd

pd.set_option(&amp;#39;display.max_rows&amp;#39;, 500)
pd.set_option(&amp;#39;display.max_columns&amp;#39;, 500)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;blood-bowl-online-fumbbl&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Blood Bowl online: FUMBBL&lt;/h1&gt;
&lt;p&gt;The &lt;strong&gt;FUMBBL&lt;/strong&gt; website (&lt;a href=&#34;https://fumbbl.com&#34; class=&#34;uri&#34;&gt;https://fumbbl.com&lt;/a&gt;) contains a large amount of data. From coach pages, with their teams, to team rosters, with players, and match histories. It’s all there.&lt;/p&gt;
&lt;p&gt;To obtain &lt;strong&gt;FUMBBL&lt;/strong&gt; data, we need to fetch it match by match, team by team. To do so, the site creator Christer Kaivo-oja, from Sweden, has made an API that allows us to easily fetch data. What follows is a short demonstration how the API works, before we fetch the &lt;strong&gt;FUMBBL&lt;/strong&gt; match and team data of the last 12 months.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;behold-the-power-of-requests&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Behold, the power of Requests&lt;/h1&gt;
&lt;p&gt;I use the &lt;a href=&#34;https://docs.python-requests.org/en/latest/&#34;&gt;Python &lt;strong&gt;Requests&lt;/strong&gt; library&lt;/a&gt; to make the API call over HTTPS and obtain the response from the FUMBLL server. The response is in the JSON format, a &lt;a href=&#34;https://www.json.org/json-en.html&#34;&gt;light-weight data-interchange format&lt;/a&gt; which is both easy to read and write for humans, and easy to parse and generate by computers. So this makes it a natural choice for an API.&lt;/p&gt;
&lt;p&gt;Here is an example of what is available at the coach level. The full documentation of the API can be found at (&lt;a href=&#34;https://fumbbl.com/apidoc/&#34; class=&#34;uri&#34;&gt;https://fumbbl.com/apidoc/&lt;/a&gt;).&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;response = requests.get(&amp;quot;https://fumbbl.com/api/coach/teams/gsverhoeven&amp;quot;)
# display the complete JSON object {}
response.json()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;{&amp;#39;id&amp;#39;: 255851,
 &amp;#39;name&amp;#39;: &amp;#39;gsverhoeven&amp;#39;,
 &amp;#39;teams&amp;#39;: [{&amp;#39;id&amp;#39;: 1003452,
   &amp;#39;coachId&amp;#39;: 255851,
   &amp;#39;name&amp;#39;: &amp;#39;Hillywood Hellraisers&amp;#39;,
   &amp;#39;rosterId&amp;#39;: 50,
   &amp;#39;race&amp;#39;: &amp;#39;Human&amp;#39;,
   &amp;#39;teamValue&amp;#39;: 1090000,
   &amp;#39;canLfg&amp;#39;: &amp;#39;Yes&amp;#39;,
   &amp;#39;isLfg&amp;#39;: &amp;#39;Yes&amp;#39;,
   &amp;#39;games&amp;#39;: &amp;#39;3&amp;#39;,
   &amp;#39;divisionId&amp;#39;: 1,
   &amp;#39;division&amp;#39;: &amp;#39;Ranked&amp;#39;,
   &amp;#39;leagueId&amp;#39;: 0,
   &amp;#39;league&amp;#39;: None,
   &amp;#39;status&amp;#39;: &amp;#39;Retired&amp;#39;,
   &amp;#39;raceLogos&amp;#39;: [{&amp;#39;size&amp;#39;: 32, &amp;#39;logo&amp;#39;: 486290},
    {&amp;#39;size&amp;#39;: 48, &amp;#39;logo&amp;#39;: 486291},
    {&amp;#39;size&amp;#39;: 64, &amp;#39;logo&amp;#39;: 486292},
    {&amp;#39;size&amp;#39;: 96, &amp;#39;logo&amp;#39;: 486293},
    {&amp;#39;size&amp;#39;: 128, &amp;#39;logo&amp;#39;: 486294},
    {&amp;#39;size&amp;#39;: 192, &amp;#39;logo&amp;#39;: 486295}]},
  {&amp;#39;id&amp;#39;: 1035833,
   &amp;#39;coachId&amp;#39;: 255851,
   &amp;#39;name&amp;#39;: &amp;#39;Pharaoh Munchers&amp;#39;,
   &amp;#39;rosterId&amp;#39;: 51,
   &amp;#39;race&amp;#39;: &amp;#39;Tomb Kings&amp;#39;,
   &amp;#39;teamValue&amp;#39;: 960000,
   &amp;#39;canLfg&amp;#39;: &amp;#39;Yes&amp;#39;,
   &amp;#39;isLfg&amp;#39;: &amp;#39;No&amp;#39;,
   &amp;#39;games&amp;#39;: &amp;#39;0&amp;#39;,
   &amp;#39;divisionId&amp;#39;: 1,
   &amp;#39;division&amp;#39;: &amp;#39;Ranked&amp;#39;,
   &amp;#39;leagueId&amp;#39;: 0,
   &amp;#39;league&amp;#39;: None,
   &amp;#39;status&amp;#39;: &amp;#39;Active&amp;#39;,
   &amp;#39;raceLogos&amp;#39;: [{&amp;#39;size&amp;#39;: 32, &amp;#39;logo&amp;#39;: 486296},
    {&amp;#39;size&amp;#39;: 48, &amp;#39;logo&amp;#39;: 486297},
    {&amp;#39;size&amp;#39;: 64, &amp;#39;logo&amp;#39;: 486298},
    {&amp;#39;size&amp;#39;: 96, &amp;#39;logo&amp;#39;: 486299},
    {&amp;#39;size&amp;#39;: 128, &amp;#39;logo&amp;#39;: 486300},
    {&amp;#39;size&amp;#39;: 192, &amp;#39;logo&amp;#39;: 486301}]},
  {&amp;#39;id&amp;#39;: 1035835,
   &amp;#39;coachId&amp;#39;: 255851,
   &amp;#39;name&amp;#39;: &amp;#39;Blackbox Bastards&amp;#39;,
   &amp;#39;rosterId&amp;#39;: 50,
   &amp;#39;race&amp;#39;: &amp;#39;Human&amp;#39;,
   &amp;#39;teamValue&amp;#39;: 840000,
   &amp;#39;canLfg&amp;#39;: &amp;#39;No&amp;#39;,
   &amp;#39;isLfg&amp;#39;: &amp;#39;No&amp;#39;,
   &amp;#39;games&amp;#39;: &amp;#39;1&amp;#39;,
   &amp;#39;divisionId&amp;#39;: 10,
   &amp;#39;division&amp;#39;: &amp;#39;Blackbox&amp;#39;,
   &amp;#39;leagueId&amp;#39;: 0,
   &amp;#39;league&amp;#39;: None,
   &amp;#39;status&amp;#39;: &amp;#39;Retired&amp;#39;,
   &amp;#39;raceLogos&amp;#39;: [{&amp;#39;size&amp;#39;: 32, &amp;#39;logo&amp;#39;: 486290},
    {&amp;#39;size&amp;#39;: 48, &amp;#39;logo&amp;#39;: 486291},
    {&amp;#39;size&amp;#39;: 64, &amp;#39;logo&amp;#39;: 486292},
    {&amp;#39;size&amp;#39;: 96, &amp;#39;logo&amp;#39;: 486293},
    {&amp;#39;size&amp;#39;: 128, &amp;#39;logo&amp;#39;: 486294},
    {&amp;#39;size&amp;#39;: 192, &amp;#39;logo&amp;#39;: 486295}]},
  {&amp;#39;id&amp;#39;: 1036599,
   &amp;#39;coachId&amp;#39;: 255851,
   &amp;#39;name&amp;#39;: &amp;#39;Seven cities of Gold&amp;#39;,
   &amp;#39;rosterId&amp;#39;: 52,
   &amp;#39;race&amp;#39;: &amp;#39;Lizardmen&amp;#39;,
   &amp;#39;teamValue&amp;#39;: 1000000,
   &amp;#39;canLfg&amp;#39;: &amp;#39;Yes&amp;#39;,
   &amp;#39;isLfg&amp;#39;: &amp;#39;No&amp;#39;,
   &amp;#39;games&amp;#39;: &amp;#39;0&amp;#39;,
   &amp;#39;divisionId&amp;#39;: 1,
   &amp;#39;division&amp;#39;: &amp;#39;Ranked&amp;#39;,
   &amp;#39;leagueId&amp;#39;: 0,
   &amp;#39;league&amp;#39;: None,
   &amp;#39;status&amp;#39;: &amp;#39;Active&amp;#39;,
   &amp;#39;raceLogos&amp;#39;: [{&amp;#39;size&amp;#39;: 32, &amp;#39;logo&amp;#39;: 486302},
    {&amp;#39;size&amp;#39;: 48, &amp;#39;logo&amp;#39;: 486303},
    {&amp;#39;size&amp;#39;: 64, &amp;#39;logo&amp;#39;: 486304},
    {&amp;#39;size&amp;#39;: 96, &amp;#39;logo&amp;#39;: 486305},
    {&amp;#39;size&amp;#39;: 128, &amp;#39;logo&amp;#39;: 486306},
    {&amp;#39;size&amp;#39;: 192, &amp;#39;logo&amp;#39;: 486307}]},
  {&amp;#39;id&amp;#39;: 1038960,
   &amp;#39;coachId&amp;#39;: 255851,
   &amp;#39;name&amp;#39;: &amp;#39;Hillywood Hellraisers 2.0&amp;#39;,
   &amp;#39;rosterId&amp;#39;: 50,
   &amp;#39;race&amp;#39;: &amp;#39;Human&amp;#39;,
   &amp;#39;teamValue&amp;#39;: 1110000,
   &amp;#39;canLfg&amp;#39;: &amp;#39;Yes&amp;#39;,
   &amp;#39;isLfg&amp;#39;: &amp;#39;Yes&amp;#39;,
   &amp;#39;games&amp;#39;: &amp;#39;4&amp;#39;,
   &amp;#39;divisionId&amp;#39;: 1,
   &amp;#39;division&amp;#39;: &amp;#39;Ranked&amp;#39;,
   &amp;#39;leagueId&amp;#39;: 0,
   &amp;#39;league&amp;#39;: None,
   &amp;#39;status&amp;#39;: &amp;#39;Active&amp;#39;,
   &amp;#39;raceLogos&amp;#39;: [{&amp;#39;size&amp;#39;: 32, &amp;#39;logo&amp;#39;: 486290},
    {&amp;#39;size&amp;#39;: 48, &amp;#39;logo&amp;#39;: 486291},
    {&amp;#39;size&amp;#39;: 64, &amp;#39;logo&amp;#39;: 486292},
    {&amp;#39;size&amp;#39;: 96, &amp;#39;logo&amp;#39;: 486293},
    {&amp;#39;size&amp;#39;: 128, &amp;#39;logo&amp;#39;: 486294},
    {&amp;#39;size&amp;#39;: 192, &amp;#39;logo&amp;#39;: 486295}]},
  {&amp;#39;id&amp;#39;: 1050267,
   &amp;#39;coachId&amp;#39;: 255851,
   &amp;#39;name&amp;#39;: &amp;#39;Gooische Heidebeukers&amp;#39;,
   &amp;#39;rosterId&amp;#39;: 4974,
   &amp;#39;race&amp;#39;: &amp;#39;Orc&amp;#39;,
   &amp;#39;teamValue&amp;#39;: 1330000,
   &amp;#39;canLfg&amp;#39;: &amp;#39;Yes&amp;#39;,
   &amp;#39;isLfg&amp;#39;: &amp;#39;Yes&amp;#39;,
   &amp;#39;games&amp;#39;: &amp;#39;6&amp;#39;,
   &amp;#39;divisionId&amp;#39;: 5,
   &amp;#39;division&amp;#39;: &amp;#39;League&amp;#39;,
   &amp;#39;leagueId&amp;#39;: 14883,
   &amp;#39;league&amp;#39;: &amp;#39;Benelux Hate Bowl&amp;#39;,
   &amp;#39;status&amp;#39;: &amp;#39;Post Match Sequence&amp;#39;,
   &amp;#39;raceLogos&amp;#39;: [{&amp;#39;size&amp;#39;: 32, &amp;#39;logo&amp;#39;: 486332},
    {&amp;#39;size&amp;#39;: 48, &amp;#39;logo&amp;#39;: 486333},
    {&amp;#39;size&amp;#39;: 64, &amp;#39;logo&amp;#39;: 486334},
    {&amp;#39;size&amp;#39;: 96, &amp;#39;logo&amp;#39;: 486335},
    {&amp;#39;size&amp;#39;: 128, &amp;#39;logo&amp;#39;: 486336},
    {&amp;#39;size&amp;#39;: 192, &amp;#39;logo&amp;#39;: 486337}]},
  {&amp;#39;id&amp;#39;: 1052980,
   &amp;#39;coachId&amp;#39;: 255851,
   &amp;#39;name&amp;#39;: &amp;#39;[2020] Hillywood Hellraisers&amp;#39;,
   &amp;#39;rosterId&amp;#39;: 4964,
   &amp;#39;race&amp;#39;: &amp;#39;Human&amp;#39;,
   &amp;#39;teamValue&amp;#39;: 1000000,
   &amp;#39;canLfg&amp;#39;: &amp;#39;Yes&amp;#39;,
   &amp;#39;isLfg&amp;#39;: &amp;#39;No&amp;#39;,
   &amp;#39;games&amp;#39;: &amp;#39;0&amp;#39;,
   &amp;#39;divisionId&amp;#39;: 2,
   &amp;#39;division&amp;#39;: &amp;#39;Competitive&amp;#39;,
   &amp;#39;leagueId&amp;#39;: 0,
   &amp;#39;league&amp;#39;: None,
   &amp;#39;status&amp;#39;: &amp;#39;Active&amp;#39;,
   &amp;#39;raceLogos&amp;#39;: [{&amp;#39;size&amp;#39;: 32, &amp;#39;logo&amp;#39;: 486290},
    {&amp;#39;size&amp;#39;: 48, &amp;#39;logo&amp;#39;: 486291},
    {&amp;#39;size&amp;#39;: 64, &amp;#39;logo&amp;#39;: 486292},
    {&amp;#39;size&amp;#39;: 96, &amp;#39;logo&amp;#39;: 486293},
    {&amp;#39;size&amp;#39;: 128, &amp;#39;logo&amp;#39;: 486294},
    {&amp;#39;size&amp;#39;: 192, &amp;#39;logo&amp;#39;: 486295}]},
  {&amp;#39;id&amp;#39;: 1053065,
   &amp;#39;coachId&amp;#39;: 255851,
   &amp;#39;name&amp;#39;: &amp;#39;[2020] Grrrl power&amp;#39;,
   &amp;#39;rosterId&amp;#39;: 5141,
   &amp;#39;race&amp;#39;: &amp;#39;Amazon&amp;#39;,
   &amp;#39;teamValue&amp;#39;: 960000,
   &amp;#39;canLfg&amp;#39;: &amp;#39;Yes&amp;#39;,
   &amp;#39;isLfg&amp;#39;: &amp;#39;No&amp;#39;,
   &amp;#39;games&amp;#39;: &amp;#39;0&amp;#39;,
   &amp;#39;divisionId&amp;#39;: 2,
   &amp;#39;division&amp;#39;: &amp;#39;Competitive&amp;#39;,
   &amp;#39;leagueId&amp;#39;: 0,
   &amp;#39;league&amp;#39;: None,
   &amp;#39;status&amp;#39;: &amp;#39;Active&amp;#39;,
   &amp;#39;raceLogos&amp;#39;: [{&amp;#39;size&amp;#39;: 32, &amp;#39;logo&amp;#39;: 486194},
    {&amp;#39;size&amp;#39;: 48, &amp;#39;logo&amp;#39;: 486192},
    {&amp;#39;size&amp;#39;: 64, &amp;#39;logo&amp;#39;: 486195},
    {&amp;#39;size&amp;#39;: 96, &amp;#39;logo&amp;#39;: 486191},
    {&amp;#39;size&amp;#39;: 128, &amp;#39;logo&amp;#39;: 486193},
    {&amp;#39;size&amp;#39;: 192, &amp;#39;logo&amp;#39;: 486190}]},
  {&amp;#39;id&amp;#39;: 1060696,
   &amp;#39;coachId&amp;#39;: 255851,
   &amp;#39;name&amp;#39;: &amp;#39;Hakflem support team&amp;#39;,
   &amp;#39;rosterId&amp;#39;: 4978,
   &amp;#39;race&amp;#39;: &amp;#39;Underworld Denizens&amp;#39;,
   &amp;#39;teamValue&amp;#39;: 730000,
   &amp;#39;canLfg&amp;#39;: &amp;#39;Yes&amp;#39;,
   &amp;#39;isLfg&amp;#39;: &amp;#39;No&amp;#39;,
   &amp;#39;games&amp;#39;: &amp;#39;0&amp;#39;,
   &amp;#39;divisionId&amp;#39;: 2,
   &amp;#39;division&amp;#39;: &amp;#39;Competitive&amp;#39;,
   &amp;#39;leagueId&amp;#39;: 0,
   &amp;#39;league&amp;#39;: None,
   &amp;#39;status&amp;#39;: &amp;#39;Active&amp;#39;,
   &amp;#39;raceLogos&amp;#39;: [{&amp;#39;size&amp;#39;: 32, &amp;#39;logo&amp;#39;: 603383},
    {&amp;#39;size&amp;#39;: 48, &amp;#39;logo&amp;#39;: 603381},
    {&amp;#39;size&amp;#39;: 64, &amp;#39;logo&amp;#39;: 603378},
    {&amp;#39;size&amp;#39;: 96, &amp;#39;logo&amp;#39;: 603382},
    {&amp;#39;size&amp;#39;: 128, &amp;#39;logo&amp;#39;: 603380},
    {&amp;#39;size&amp;#39;: 192, &amp;#39;logo&amp;#39;: 603379}]}]}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;parsing-json-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Parsing JSON data&lt;/h1&gt;
&lt;p&gt;Let’s have a closer look at the JSON data structure here.
We have a list of key-value pairs.
Using brackets &lt;code&gt;[]&lt;/code&gt; we can choose keys and retrieve their values.
Some keys contain simple values, such as &lt;code&gt;name&lt;/code&gt;,&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;response.json()[&amp;#39;name&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;gsverhoeven&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;but some return as value a new list of key-value pairs, such as &lt;code&gt;teams&lt;/code&gt;.
Actually this is a “list of”lists of key-value pairs”, since we have a separate list for each team.
Even the list of a single team contains new structure, for example under the key &lt;code&gt;raceLogos&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;response.json()[&amp;#39;teams&amp;#39;][2][&amp;#39;raceLogos&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[{&amp;#39;size&amp;#39;: 32, &amp;#39;logo&amp;#39;: 486290},
 {&amp;#39;size&amp;#39;: 48, &amp;#39;logo&amp;#39;: 486291},
 {&amp;#39;size&amp;#39;: 64, &amp;#39;logo&amp;#39;: 486292},
 {&amp;#39;size&amp;#39;: 96, &amp;#39;logo&amp;#39;: 486293},
 {&amp;#39;size&amp;#39;: 128, &amp;#39;logo&amp;#39;: 486294},
 {&amp;#39;size&amp;#39;: 192, &amp;#39;logo&amp;#39;: 486295}]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;response.json()[&amp;#39;teams&amp;#39;][2][&amp;#39;name&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;Blackbox Bastards&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;what-data-do-we-need-and-in-what-shape&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What data do we need? And in what shape?&lt;/h1&gt;
&lt;p&gt;Now we know how the data comes in, we need to think about which variables we want, and how to structure them.
The most straightforward level to analyze race strength is to look at &lt;strong&gt;match outcomes&lt;/strong&gt;.
At its core, the data consists of matches played by teams, commanded by coaches.
Furthermore, we expect race strength to change over time, as new strategies are discovered by the players, or new rules get introduced. So the time dimension is important as well.&lt;/p&gt;
&lt;p&gt;So, let’s go with a flat data frame with &lt;strong&gt;rows for each match&lt;/strong&gt;, and columns for the various variables associated with each match.
These would include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Coach ids&lt;/li&gt;
&lt;li&gt;Team races&lt;/li&gt;
&lt;li&gt;Team ids&lt;/li&gt;
&lt;li&gt;Date of the match&lt;/li&gt;
&lt;li&gt;Outcome (Touchdowns of both teams)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With this basic structure, we can add as many match related variables in the future, keeping the basic structure (each row is a match) unchanged.&lt;/p&gt;
&lt;p&gt;So lets get the match data!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-1-api-scraping-the-match-data-df_matches&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Step 1: API scraping the match data: df_matches&lt;/h1&gt;
&lt;p&gt;So we are mostly interested in the current ruleset, this is &lt;code&gt;BB2020&lt;/code&gt;. This ruleset became available for play on &lt;strong&gt;FUMBBL&lt;/strong&gt; at september 1st 2021, and two months later, already some 5000 games have been played. We also want to compare with the previous ruleset, where we have much more data available. How far do we go back?
Let’s go for roughly 12 months of &lt;code&gt;BB2016&lt;/code&gt; ruleset matches, and a few months of &lt;code&gt;BB2020&lt;/code&gt; matches.&lt;/p&gt;
&lt;p&gt;The easiest way to collect match data over a particular period of time is to just loop over &lt;code&gt;match_id&lt;/code&gt;. The most recent match at the time of writing was 4.347.800, and since roughly 100.000 matches are played each year, we can fiddle about and we find match 4.216.258 played on august 1st, 2020. So that means we need to collect some 130K matches.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;VERY IMPORTANT&lt;/strong&gt;: We do not want to overload the &lt;strong&gt;FUMBBL&lt;/strong&gt; server, so we make only three API requests per second. In this way, the server load is hardly affected and it can continue functioning properly for all the Blood Bowl coaches playing their daily games!&lt;/p&gt;
&lt;p&gt;To collect 130K matches, we will need 130000*0.333/3600 = 12 hours.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# estimated hours fetching data
(4347800-4216257)*0.333/3600&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;12.16773&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df_matches = pd.DataFrame(columns=[&amp;#39;match_id&amp;#39;, &amp;#39;match_date&amp;#39;, &amp;#39;match_time&amp;#39;,  
    &amp;#39;team1_id&amp;#39;, &amp;#39;team1_coach_id&amp;#39;, &amp;#39;team1_roster_id&amp;#39;, &amp;#39;team1_race_name&amp;#39;, &amp;#39;team1_value&amp;#39;,
    &amp;#39;team2_id&amp;#39;, &amp;#39;team2_coach_id&amp;#39;, &amp;#39;team2_roster_id&amp;#39;, &amp;#39;team2_race_name&amp;#39;, &amp;#39;team2_value&amp;#39;,
    &amp;#39;team1_score&amp;#39;, &amp;#39;team2_score&amp;#39;])

target = &amp;#39;data/df_matches_&amp;#39; + time.strftime(&amp;quot;%Y%m%d_%H%M%S&amp;quot;) + &amp;#39;.h5&amp;#39;
print(target)

end_match = 4347800
begin_match = 4216258
n_matches = end_match - begin_match
full_run = 0
print(n_matches)

if(full_run):
    for i in range(n_matches):
        api_string = &amp;quot;https://fumbbl.com/api/match/get/&amp;quot; + str(end_match - i)
        # wait 0.33 s on average between each API call
        wait_time = (random.uniform(0.5, 1) + 0.25)/3
        time.sleep(wait_time)
        match = requests.get(api_string)
        match = match.json()
        if match: # fix for matches that do not exist
            match_id = match[&amp;#39;id&amp;#39;]
            match_date = match[&amp;#39;date&amp;#39;]
            match_time = match[&amp;#39;time&amp;#39;]
            team1_id = match[&amp;#39;team1&amp;#39;][&amp;#39;id&amp;#39;]
            team2_id = match[&amp;#39;team2&amp;#39;][&amp;#39;id&amp;#39;]
            team1_score = match[&amp;#39;team1&amp;#39;][&amp;#39;score&amp;#39;]
            team2_score = match[&amp;#39;team2&amp;#39;][&amp;#39;score&amp;#39;]  
            team1_roster_id = match[&amp;#39;team1&amp;#39;][&amp;#39;roster&amp;#39;][&amp;#39;id&amp;#39;]
            team2_roster_id = match[&amp;#39;team2&amp;#39;][&amp;#39;roster&amp;#39;][&amp;#39;id&amp;#39;]            
            team1_coach_id = match[&amp;#39;team1&amp;#39;][&amp;#39;coach&amp;#39;][&amp;#39;id&amp;#39;]
            team2_coach_id = match[&amp;#39;team2&amp;#39;][&amp;#39;coach&amp;#39;][&amp;#39;id&amp;#39;]
            team1_race_name = match[&amp;#39;team1&amp;#39;][&amp;#39;roster&amp;#39;][&amp;#39;name&amp;#39;] 
            team2_race_name = match[&amp;#39;team2&amp;#39;][&amp;#39;roster&amp;#39;][&amp;#39;name&amp;#39;] 
            team1_value = match[&amp;#39;team1&amp;#39;][&amp;#39;teamValue&amp;#39;]
            team2_value = match[&amp;#39;team2&amp;#39;][&amp;#39;teamValue&amp;#39;]
            #print(match_id)     
            df_matches.loc[i] = [match_id, match_date, match_time, 
                team1_id, team1_coach_id, team1_roster_id, team1_race_name, team1_value,
                team2_id, team2_coach_id, team2_roster_id, team2_race_name, team2_value,
                team1_score, team2_score]
        else:
            # empty data for this match, create empty row
            match_id = int(end_match - i)
            df_matches.loc[i] = [np.NaN, np.NaN, np.NaN, 
            np.NaN,np.NaN,np.NaN,np.NaN,
            np.NaN,np.NaN,np.NaN,np.NaN,
            np.NaN,np.NaN, np.NaN, np.NaN] # try np.repeat([np.NaN], 13, axis=0) next time
            df_matches.loc[i][&amp;#39;match_id&amp;#39;] = int(match_id)
        if i % 100 == 0: 
            # write tmp data as hdf5 file
            print(i, end=&amp;#39;&amp;#39;)
            print(&amp;quot;.&amp;quot;, end=&amp;#39;&amp;#39;)
            df_matches.to_hdf(target, key=&amp;#39;df_matches&amp;#39;, mode=&amp;#39;w&amp;#39;)

    # write data as hdf5 file
    df_matches.to_hdf(target, key=&amp;#39;df_matches&amp;#39;, mode=&amp;#39;w&amp;#39;)
else:
    # read from hdf5 file
    df_matches = pd.read_hdf(&amp;#39;data/df_matches_20211215_212935.h5&amp;#39;)

df_matches.shape
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;data/df_matches_20211230_192705.h5
131542

(131542, 15)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since we manually filled the &lt;code&gt;pandas&lt;/code&gt; DataFrame, most of the columns are now of &lt;code&gt;object&lt;/code&gt; datatype.
We need to change this to be able to work properly with the data, as well as store it properly.
Here I convert each column manually, however I later found out about &lt;code&gt;DataFrame.infer_objects()&lt;/code&gt;, that can detect the proper dtype automatically.
This I will try next time.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# convert object dtype columns to proper pandas dtypes datetime and numeric
df_matches[&amp;#39;match_date&amp;#39;] = pd.to_datetime(df_matches.match_date) # Datetime object
df_matches[&amp;#39;match_id&amp;#39;] = pd.to_numeric(df_matches.match_id) 
df_matches[&amp;#39;team1_id&amp;#39;] = pd.to_numeric(df_matches.team1_id) 
df_matches[&amp;#39;team1_coach_id&amp;#39;] = pd.to_numeric(df_matches.team1_coach_id) 
df_matches[&amp;#39;team1_roster_id&amp;#39;] = pd.to_numeric(df_matches.team1_roster_id) 
df_matches[&amp;#39;team2_id&amp;#39;] = pd.to_numeric(df_matches.team2_id) 
df_matches[&amp;#39;team2_coach_id&amp;#39;] = pd.to_numeric(df_matches.team2_coach_id) 
df_matches[&amp;#39;team2_roster_id&amp;#39;] = pd.to_numeric(df_matches.team2_roster_id) 
df_matches[&amp;#39;team1_score&amp;#39;] = pd.to_numeric(df_matches.team1_score) 
df_matches[&amp;#39;team2_score&amp;#39;] = pd.to_numeric(df_matches.team2_score) 

# calculate match score difference
df_matches[&amp;#39;team1_win&amp;#39;] = np.sign(df_matches[&amp;#39;team1_score&amp;#39;] - df_matches[&amp;#39;team2_score&amp;#39;])
df_matches[&amp;#39;team2_win&amp;#39;] = np.sign(df_matches[&amp;#39;team2_score&amp;#39;] - df_matches[&amp;#39;team1_score&amp;#39;])

# mirror match
df_matches[&amp;#39;mirror_match&amp;#39;] = 0
df_matches.loc[df_matches[&amp;#39;team1_race_name&amp;#39;] == df_matches[&amp;#39;team2_race_name&amp;#39;], &amp;#39;mirror_match&amp;#39;] = 1
&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# 5K mirror matches
df_matches.query(&amp;#39;mirror_match == 1&amp;#39;).shape&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(5081, 18)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Make BB2020 Khorne team name equal to BB2016 to compare them more easily
df_matches.loc[df_matches[&amp;#39;team1_race_name&amp;#39;] == &amp;quot;Khorne&amp;quot;, &amp;#39;team1_race_name&amp;#39;] = &amp;#39;Daemons of Khorne&amp;#39;
df_matches.loc[df_matches[&amp;#39;team2_race_name&amp;#39;] == &amp;quot;Khorne&amp;quot;, &amp;#39;team2_race_name&amp;#39;] = &amp;#39;Daemons of Khorne&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;dataprep-transforming-the-team-values&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Dataprep: transforming the team values&lt;/h2&gt;
&lt;p&gt;In Blood Bowl, teams can develop themselves over the course of multiple matches. The winnings of each match can be spend on buying new, stronger players, or replace the players that ended up getting injured or even killed. In addition, players receive so-called &lt;em&gt;star player points (SPP)&lt;/em&gt; for important events, such as scoring, or inflicting a casualty on the opponent. Therefore, a balancing mechanism is needed when a newly created “rookie” team is facing a highly developed opposing team with lots of extra skills and strong players.&lt;/p&gt;
&lt;p&gt;Blood Bowl solves this by calculating for both teams their &lt;strong&gt;Current team value&lt;/strong&gt;.
The &lt;strong&gt;Team value difference&lt;/strong&gt; for a match determines the amount of gold that the weaker team can use to buy so-called &lt;strong&gt;inducements&lt;/strong&gt;.
These inducements are temporary, and can consists of a famous “star player” who joins the team just for this match. Another popular option is to hire a wizard that can be used to turn one of the opposing players into a frog.&lt;/p&gt;
&lt;p&gt;It is well known that the win rates of the teams depend on how developed a team is. For example, Amazons are thought to be strongest at low team value, as they already start out with lots of &lt;em&gt;block&lt;/em&gt; and &lt;em&gt;dodge&lt;/em&gt; skills, whereas a Chaos team start out with almost no skills.
So if we compare win rates, we would like take into account the current team value.
Now as this can differ between the two teams in a match up, I reasoned that the highest team value is most informative about the average strength level of both teams, because of the inducement mechanism described above.&lt;/p&gt;
&lt;p&gt;In the dataset, we have for each match the current team values of both teams as a text string.
We transform the text string &lt;code&gt;1100k&lt;/code&gt; into an integer number &lt;code&gt;1100&lt;/code&gt;, so that we can calculated the difference as &lt;code&gt;tv_diff&lt;/code&gt;, and pick for each match the maximum team value and store it as &lt;code&gt;tv_match&lt;/code&gt;. Finally, we create a team value bin &lt;code&gt;tv_bin&lt;/code&gt; to be able to compare win rates for binned groups of matches where races have comparable team strength / team development.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# convert team value 1100k to 1100 integer and and above / below median (= low / high TV)
df_matches[&amp;#39;team1_value&amp;#39;] = df_matches[&amp;#39;team1_value&amp;#39;].str.replace(&amp;#39;k$&amp;#39;, &amp;#39;&amp;#39;)
df_matches[&amp;#39;team1_value&amp;#39;] = df_matches[&amp;#39;team1_value&amp;#39;].fillna(0).astype(np.int64)

df_matches[&amp;#39;team2_value&amp;#39;] = df_matches[&amp;#39;team2_value&amp;#39;].str.replace(&amp;#39;k$&amp;#39;, &amp;#39;&amp;#39;)
df_matches[&amp;#39;team2_value&amp;#39;] = df_matches[&amp;#39;team2_value&amp;#39;].fillna(0).astype(np.int64)

df_matches[&amp;#39;tv_diff&amp;#39;] = np.abs(df_matches[&amp;#39;team2_value&amp;#39;] - df_matches[&amp;#39;team1_value&amp;#39;])

df_matches[&amp;#39;tv_match&amp;#39;] = df_matches[[&amp;quot;team1_value&amp;quot;, &amp;quot;team2_value&amp;quot;]].max(axis=1)

df_matches[&amp;#39;tv_bin&amp;#39;] = pd.cut(df_matches[&amp;#39;tv_match&amp;#39;], 
    bins = [0, 950, 1250,1550, 1850, float(&amp;quot;inf&amp;quot;)], 
    labels=[&amp;#39;&amp;lt; 950&amp;#39;, &amp;#39;1.1K&amp;#39;, &amp;#39;1.4K&amp;#39;, &amp;#39;1.7K&amp;#39;, &amp;#39;&amp;gt; 1850&amp;#39;]
)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;dropping-empty-matches&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Dropping empty matches&lt;/h2&gt;
&lt;p&gt;Some match_id’s do not have match information attached to them, presumably these matches were not played or some real life event interfered. These match_ids are dropped from the dataset to get rid of the NAs in all the columns.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df_matches = df_matches.dropna(subset=[&amp;#39;match_date&amp;#39;])&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# 131K matches
len(df_matches)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;131509&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;dataprep-getting-the-dates-right&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Dataprep: getting the dates right&lt;/h2&gt;
&lt;p&gt;To see time trends, its useful to aggregate the data by week. For this we add &lt;code&gt;week_number&lt;/code&gt; for each date, and from this week number, we convert back to a date to get a &lt;code&gt;week_date&lt;/code&gt;. This last part is useful for plotting with &lt;code&gt;plotnine&lt;/code&gt;, as this treats dates in a special way. We use the ISO definition of week, this has some unexpected behavior near the beginning / end of each year, that we fix manually.&lt;/p&gt;
&lt;p&gt;The data starts in week 36 (september) of 2020, and stops halfway week 44 in 2021.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df_matches[&amp;#39;week_number&amp;#39;] = df_matches[&amp;#39;match_date&amp;#39;].dt.isocalendar().week

# cannot serialize numpy int OR Int64 when writing HDF5 file, so we go for plain int as all NAs are gone now
df_matches[&amp;#39;week_number&amp;#39;] = df_matches[&amp;#39;week_number&amp;#39;].fillna(0).astype(int)

# add year based on match date (but want it based on match ISO week)
df_matches[&amp;#39;year&amp;#39;] = pd.DatetimeIndex(df_matches[&amp;#39;match_date&amp;#39;]).year

# manual fix year for ISO week 2020-53 (2020 has 53 ISO weeks, including a few days in jan 2021)
df_matches.loc[(df_matches[&amp;#39;year&amp;#39;] == 2021) &amp;amp; (df_matches[&amp;#39;week_number&amp;#39;] == 53), &amp;#39;year&amp;#39;] = 2020

df_matches[&amp;#39;week_year&amp;#39;] = df_matches[&amp;#39;year&amp;#39;].astype(str) + &amp;#39;-&amp;#39; + df_matches[&amp;#39;week_number&amp;#39;].astype(str)

df_matches[&amp;#39;week_date&amp;#39;] = pd.to_datetime(df_matches[&amp;#39;week_year&amp;#39;].astype(&amp;quot;string&amp;quot;) + &amp;#39;-1&amp;#39;, format = &amp;quot;%Y-%U-%w&amp;quot;)

# manual fix of week date (grrrr)
df_matches.loc[(df_matches[&amp;#39;week_date&amp;#39;] == &amp;#39;2021-01-04&amp;#39;) &amp;amp; (df_matches[&amp;#39;week_number&amp;#39;] == 53), &amp;#39;week_date&amp;#39;] = pd.to_datetime(&amp;#39;2020-12-31&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;step-2-html-scraping-the-inducements-and-coach-rankings-for-each-match&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Step 2: HTML Scraping the inducements and coach rankings for each match&lt;/h1&gt;
&lt;p&gt;Next, we collect for all the matches in &lt;code&gt;df_matches&lt;/code&gt; the &lt;strong&gt;inducements&lt;/strong&gt; and &lt;strong&gt;coach rankings&lt;/strong&gt;.
This information is not available through the API, but is presented on a HTML page at &lt;a href=&#34;https://fumbbl.com/FUMBBL.php?page=match&amp;amp;id=4350014&#34; class=&#34;uri&#34;&gt;https://fumbbl.com/FUMBBL.php?page=match&amp;amp;id=4350014&lt;/a&gt; summarizing information for in this case match 4350014.&lt;/p&gt;
&lt;p&gt;I highly recommend &lt;a href=&#34;https://hackersandslackers.com/scraping-urls-with-beautifulsoup/&#34;&gt;this tutorial&lt;/a&gt; for a great introduction to &lt;code&gt;BeautifulSoup&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In addition, to clean up the scraped text, I used the &lt;code&gt;re&lt;/code&gt; Python module (Regular expressions), part of the &lt;a href=&#34;https://docs.python.org/3/library/index.html&#34;&gt;Python standard library&lt;/a&gt; to extract the actual inducements from the text string that contains them.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from bs4 import BeautifulSoup
import re

df_inducements = pd.DataFrame(columns=[&amp;#39;match_id&amp;#39;, &amp;#39;team1_inducements&amp;#39;, &amp;#39;team2_inducements&amp;#39;, &amp;#39;coach1_ranking&amp;#39;, &amp;#39;coach2_ranking&amp;#39;])

target = &amp;#39;data/df_inducements_&amp;#39; + time.strftime(&amp;quot;%Y%m%d_%H%M%S&amp;quot;) + &amp;#39;.h5&amp;#39;
print(target)

end_match = 4347800  
begin_match = 4216257

n_matches = end_match - begin_match

full_run = 0

print(n_matches)

if(full_run):
    for i in range(n_matches):
        match_id = end_match - i
        api_string = &amp;quot;https://fumbbl.com/FUMBBL.php?page=match&amp;amp;id=&amp;quot; + str(match_id)
        # wait 0.33 s on average between each GET call
        wait_time = (random.uniform(0.5, 1) + 0.25)/3
        time.sleep(wait_time)
        response = requests.get(api_string)

        soup = BeautifulSoup(response.content, &amp;#39;html.parser&amp;#39;)

        if soup.find(&amp;quot;div&amp;quot;, {&amp;quot;class&amp;quot;: &amp;quot;matchrecord&amp;quot;}) is not None:
            # match record is available
            inducements = soup.find_all(&amp;quot;div&amp;quot;, class_=&amp;quot;inducements&amp;quot;)

            pattern = re.compile(r&amp;#39;\s+Inducements: (.*)\n&amp;#39;)

            match = re.match(pattern, inducements[0].get_text())
            if match:
                team1_inducements = match.group(1)
            else:
                team1_inducements = &amp;#39;&amp;#39;

            match = re.match(pattern, inducements[1].get_text())
            if match:
                team2_inducements = match.group(1)
            else:
                team2_inducements = &amp;#39;&amp;#39;

            coach_rankings = soup.find_all(&amp;quot;div&amp;quot;, class_=&amp;quot;coach&amp;quot;)

            coach1_ranking = coach_rankings[0].get_text()
            coach2_ranking = coach_rankings[1].get_text()

            df_inducements.loc[i] = [match_id, team1_inducements, team2_inducements, coach1_ranking, coach2_ranking]

        if i % 100 == 0: 
                    # write tmp data as hdf5 file
                    print(i, end=&amp;#39;&amp;#39;)
                    print(&amp;quot;.&amp;quot;, end=&amp;#39;&amp;#39;)
                    df_inducements.to_hdf(target, key=&amp;#39;df_inducements&amp;#39;, mode=&amp;#39;w&amp;#39;)

    # write data as hdf5 file
    df_inducements.to_hdf(target, key=&amp;#39;df_inducements&amp;#39;, mode=&amp;#39;w&amp;#39;)
else:
    # read from hdf5 file
    
    df_inducements = pd.read_hdf(&amp;#39;data/df_inducements_20211215_112148.h5&amp;#39;) 


df_inducements.info()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;data/df_inducements_20211230_192709.h5
131543
&amp;lt;class &amp;#39;pandas.core.frame.DataFrame&amp;#39;&amp;gt;
Int64Index: 131509 entries, 0 to 131508
Data columns (total 5 columns):
 #   Column             Non-Null Count   Dtype 
---  ------             --------------   ----- 
 0   match_id           131509 non-null  object
 1   team1_inducements  131509 non-null  object
 2   team2_inducements  131509 non-null  object
 3   coach1_ranking     131509 non-null  object
 4   coach2_ranking     131509 non-null  object
dtypes: object(5)
memory usage: 6.0+ MB&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df_inducements.query(&amp;quot;match_id == 4347799&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
&lt;thead&gt;
&lt;tr style=&#34;text-align: right;&#34;&gt;
&lt;th&gt;
&lt;/th&gt;
&lt;th&gt;
match_id
&lt;/th&gt;
&lt;th&gt;
team1_inducements
&lt;/th&gt;
&lt;th&gt;
team2_inducements
&lt;/th&gt;
&lt;th&gt;
coach1_ranking
&lt;/th&gt;
&lt;th&gt;
coach2_ranking
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;
1
&lt;/th&gt;
&lt;td&gt;
4347799
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
1 bloodweiser babe, Mercenary Merc Thrall Line…
&lt;/td&gt;
&lt;td&gt;
CR 154.6 (+0.97) Emerging Star Vini
&lt;/td&gt;
&lt;td&gt;
motay666 Emerging Star CR 151.12 (-0.97)
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;dataprep-coach-rankings&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Dataprep: coach rankings&lt;/h2&gt;
&lt;p&gt;We want to extract the part &lt;code&gt;CR 152.53&lt;/code&gt; from the scraped coach information field. Just as we matched on &lt;code&gt;Inducements:&lt;/code&gt;, we can match on &lt;code&gt;CR&lt;/code&gt; and grab the contents directly after that, stopping when we encounter a whitespace.&lt;/p&gt;
&lt;p&gt;We first play around a bit and test until we discover the proper Regular Expression to use :-)&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;#pattern = re.compile(r&amp;#39;\s+Inducements: (.*)\n&amp;#39;)
pattern = re.compile(r&amp;#39;.*CR (.*)\s\(.*&amp;#39;)

match = re.match(pattern, df_inducements.loc[0, &amp;#39;coach2_ranking&amp;#39;])

if match is not None:
    print(match.group(1)) # group(0) is the whole string
else:
    print(&amp;quot;match is none&amp;quot;)

df_inducements.loc[0, &amp;#39;coach2_ranking&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;144.51

&amp;#39;LoxodonP Veteran CR 144.51 (-0.76)&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Got ’m! Now that we have figured it out, we can write the code that extracts the coach rankings:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Dataprep fix match_id
df_inducements[&amp;#39;match_id&amp;#39;] = pd.to_numeric(df_inducements.match_id) 

# Dataprep: add the coach rankings as separate cols
df_inducements[&amp;#39;coach1_CR&amp;#39;] = df_inducements[&amp;#39;coach1_ranking&amp;#39;].str.extract(r&amp;#39;.*CR (.*)\s\(.*&amp;#39;)
df_inducements[&amp;#39;coach2_CR&amp;#39;] = df_inducements[&amp;#39;coach2_ranking&amp;#39;].str.extract(r&amp;#39;.*CR (.*)\s\(.*&amp;#39;)

df_inducements[&amp;#39;coach1_CR&amp;#39;] = pd.to_numeric(df_inducements[&amp;#39;coach1_CR&amp;#39;])
df_inducements[&amp;#39;coach2_CR&amp;#39;] = pd.to_numeric(df_inducements[&amp;#39;coach2_CR&amp;#39;])

df_inducements[&amp;#39;CR_diff&amp;#39;] = np.abs(df_inducements[&amp;#39;coach1_CR&amp;#39;] - df_inducements[&amp;#39;coach2_CR&amp;#39;])
df_inducements[&amp;#39;CR_diff&amp;#39;] = df_inducements[&amp;#39;CR_diff&amp;#39;].astype(float)

df_inducements[&amp;#39;cr_diff2&amp;#39;] = df_inducements[&amp;#39;coach1_CR&amp;#39;] - df_inducements[&amp;#39;coach2_CR&amp;#39;]

df_inducements[&amp;#39;cr_bin&amp;#39;] = pd.cut(df_inducements[&amp;#39;cr_diff2&amp;#39;], bins = [-1*float(&amp;quot;inf&amp;quot;), -30, -20, -10, -5, 5, 10, 20, 30, float(&amp;quot;inf&amp;quot;)], 
 labels=[&amp;#39;{-Inf,-30]&amp;#39;, &amp;#39;[-30,-20]&amp;#39;, &amp;#39;[-20,-10]&amp;#39;, &amp;#39;[-10,-5]&amp;#39;, &amp;#39;[-5,5]&amp;#39;, &amp;#39;[5,10]&amp;#39;, &amp;#39;[10,20]&amp;#39;, &amp;#39;[20,30]&amp;#39;, &amp;#39;[30,Inf]&amp;#39;]) &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df_inducements.dtypes&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;match_id                int64
team1_inducements      object
team2_inducements      object
coach1_ranking         object
coach2_ranking         object
coach1_CR             float64
coach2_CR             float64
CR_diff               float64
cr_diff2              float64
cr_bin               category
dtype: object&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;dataprep-match-inducements-for-each-team&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Dataprep match inducements for each team&lt;/h2&gt;
&lt;p&gt;The next trick is to use &lt;code&gt;pandas&lt;/code&gt; &lt;code&gt;explode()&lt;/code&gt; method (similar to &lt;code&gt;separate_rows()&lt;/code&gt; in &lt;code&gt;tidyverse&lt;/code&gt; R) to give each inducement its own row in the dataset.
This creates a dataframe (&lt;code&gt;inducements&lt;/code&gt;) similar to &lt;code&gt;df_mbt&lt;/code&gt; with each match generating at least two rows.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;team1_inducements = df_inducements[[&amp;#39;match_id&amp;#39;, &amp;#39;team1_inducements&amp;#39;]]
team2_inducements = df_inducements[[&amp;#39;match_id&amp;#39;, &amp;#39;team2_inducements&amp;#39;]]

# make column names equal
team1_inducements.columns = team2_inducements.columns = [&amp;#39;match_id&amp;#39;, &amp;#39;inducements&amp;#39;]

# row bind the two dataframes
inducements = pd.concat([team1_inducements, team2_inducements], ignore_index = True)

# convert comma separated string to list
inducements[&amp;#39;inducements&amp;#39;] = inducements[&amp;#39;inducements&amp;#39;].str.split(&amp;#39;,&amp;#39;)

# make each element of the list a separate row
inducements = inducements.explode(&amp;#39;inducements&amp;#39;)

# strip leading and trailing whitespaces
inducements[&amp;#39;inducements&amp;#39;] = inducements[&amp;#39;inducements&amp;#39;].str.strip()

# create &amp;quot;star player&amp;quot; label
inducements[&amp;#39;star_player&amp;#39;] = 0
inducements.loc[inducements[&amp;#39;inducements&amp;#39;].str.contains(&amp;quot;Star player&amp;quot;), &amp;#39;star_player&amp;#39;] = 1

# create &amp;quot;card&amp;quot; label
inducements[&amp;#39;special_card&amp;#39;] = 0
inducements.loc[inducements[&amp;#39;inducements&amp;#39;].str.contains(&amp;quot;Card&amp;quot;), &amp;#39;special_card&amp;#39;] = 1
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;add-inducement-info-to-df_matches&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Add inducement info to df_matches&lt;/h2&gt;
&lt;p&gt;Here we add &lt;code&gt;df_inducements&lt;/code&gt; to &lt;code&gt;df_matches&lt;/code&gt;. This contains each players inducements as a single string, not convenient for analysis.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;inducements&lt;/code&gt; dataframe cannot easily be added to &lt;code&gt;df_matches&lt;/code&gt;. We can however, extract information from &lt;code&gt;inducements&lt;/code&gt; at the match level and add this to &lt;code&gt;df_matches&lt;/code&gt;. Here, I show how to add a 1/0 flag &lt;code&gt;has_sp&lt;/code&gt; that codes for if a match included any star player.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df_matches = pd.merge(df_matches, df_inducements, on=&amp;#39;match_id&amp;#39;, how=&amp;#39;left&amp;#39;)

df_sp = (inducements
            .groupby(&amp;quot;match_id&amp;quot;)
            .agg(has_sp = (&amp;quot;star_player&amp;quot;, &amp;quot;max&amp;quot;))
            .reset_index()
)


df_matches = pd.merge(df_matches, df_sp, on = &amp;quot;match_id&amp;quot;, how = &amp;quot;left&amp;quot;)

df_matches[&amp;#39;match_id&amp;#39;] = pd.to_numeric(df_matches.match_id) &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;step-3-create-matches-by-team-dataframe&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Step 3: Create matches by team DataFrame&lt;/h1&gt;
&lt;p&gt;When analyzing the data, we also like to have a dataframe &lt;code&gt;df_mbt (df_matches_by_team)&lt;/code&gt; that contains, for each match, a separate row for each team participating in that match.
This structure is nicely visualized &lt;a href=&#34;https://www.nufflytics.com/post/the-value-of-tv/&#34;&gt;at the Nufflytics blog&lt;/a&gt;.
Such a dataset is suitable for analysis by team, e.g. win rates. We can extend this further by adding, at the match level, data that is specific for each team - coach pair, such as team value, coach rating etc.
For example, we can imagine adding more team level data, such as casualties caused during the match, or team composition at the start of the match etc.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# make two copies, one for each team in the match
team1_data = df_matches[[&amp;#39;match_id&amp;#39;, &amp;#39;match_date&amp;#39;, &amp;#39;week_number&amp;#39;,   &amp;#39;year&amp;#39;, &amp;#39;week_year&amp;#39;, &amp;#39;week_date&amp;#39;, &amp;#39;team1_id&amp;#39;,
    &amp;#39;team1_coach_id&amp;#39;, &amp;#39;team1_race_name&amp;#39;, &amp;#39;team1_value&amp;#39;, &amp;#39;team1_win&amp;#39;, &amp;#39;tv_diff&amp;#39;, &amp;#39;tv_match&amp;#39;,
    &amp;#39;tv_bin&amp;#39;, &amp;#39;mirror_match&amp;#39;, &amp;#39;coach1_CR&amp;#39;, &amp;#39;CR_diff&amp;#39;, &amp;#39;has_sp&amp;#39;]].copy()

team2_data = df_matches[[&amp;#39;match_id&amp;#39;, &amp;#39;match_date&amp;#39;, &amp;#39;week_number&amp;#39;,   &amp;#39;year&amp;#39;, &amp;#39;week_year&amp;#39;, &amp;#39;week_date&amp;#39;, &amp;#39;team2_id&amp;#39;, 
    &amp;#39;team2_coach_id&amp;#39;, &amp;#39;team2_race_name&amp;#39;, &amp;#39;team2_value&amp;#39;, &amp;#39;team2_win&amp;#39;, &amp;#39;tv_diff&amp;#39;, &amp;#39;tv_match&amp;#39;, 
    &amp;#39;tv_bin&amp;#39;,&amp;#39;mirror_match&amp;#39;, &amp;#39;coach2_CR&amp;#39;, &amp;#39;CR_diff&amp;#39;, &amp;#39;has_sp&amp;#39;]].copy()

team1_data.columns = team2_data.columns = [&amp;#39;match_id&amp;#39;, &amp;#39;match_date&amp;#39;, &amp;#39;week_number&amp;#39;, &amp;#39;year&amp;#39;, &amp;#39;week_year&amp;#39;, &amp;#39;week_date&amp;#39;, &amp;#39;team_id&amp;#39;, 
    &amp;#39;coach_id&amp;#39;, &amp;#39;race_name&amp;#39;, &amp;#39;team_value&amp;#39;, &amp;#39;wins&amp;#39;, &amp;#39;tv_diff&amp;#39;, &amp;#39;tv_match&amp;#39;, 
    &amp;#39;tv_bin&amp;#39;, &amp;#39;mirror_match&amp;#39;, &amp;#39;coach_CR&amp;#39;, &amp;#39;CR_diff&amp;#39;, &amp;#39;has_sp&amp;#39;]

# combine both dataframes
df_mbt = pd.concat([team1_data, team2_data])&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;adding-outcome-weights&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Adding outcome weights&lt;/h2&gt;
&lt;p&gt;One way to measure team strength is to calculate a win rate.
If we want to calculate win rates, we need to decide how to weigh a draw.
In Blood Bowl data analysis, it seems that a 2:1:0 (W / D / L) weighting scheme is most commonly used.
So if we want to compare with others, it makes sense to adapt this scheme as well.
If we divide these weights by two we get something that, if we average it, we can interpret as a win rate.&lt;/p&gt;
&lt;p&gt;This scheme has the advantage that the weighted average win percentage over all matches is always 50%, creating a nice reference point allowing conclusions such as “this and that team has an x percent above average win percentage”.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df_mbt.loc[df_mbt[&amp;#39;wins&amp;#39;] == 0, &amp;#39;wins&amp;#39;] = 0.5
df_mbt.loc[df_mbt[&amp;#39;wins&amp;#39;] == -1, &amp;#39;wins&amp;#39;] = 0

# convert to float
df_mbt[&amp;#39;wins&amp;#39;] = df_mbt[&amp;#39;wins&amp;#39;].astype(float)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this point, Lets have a look at our dataset again:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df_mbt.query(&amp;quot;coach_id == 255851&amp;quot;).sort_values(&amp;#39;match_date&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
&lt;thead&gt;
&lt;tr style=&#34;text-align: right;&#34;&gt;
&lt;th&gt;
&lt;/th&gt;
&lt;th&gt;
match_id
&lt;/th&gt;
&lt;th&gt;
match_date
&lt;/th&gt;
&lt;th&gt;
week_number
&lt;/th&gt;
&lt;th&gt;
year
&lt;/th&gt;
&lt;th&gt;
week_year
&lt;/th&gt;
&lt;th&gt;
week_date
&lt;/th&gt;
&lt;th&gt;
team_id
&lt;/th&gt;
&lt;th&gt;
coach_id
&lt;/th&gt;
&lt;th&gt;
race_name
&lt;/th&gt;
&lt;th&gt;
team_value
&lt;/th&gt;
&lt;th&gt;
wins
&lt;/th&gt;
&lt;th&gt;
tv_diff
&lt;/th&gt;
&lt;th&gt;
tv_match
&lt;/th&gt;
&lt;th&gt;
tv_bin
&lt;/th&gt;
&lt;th&gt;
mirror_match
&lt;/th&gt;
&lt;th&gt;
coach_CR
&lt;/th&gt;
&lt;th&gt;
CR_diff
&lt;/th&gt;
&lt;th&gt;
has_sp
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;
99692
&lt;/th&gt;
&lt;td&gt;
4248074
&lt;/td&gt;
&lt;td&gt;
2020-11-13
&lt;/td&gt;
&lt;td&gt;
46
&lt;/td&gt;
&lt;td&gt;
2020
&lt;/td&gt;
&lt;td&gt;
2020-46
&lt;/td&gt;
&lt;td&gt;
2020-11-16
&lt;/td&gt;
&lt;td&gt;
1003452.0
&lt;/td&gt;
&lt;td&gt;
255851.0
&lt;/td&gt;
&lt;td&gt;
Human
&lt;/td&gt;
&lt;td&gt;
1000
&lt;/td&gt;
&lt;td&gt;
0.0
&lt;/td&gt;
&lt;td&gt;
0
&lt;/td&gt;
&lt;td&gt;
1000
&lt;/td&gt;
&lt;td&gt;
1.1K
&lt;/td&gt;
&lt;td&gt;
0
&lt;/td&gt;
&lt;td&gt;
150.00
&lt;/td&gt;
&lt;td&gt;
0.80
&lt;/td&gt;
&lt;td&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
77673
&lt;/th&gt;
&lt;td&gt;
4270093
&lt;/td&gt;
&lt;td&gt;
2021-01-17
&lt;/td&gt;
&lt;td&gt;
2
&lt;/td&gt;
&lt;td&gt;
2021
&lt;/td&gt;
&lt;td&gt;
2021-2
&lt;/td&gt;
&lt;td&gt;
2021-01-11
&lt;/td&gt;
&lt;td&gt;
1003452.0
&lt;/td&gt;
&lt;td&gt;
255851.0
&lt;/td&gt;
&lt;td&gt;
Human
&lt;/td&gt;
&lt;td&gt;
1000
&lt;/td&gt;
&lt;td&gt;
0.0
&lt;/td&gt;
&lt;td&gt;
40
&lt;/td&gt;
&lt;td&gt;
1000
&lt;/td&gt;
&lt;td&gt;
1.1K
&lt;/td&gt;
&lt;td&gt;
1
&lt;/td&gt;
&lt;td&gt;
149.22
&lt;/td&gt;
&lt;td&gt;
3.63
&lt;/td&gt;
&lt;td&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
32991
&lt;/th&gt;
&lt;td&gt;
4314794
&lt;/td&gt;
&lt;td&gt;
2021-07-01
&lt;/td&gt;
&lt;td&gt;
26
&lt;/td&gt;
&lt;td&gt;
2021
&lt;/td&gt;
&lt;td&gt;
2021-26
&lt;/td&gt;
&lt;td&gt;
2021-06-28
&lt;/td&gt;
&lt;td&gt;
1003452.0
&lt;/td&gt;
&lt;td&gt;
255851.0
&lt;/td&gt;
&lt;td&gt;
Human
&lt;/td&gt;
&lt;td&gt;
980
&lt;/td&gt;
&lt;td&gt;
0.0
&lt;/td&gt;
&lt;td&gt;
10
&lt;/td&gt;
&lt;td&gt;
990
&lt;/td&gt;
&lt;td&gt;
1.1K
&lt;/td&gt;
&lt;td&gt;
0
&lt;/td&gt;
&lt;td&gt;
147.60
&lt;/td&gt;
&lt;td&gt;
1.37
&lt;/td&gt;
&lt;td&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
31599
&lt;/th&gt;
&lt;td&gt;
4316193
&lt;/td&gt;
&lt;td&gt;
2021-07-10
&lt;/td&gt;
&lt;td&gt;
27
&lt;/td&gt;
&lt;td&gt;
2021
&lt;/td&gt;
&lt;td&gt;
2021-27
&lt;/td&gt;
&lt;td&gt;
2021-07-05
&lt;/td&gt;
&lt;td&gt;
1035835.0
&lt;/td&gt;
&lt;td&gt;
255851.0
&lt;/td&gt;
&lt;td&gt;
Human
&lt;/td&gt;
&lt;td&gt;
980
&lt;/td&gt;
&lt;td&gt;
0.0
&lt;/td&gt;
&lt;td&gt;
20
&lt;/td&gt;
&lt;td&gt;
1000
&lt;/td&gt;
&lt;td&gt;
1.1K
&lt;/td&gt;
&lt;td&gt;
0
&lt;/td&gt;
&lt;td&gt;
146.62
&lt;/td&gt;
&lt;td&gt;
9.49
&lt;/td&gt;
&lt;td&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
25968
&lt;/th&gt;
&lt;td&gt;
4321824
&lt;/td&gt;
&lt;td&gt;
2021-08-16
&lt;/td&gt;
&lt;td&gt;
33
&lt;/td&gt;
&lt;td&gt;
2021
&lt;/td&gt;
&lt;td&gt;
2021-33
&lt;/td&gt;
&lt;td&gt;
2021-08-16
&lt;/td&gt;
&lt;td&gt;
1038960.0
&lt;/td&gt;
&lt;td&gt;
255851.0
&lt;/td&gt;
&lt;td&gt;
Human
&lt;/td&gt;
&lt;td&gt;
970
&lt;/td&gt;
&lt;td&gt;
0.5
&lt;/td&gt;
&lt;td&gt;
30
&lt;/td&gt;
&lt;td&gt;
1000
&lt;/td&gt;
&lt;td&gt;
1.1K
&lt;/td&gt;
&lt;td&gt;
1
&lt;/td&gt;
&lt;td&gt;
145.93
&lt;/td&gt;
&lt;td&gt;
9.30
&lt;/td&gt;
&lt;td&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
23609
&lt;/th&gt;
&lt;td&gt;
4324190
&lt;/td&gt;
&lt;td&gt;
2021-09-01
&lt;/td&gt;
&lt;td&gt;
35
&lt;/td&gt;
&lt;td&gt;
2021
&lt;/td&gt;
&lt;td&gt;
2021-35
&lt;/td&gt;
&lt;td&gt;
2021-08-30
&lt;/td&gt;
&lt;td&gt;
1038960.0
&lt;/td&gt;
&lt;td&gt;
255851.0
&lt;/td&gt;
&lt;td&gt;
Human
&lt;/td&gt;
&lt;td&gt;
1000
&lt;/td&gt;
&lt;td&gt;
0.0
&lt;/td&gt;
&lt;td&gt;
10
&lt;/td&gt;
&lt;td&gt;
1000
&lt;/td&gt;
&lt;td&gt;
1.1K
&lt;/td&gt;
&lt;td&gt;
0
&lt;/td&gt;
&lt;td&gt;
145.90
&lt;/td&gt;
&lt;td&gt;
10.19
&lt;/td&gt;
&lt;td&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
21018
&lt;/th&gt;
&lt;td&gt;
4326781
&lt;/td&gt;
&lt;td&gt;
2021-09-13
&lt;/td&gt;
&lt;td&gt;
37
&lt;/td&gt;
&lt;td&gt;
2021
&lt;/td&gt;
&lt;td&gt;
2021-37
&lt;/td&gt;
&lt;td&gt;
2021-09-13
&lt;/td&gt;
&lt;td&gt;
1038960.0
&lt;/td&gt;
&lt;td&gt;
255851.0
&lt;/td&gt;
&lt;td&gt;
Human
&lt;/td&gt;
&lt;td&gt;
1010
&lt;/td&gt;
&lt;td&gt;
1.0
&lt;/td&gt;
&lt;td&gt;
80
&lt;/td&gt;
&lt;td&gt;
1010
&lt;/td&gt;
&lt;td&gt;
1.1K
&lt;/td&gt;
&lt;td&gt;
0
&lt;/td&gt;
&lt;td&gt;
144.24
&lt;/td&gt;
&lt;td&gt;
6.02
&lt;/td&gt;
&lt;td&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
12941
&lt;/th&gt;
&lt;td&gt;
4334858
&lt;/td&gt;
&lt;td&gt;
2021-10-11
&lt;/td&gt;
&lt;td&gt;
41
&lt;/td&gt;
&lt;td&gt;
2021
&lt;/td&gt;
&lt;td&gt;
2021-41
&lt;/td&gt;
&lt;td&gt;
2021-10-11
&lt;/td&gt;
&lt;td&gt;
1038960.0
&lt;/td&gt;
&lt;td&gt;
255851.0
&lt;/td&gt;
&lt;td&gt;
Human
&lt;/td&gt;
&lt;td&gt;
1110
&lt;/td&gt;
&lt;td&gt;
0.0
&lt;/td&gt;
&lt;td&gt;
150
&lt;/td&gt;
&lt;td&gt;
1260
&lt;/td&gt;
&lt;td&gt;
1.4K
&lt;/td&gt;
&lt;td&gt;
0
&lt;/td&gt;
&lt;td&gt;
145.22
&lt;/td&gt;
&lt;td&gt;
16.80
&lt;/td&gt;
&lt;td&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
10050
&lt;/th&gt;
&lt;td&gt;
4337749
&lt;/td&gt;
&lt;td&gt;
2021-10-22
&lt;/td&gt;
&lt;td&gt;
42
&lt;/td&gt;
&lt;td&gt;
2021
&lt;/td&gt;
&lt;td&gt;
2021-42
&lt;/td&gt;
&lt;td&gt;
2021-10-18
&lt;/td&gt;
&lt;td&gt;
1050267.0
&lt;/td&gt;
&lt;td&gt;
255851.0
&lt;/td&gt;
&lt;td&gt;
Orc
&lt;/td&gt;
&lt;td&gt;
1080
&lt;/td&gt;
&lt;td&gt;
0.0
&lt;/td&gt;
&lt;td&gt;
10
&lt;/td&gt;
&lt;td&gt;
1090
&lt;/td&gt;
&lt;td&gt;
1.1K
&lt;/td&gt;
&lt;td&gt;
0
&lt;/td&gt;
&lt;td&gt;
NaN
&lt;/td&gt;
&lt;td&gt;
NaN
&lt;/td&gt;
&lt;td&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
9027
&lt;/th&gt;
&lt;td&gt;
4338772
&lt;/td&gt;
&lt;td&gt;
2021-10-26
&lt;/td&gt;
&lt;td&gt;
43
&lt;/td&gt;
&lt;td&gt;
2021
&lt;/td&gt;
&lt;td&gt;
2021-43
&lt;/td&gt;
&lt;td&gt;
2021-10-25
&lt;/td&gt;
&lt;td&gt;
1050267.0
&lt;/td&gt;
&lt;td&gt;
255851.0
&lt;/td&gt;
&lt;td&gt;
Orc
&lt;/td&gt;
&lt;td&gt;
1040
&lt;/td&gt;
&lt;td&gt;
1.0
&lt;/td&gt;
&lt;td&gt;
50
&lt;/td&gt;
&lt;td&gt;
1040
&lt;/td&gt;
&lt;td&gt;
1.1K
&lt;/td&gt;
&lt;td&gt;
0
&lt;/td&gt;
&lt;td&gt;
NaN
&lt;/td&gt;
&lt;td&gt;
NaN
&lt;/td&gt;
&lt;td&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
3783
&lt;/th&gt;
&lt;td&gt;
4344017
&lt;/td&gt;
&lt;td&gt;
2021-11-15
&lt;/td&gt;
&lt;td&gt;
46
&lt;/td&gt;
&lt;td&gt;
2021
&lt;/td&gt;
&lt;td&gt;
2021-46
&lt;/td&gt;
&lt;td&gt;
2021-11-15
&lt;/td&gt;
&lt;td&gt;
1050267.0
&lt;/td&gt;
&lt;td&gt;
255851.0
&lt;/td&gt;
&lt;td&gt;
Orc
&lt;/td&gt;
&lt;td&gt;
1150
&lt;/td&gt;
&lt;td&gt;
0.0
&lt;/td&gt;
&lt;td&gt;
170
&lt;/td&gt;
&lt;td&gt;
1150
&lt;/td&gt;
&lt;td&gt;
1.1K
&lt;/td&gt;
&lt;td&gt;
0
&lt;/td&gt;
&lt;td&gt;
NaN
&lt;/td&gt;
&lt;td&gt;
NaN
&lt;/td&gt;
&lt;td&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
1186
&lt;/th&gt;
&lt;td&gt;
4346614
&lt;/td&gt;
&lt;td&gt;
2021-11-25
&lt;/td&gt;
&lt;td&gt;
47
&lt;/td&gt;
&lt;td&gt;
2021
&lt;/td&gt;
&lt;td&gt;
2021-47
&lt;/td&gt;
&lt;td&gt;
2021-11-22
&lt;/td&gt;
&lt;td&gt;
1050267.0
&lt;/td&gt;
&lt;td&gt;
255851.0
&lt;/td&gt;
&lt;td&gt;
Orc
&lt;/td&gt;
&lt;td&gt;
1020
&lt;/td&gt;
&lt;td&gt;
0.5
&lt;/td&gt;
&lt;td&gt;
150
&lt;/td&gt;
&lt;td&gt;
1170
&lt;/td&gt;
&lt;td&gt;
1.1K
&lt;/td&gt;
&lt;td&gt;
0
&lt;/td&gt;
&lt;td&gt;
NaN
&lt;/td&gt;
&lt;td&gt;
NaN
&lt;/td&gt;
&lt;td&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Great! Almost there. There is still something missing though, we need to know, for all the teams in our matches dataset, in what division or league they are playing, and what version of the rules they use. For these we turn to the API again, to fetch more data, now on the team level.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;step-4-fetch-data-on-team-division-and-ruleset&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Step 4: Fetch data on team division and ruleset&lt;/h1&gt;
&lt;p&gt;Let grab for all teams in &lt;code&gt;df_mbt&lt;/code&gt; the team &lt;strong&gt;division&lt;/strong&gt; and &lt;strong&gt;ruleset&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;A limitation of the FUMBBL API is that it shows only the latest version of the teams and leagues data. This hides the fact that leagues have changed their rules since they were first created. For example, the NAF used BB2016 rules up until summer of 2021, and thereafter switched to the new BB2020 ruleset for their latest online tournament.
So we have to use our “domain knowledge” here to interpret the data properly.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# make list of all teams that need to be fetched
team_ids = list(df_mbt[&amp;#39;team_id&amp;#39;].dropna())

# get unique values by converting to a Python set and back to list
team_ids = list(set(team_ids))

len(team_ids)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;46831&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we have to fetch data for 47K different teams.&lt;/p&gt;
&lt;p&gt;We use the same approach as above, looping over all &lt;code&gt;team_id&lt;/code&gt; ’s and making a separate API call for each team.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;IMPORTANT: here too, we limit ourselves to a maximum of 3 API calls per second to avoid overloading the FUMBBL server&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df_teams = pd.DataFrame(columns=[&amp;#39;team_id&amp;#39;, &amp;#39;division_id&amp;#39;, &amp;#39;division_name&amp;#39;,  &amp;#39;league&amp;#39; ,
    &amp;#39;ruleset&amp;#39;, &amp;#39;roster_id&amp;#39;, &amp;#39;race_name&amp;#39;,  &amp;#39;games_played&amp;#39;])

target = &amp;#39;data/df_teams_&amp;#39; + time.strftime(&amp;quot;%Y%m%d_%H%M%S&amp;quot;) + &amp;#39;.h5&amp;#39;
print(target)

fullrun = 0

if fullrun:
    print(&amp;#39;fetching team data for &amp;#39;, len(team_ids), &amp;#39; teams&amp;#39;)
    for t in range(len(team_ids)):    
        api_string = &amp;quot;https://fumbbl.com/api/team/get/&amp;quot; + str(int(team_ids[t]))
        wait_time = (random.uniform(0.5, 1) + 0.25)/3
        time.sleep(wait_time)
        team = requests.get(api_string)
        team = team.json()
        # grab fields
        team_id = team[&amp;#39;id&amp;#39;]
        division_id = team[&amp;#39;divisionId&amp;#39;]
        division_name = team[&amp;#39;division&amp;#39;]
        ruleset = team[&amp;#39;ruleset&amp;#39;]
        league = team[&amp;#39;league&amp;#39;]
        roster_id = team[&amp;#39;roster&amp;#39;][&amp;#39;id&amp;#39;]
        race_name = team[&amp;#39;roster&amp;#39;][&amp;#39;name&amp;#39;]
        games_played = team[&amp;#39;record&amp;#39;][&amp;#39;games&amp;#39;]
        # add to dataframe
        df_teams.loc[t] = [team_id, division_id, division_name, league, ruleset, roster_id, race_name, games_played]
        if t % 100 == 0: 
            # write tmp data as hdf5 file
            print(t, end=&amp;#39;&amp;#39;)
            print(&amp;quot;.&amp;quot;, end=&amp;#39;&amp;#39;)
            df_teams.to_hdf(target, key=&amp;#39;df_teams&amp;#39;, mode=&amp;#39;w&amp;#39;)
    
    df_teams.to_hdf(target, key=&amp;#39;df_teams&amp;#39;, mode=&amp;#39;w&amp;#39;)
else:
    # read from hdf5 file
    df_teams = pd.read_hdf(&amp;#39;data/df_teams_20211215_211746.h5&amp;#39;)


df_teams[&amp;#39;roster_name&amp;#39;] = df_teams[&amp;#39;roster_id&amp;#39;].astype(str) + &amp;#39;_&amp;#39; + df_teams[&amp;#39;race_name&amp;#39;]

df_teams.shape

    &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;data/df_teams_20211230_192715.h5

(46831, 9)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;dataprep-add-ruleset_version-and-division_name&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Dataprep: Add ruleset_version and division_name&lt;/h2&gt;
&lt;p&gt;FUMBBL allows coaches to create their own rulesets to play their own leagues and tournaments with. For example, there is a so-called “Secret League” where coaches can play with “Ninja halflings”, “Ethereal” spirits etc. Instead of plain normal regular “Halflings” and “Shambling Undead” :-)&lt;/p&gt;
&lt;p&gt;Since we want the team strength for the official rulesets BB2016 and BB2020, we need to distinguish those matches from the matches that are played under different rules.&lt;/p&gt;
&lt;p&gt;Lets have look at the various divisions and leagues, which rulesets are used, and which races are played how often.
There are a lot of small leagues being played on FUMBBL. We only look at divisions and leagues with a sufficient volume of matches, or otherwise we do not have sufficient statistics for each race.&lt;/p&gt;
&lt;p&gt;So I aggregated the data by division, league and ruleset, and filtered on at least 150 different teams that have played at least once last year.&lt;/p&gt;
&lt;p&gt;Apart from the main “Divisions” that are part of FUMBBL, there were a few user-run leagues present in this table, so I looked up their names on FUMBBL and what ruleset is used (BB2016, BB2020 or some other variant). This information (contained in an xlsx) is added to the dataset below.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# add ruleset_version and division_name from xlsx
ruleset_division_names = pd.read_excel(&amp;#39;data/ruleset_division_names.xlsx&amp;#39;,  engine=&amp;#39;openpyxl&amp;#39;)

df_teams = pd.merge(df_teams, ruleset_division_names, on= [&amp;#39;league&amp;#39;, &amp;#39;ruleset&amp;#39;, &amp;#39;division_id&amp;#39;], how=&amp;#39;left&amp;#39;)

df_teams[&amp;#39;division_name&amp;#39;] = df_teams[&amp;#39;new_division_name&amp;#39;]

df_teams = df_teams.drop(&amp;#39;new_division_name&amp;#39;, 1)

df_teams[&amp;#39;division_id&amp;#39;] = pd.to_numeric(df_teams.division_id) 
df_teams[&amp;#39;roster_id&amp;#39;] = pd.to_numeric(df_teams.roster_id) 
df_teams[&amp;#39;team_id&amp;#39;] = pd.to_numeric(df_teams.team_id) 
df_teams[&amp;#39;games_played&amp;#39;] = pd.to_numeric(df_teams.games_played) 

df_teams[&amp;#39;league&amp;#39;] = pd.to_numeric(df_teams.league) 
df_teams[&amp;#39;ruleset&amp;#39;] = pd.to_numeric(df_teams.ruleset) 
&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;(df_teams
    .groupby([&amp;#39;ruleset&amp;#39;, &amp;#39;league&amp;#39;, &amp;#39;division_id&amp;#39;, &amp;#39;division_name&amp;#39;,  &amp;#39;ruleset_version&amp;#39;], dropna=False)
    .agg( n_teams = (&amp;#39;ruleset&amp;#39;, &amp;#39;count&amp;#39;)
    )
    .sort_values(&amp;#39;n_teams&amp;#39;, ascending = False)
    .query(&amp;#39;n_teams &amp;gt; 150&amp;#39;)[&amp;#39;n_teams&amp;#39;]
    .reset_index()
)&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
&lt;thead&gt;
&lt;tr style=&#34;text-align: right;&#34;&gt;
&lt;th&gt;
&lt;/th&gt;
&lt;th&gt;
ruleset
&lt;/th&gt;
&lt;th&gt;
league
&lt;/th&gt;
&lt;th&gt;
division_id
&lt;/th&gt;
&lt;th&gt;
division_name
&lt;/th&gt;
&lt;th&gt;
ruleset_version
&lt;/th&gt;
&lt;th&gt;
n_teams
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;
0
&lt;/th&gt;
&lt;td&gt;
1
&lt;/td&gt;
&lt;td&gt;
0
&lt;/td&gt;
&lt;td&gt;
10
&lt;/td&gt;
&lt;td&gt;
Blackbox
&lt;/td&gt;
&lt;td&gt;
bb2016
&lt;/td&gt;
&lt;td&gt;
10439
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
1
&lt;/th&gt;
&lt;td&gt;
1
&lt;/td&gt;
&lt;td&gt;
0
&lt;/td&gt;
&lt;td&gt;
1
&lt;/td&gt;
&lt;td&gt;
Ranked
&lt;/td&gt;
&lt;td&gt;
bb2016
&lt;/td&gt;
&lt;td&gt;
7944
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
2
&lt;/th&gt;
&lt;td&gt;
4
&lt;/td&gt;
&lt;td&gt;
0
&lt;/td&gt;
&lt;td&gt;
2
&lt;/td&gt;
&lt;td&gt;
Competitive
&lt;/td&gt;
&lt;td&gt;
bb2020
&lt;/td&gt;
&lt;td&gt;
6532
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
3
&lt;/th&gt;
&lt;td&gt;
6
&lt;/td&gt;
&lt;td&gt;
0
&lt;/td&gt;
&lt;td&gt;
5
&lt;/td&gt;
&lt;td&gt;
Regular_league
&lt;/td&gt;
&lt;td&gt;
bb2016
&lt;/td&gt;
&lt;td&gt;
5942
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
4
&lt;/th&gt;
&lt;td&gt;
303
&lt;/td&gt;
&lt;td&gt;
10263
&lt;/td&gt;
&lt;td&gt;
5
&lt;/td&gt;
&lt;td&gt;
Secret League
&lt;/td&gt;
&lt;td&gt;
bb2016
&lt;/td&gt;
&lt;td&gt;
2554
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
5
&lt;/th&gt;
&lt;td&gt;
2228
&lt;/td&gt;
&lt;td&gt;
9298
&lt;/td&gt;
&lt;td&gt;
5
&lt;/td&gt;
&lt;td&gt;
NAF
&lt;/td&gt;
&lt;td&gt;
mixed
&lt;/td&gt;
&lt;td&gt;
1466
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
6
&lt;/th&gt;
&lt;td&gt;
2
&lt;/td&gt;
&lt;td&gt;
0
&lt;/td&gt;
&lt;td&gt;
3
&lt;/td&gt;
&lt;td&gt;
Stunty Leeg
&lt;/td&gt;
&lt;td&gt;
bb2016
&lt;/td&gt;
&lt;td&gt;
607
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
7
&lt;/th&gt;
&lt;td&gt;
2198
&lt;/td&gt;
&lt;td&gt;
14708
&lt;/td&gt;
&lt;td&gt;
5
&lt;/td&gt;
&lt;td&gt;
SL BB2020
&lt;/td&gt;
&lt;td&gt;
bb2020
&lt;/td&gt;
&lt;td&gt;
452
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
8
&lt;/th&gt;
&lt;td&gt;
888
&lt;/td&gt;
&lt;td&gt;
11676
&lt;/td&gt;
&lt;td&gt;
5
&lt;/td&gt;
&lt;td&gt;
LegaGladio
&lt;/td&gt;
&lt;td&gt;
mixed
&lt;/td&gt;
&lt;td&gt;
345
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
9
&lt;/th&gt;
&lt;td&gt;
4
&lt;/td&gt;
&lt;td&gt;
14713
&lt;/td&gt;
&lt;td&gt;
5
&lt;/td&gt;
&lt;td&gt;
Test Open League BB2020
&lt;/td&gt;
&lt;td&gt;
bb2020
&lt;/td&gt;
&lt;td&gt;
296
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
10
&lt;/th&gt;
&lt;td&gt;
1049
&lt;/td&gt;
&lt;td&gt;
12026
&lt;/td&gt;
&lt;td&gt;
5
&lt;/td&gt;
&lt;td&gt;
NAF 7s
&lt;/td&gt;
&lt;td&gt;
bb2016
&lt;/td&gt;
&lt;td&gt;
196
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
11
&lt;/th&gt;
&lt;td&gt;
432
&lt;/td&gt;
&lt;td&gt;
10455
&lt;/td&gt;
&lt;td&gt;
5
&lt;/td&gt;
&lt;td&gt;
CIBBL
&lt;/td&gt;
&lt;td&gt;
bb2016
&lt;/td&gt;
&lt;td&gt;
189
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;dataprep-merging-the-match-data-with-the-team-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Dataprep: Merging the match data with the team data&lt;/h2&gt;
&lt;p&gt;For each match in the &lt;code&gt;df_mbt&lt;/code&gt; &lt;strong&gt;DataFrame&lt;/strong&gt; we can now add the team-level information from &lt;code&gt;df_teams&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;As both datasets contain ‘race_name’, we drop one of them.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df_mbt = pd.merge(df_mbt, df_teams.drop(&amp;#39;race_name&amp;#39;, 1), on=&amp;#39;team_id&amp;#39;, how=&amp;#39;left&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df_matches = pd.merge(df_matches, df_teams.drop([&amp;#39;race_name&amp;#39;, &amp;#39;roster_id&amp;#39;, &amp;#39;roster_name&amp;#39;, &amp;#39;games_played&amp;#39;], 1), left_on=&amp;#39;team1_id&amp;#39;, right_on = &amp;#39;team_id&amp;#39;, how=&amp;#39;left&amp;#39;)

df_matches[&amp;#39;team1_id&amp;#39;] = pd.to_numeric(df_matches.team1_id) 
df_matches = df_matches.drop(&amp;#39;team_id&amp;#39;, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;step-5-adding-team-tiers&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Step 5: adding team tiers&lt;/h1&gt;
&lt;p&gt;According to &lt;a href=&#34;https://www.thenaf.net/2017/05/tiers/&#34;&gt;this article from the NAF from 2017&lt;/a&gt;, already since 2010 efforts were made to balance things out a bit between the different team strengths. For example, the weaker teams get more gold to spend on players, or get more so-called “Star player points” to spend on skilling players up. According to &lt;a href=&#34;https://www.thenaf.net/tournaments/information/tiers-and-tiering/&#34;&gt;the NAF&lt;/a&gt;, traditionally team tiering consists of three groups, with Tier 1 being the strongest teams, and tier 3 the weakest teams. The GW BB2020 rule book also contains three tier groups, that are similar to the NAF tiers: except for Humans and Old World Alliance. And in november 2021, Games Workshop published an update of the three tier groups, now with High Elves moving from tier 2 to tier 1, and Old World Alliance moving back to tier 2.&lt;/p&gt;
&lt;p&gt;This is most naturally added to the &lt;code&gt;df_mbt&lt;/code&gt; dataframe, as it can differ for each team in a match.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;race_tiers = pd.read_excel(&amp;#39;data/race_tiers_mapping.xlsx&amp;#39;,  engine=&amp;#39;openpyxl&amp;#39;)
race_tiers = race_tiers[ [&amp;#39;race_name&amp;#39;, &amp;#39;bb2020_tier&amp;#39;, &amp;#39;naf_tier&amp;#39;, &amp;#39;bb2020_nov21_tier&amp;#39;]]
race_tiers = race_tiers.dropna()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# add bb2020 tiers
df_mbt = pd.merge(df_mbt, race_tiers, on=&amp;#39;race_name&amp;#39;, how=&amp;#39;left&amp;#39;)

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;save-all-prepped-datasets-as-hdf5-files&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Save all prepped datasets as HDF5 files&lt;/h1&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;target = &amp;#39;data/df_inducements_final.h5&amp;#39;
df_inducements.to_hdf(target, key=&amp;#39;df_inducements&amp;#39;, mode=&amp;#39;w&amp;#39;, format = &amp;#39;t&amp;#39;, complevel = 9)

target = &amp;#39;data/inducements_final.h5&amp;#39;
inducements.to_hdf(target, key=&amp;#39;inducements&amp;#39;, mode=&amp;#39;w&amp;#39;, format = &amp;#39;t&amp;#39;,  complevel = 9)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;target = &amp;#39;data/df_matches_final.h5&amp;#39;

df_matches.to_hdf(target, key=&amp;#39;df_matches&amp;#39;, mode=&amp;#39;w&amp;#39;, format = &amp;#39;t&amp;#39;,  complevel = 9)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;target = &amp;#39;data/df_mbt_final.h5&amp;#39;

df_mbt.to_hdf(target, key=&amp;#39;df_mbt&amp;#39;, mode=&amp;#39;w&amp;#39;, format = &amp;#39;t&amp;#39;,  complevel = 9)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;choosing-a-license-for-the-public-dataset&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Choosing a license for the public dataset&lt;/h1&gt;
&lt;p&gt;An important part of making data publicly available is being explicit about what is allowed if people want to use the dataset.
However, before we do so, we have to check if &lt;strong&gt;we&lt;/strong&gt; are actually allowed to publish the data. This is explained nicely &lt;a href=&#34;https://datacarpentry.org/blog/2016/06/data-licensing&#34;&gt;in a blogpost by Elizabeth Wickes&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Since our data will come from the &lt;strong&gt;FUMBBL.com&lt;/strong&gt; website, we check the &lt;a href=&#34;https://fumbbl.com/p/privacy&#34;&gt;&lt;strong&gt;Privacy policy&lt;/strong&gt;&lt;/a&gt; where all users, including myself have agreed on when signing up. It contains this part which is specific to the unauthenticated API, which we use to fetch the data, as well as additional public match data, such as which inducements are used in a match, and the Coach rankings of the playing coaches that were current when the match was played.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Content you provide through the website
All the information you provide through the website is processed by FUMBBL. This includes things such as forum posts, private message posts, blog entries, team and player names and biographies and news comments. Data provided this way is visible by other people on the website and in most cases public even to individuals without accounts (not including private messages), and as such are considered of public interest. If direct personal information is posted in public view, you can contact moderators to resolve this. Match records are also considered content in this context, and is also considered of public interest. This data is collected as the primary purpose of the website and it is of course entirely up to you how much of this is provided to FUMBBL. 

Third party sharing
Some of the public data is available through a public (*i.e. unauthenticated*) API, which shares some of the information provided by FUMBBL users in a way suitable for third-party websites and services to process.

The data available through the unauthenticated API is considered non-personal as it only reflects information that is public by its nature on the website. The authenticated API will only show information connected to the authenticated account.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I conclude that since the match data is already considered public content, there is no harm in collecting this public data in a structured dataset and placing this data in a public repository. The final step is then to decide what others are allowed to do with this data. In practice, this means choosing a license under which to release the dataset. I decided to choose a &lt;a href=&#34;https://creativecommons.org/publicdomain/zero/1.0/&#34;&gt;CC0 license&lt;/a&gt;: this places the data in the public domain, and people can use the dataset as they wish. Citing or mentioning the source of the data would still be appreciated of course.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>OpenAI Gym&#39;s FrozenLake: Converging on the true Q-values</title>
      <link>/post/frozenlake-qlearning-convergence/</link>
      <pubDate>Sun, 07 Mar 2021 00:00:00 +0000</pubDate>
      <guid>/post/frozenlake-qlearning-convergence/</guid>
      <description>


&lt;p&gt;(Photo by Ryan Fishel on Unsplash)&lt;/p&gt;
&lt;p&gt;This blog post concerns a famous “toy” problem in Reinforcement Learning, the &lt;a href=&#34;https://gym.openai.com/envs/FrozenLake-v0/&#34;&gt;FrozenLake environment&lt;/a&gt;. We compare solving an environment with RL by reaching &lt;strong&gt;maximum performance&lt;/strong&gt; versus obtaining the &lt;strong&gt;true state-action values&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(Q_{s,a}\)&lt;/span&gt;. In doing so I learned a lot about RL as well as about Python (such as the existence of a &lt;code&gt;ggplot&lt;/code&gt; clone for Python, &lt;code&gt;plotnine&lt;/code&gt;, see this blog post for some cool examples).&lt;/p&gt;
&lt;p&gt;FrozenLake was created by OpenAI in 2016 as part of their Gym python package for Reinforcement Learning. Nowadays, the interwebs is full of tutorials how to “solve” FrozenLake. Most of them focus on performance in terms of episodic reward. As soon as this maxes out the algorithm is often said to have converged. For example, in this &lt;a href=&#34;https://stats.stackexchange.com/questions/206944/how-do-i-know-when-a-q-learning-algorithm-converges&#34;&gt;question on Cross-Validated&lt;/a&gt; about Convergence and Q-learning:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;In practice, a reinforcement learning algorithm is considered to converge when the learning curve gets flat and no longer increases.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Now, for &lt;strong&gt;Q-learning&lt;/strong&gt; it has been proven that, &lt;em&gt;under certain conditions&lt;/em&gt;, the Q-values convergence to their true values. But does this happens when the performance maxes out? In this blog we’ll see that this is not generally the case.&lt;/p&gt;
&lt;p&gt;We start with obtaining the true, exact state-action values. For this we use Dynamic Programming (DP). Having implemented Dynamic Programming (DP) for the FrozenLake environment as an exercise notebook already (created by Udacity, go check them out), this was a convenient starting point.&lt;/p&gt;
&lt;div id=&#34;loading-the-packages-and-modules&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Loading the packages and modules&lt;/h2&gt;
&lt;p&gt;We need various Python packages and modules.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# data science packages
import numpy as np
import pandas as pd
import plotnine as p9
import matplotlib.pyplot as plt
%matplotlib inline

# RL algorithms
from qlearning import *
from dp import *

# utils
from helpers import *
from plot_utils import plot_values
import copy
import dill

from frozenlake import FrozenLakeEnv

env = FrozenLakeEnv(is_slippery = True)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;frozen-lake-environment-description&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Frozen Lake Environment description&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake (G).
The water is mostly frozen (F), but there are a few holes (H) where the ice has melted.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-03-07-qlearning_frozenlake_convergence_files/Frozen-Lake.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;If you step into one of those holes, you’ll fall into the freezing water. At this time, there’s an international frisbee shortage, so it’s absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won’t always move in the direction you intend.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The episode ends when you reach the goal or fall in a hole. You receive a reward of 1 if you reach the goal, and zero otherwise.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The agent moves through a &lt;span class=&#34;math inline&#34;&gt;\(4 \times 4\)&lt;/span&gt; gridworld, with states numbered as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[[ 0  1  2  3]
 [ 4  5  6  7]
 [ 8  9 10 11]
 [12 13 14 15]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and the agent has 4 potential actions:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;LEFT = 0
DOWN = 1
RIGHT = 2
UP = 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The FrozenLake Gym environment has been amended to make the one-step dynamics accessible to the agent. For example, if we are in State &lt;code&gt;s = 1&lt;/code&gt; and we perform action &lt;code&gt;a = 0&lt;/code&gt;, the probabilities of ending up in new states, including the associated rewards are contained in the &lt;span class=&#34;math inline&#34;&gt;\(P_{s,a}\)&lt;/span&gt; array:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;env.P[1][0]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[(0.3333333333333333, 1, 0.0, False),
 (0.3333333333333333, 0, 0.0, False),
 (0.3333333333333333, 5, 0.0, True)]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The FrozenLake environment is highly stochastic, with a very sparse reward: only when the agent reaches the goal, a reward of &lt;code&gt;+1&lt;/code&gt; is obtained. This means that if we do not set a discount rate, the agent can keep on wandering around without receiving a learning “signal” that can be propagated back through the preceding state-actions (since falling into the holes does not result in a negative reward) and thus learning very slowly. We will focus on the discounting case (&lt;code&gt;gamma = 0.95&lt;/code&gt;) for this reason (less computation needed for convergence), but compare also with the undiscounted case.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;solving-frozen-lake-using-dp&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Solving Frozen Lake using DP&lt;/h2&gt;
&lt;p&gt;Let us solve FrozenLake first for the no discounting case (&lt;code&gt;gamma = 1&lt;/code&gt;).
The Q-value for the first state will then tell us the average episodic reward, which for FrozenLake translates into the percentage of episodes in which the Agent succesfully reaches its goal.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;policy_pi, V_pi = policy_iteration(env, gamma = 1, theta=1e-9, \
                                   verbose = False)


plot_values(V_pi)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_9_1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The Q-value for state &lt;code&gt;s = 0&lt;/code&gt; is &lt;code&gt;0.824&lt;/code&gt;. This means that for &lt;code&gt;gamma = 1&lt;/code&gt; and following an optimal policy &lt;span class=&#34;math inline&#34;&gt;\(\pi^*\)&lt;/span&gt;, 82.4% of all episodes ends in succes.&lt;/p&gt;
&lt;p&gt;As already mentioned above, for computational reasons we will apply Q-learning to the environment with &lt;code&gt;gamma = 0.95&lt;/code&gt;. So… lets solve FrozenLake for &lt;code&gt;gamma = 0.95&lt;/code&gt; as well:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# obtain the optimal policy and optimal state-value function
policy_pi_dc, V_pi_dc = policy_iteration(env, gamma = 0.95, theta=1e-9, \
                                   verbose = False)

Q_perfect = create_q_dict_from_v(env, V_pi_dc, gamma = 0.95)

df_true = convert_vals_to_pd(V_pi_dc)

plot_values(V_pi_dc)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_11_1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now, comparing the optimal policies for &lt;code&gt;gamma = 0.95&lt;/code&gt; and for &lt;code&gt;gamma = 1&lt;/code&gt; (not shown here) we find that they are not the same. Therefore, 82.4% succes rate is likely an upper bound for &lt;code&gt;gamma = 0.95&lt;/code&gt;, since introducing discounting in this stochastic environment can (intuitively) either have no effect on the optimal policy, or favor more risky (lower succes rate) but faster (less discounting) policies, leading to a lower overall succes rate.&lt;/p&gt;
&lt;p&gt;For example, for the undiscounted case, the Agent is indifferent in choosing direction in the first state. If it ends up going &lt;strong&gt;right&lt;/strong&gt; , it can choose UP and wander around at the top row at no cost until it reaches the starting state again. As soon as this wandering around gets a cost by discounting we see (not shown) that the Agent is no longer indifferent, and does NOT want to end up wandering in the top row, but chooses to play LEFT in state &lt;code&gt;s = 0&lt;/code&gt; instead.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;checking-the-performance-of-an-optimal-greedy-policy-based-on-perfect-q-values&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Checking the performance of an optimal greedy policy based on perfect Q-values&lt;/h1&gt;
&lt;p&gt;Now that we have the &lt;span class=&#34;math inline&#34;&gt;\(Q_{s,a}\)&lt;/span&gt; values corresponding to the optimal policy given that &lt;code&gt;gamma = 0.95&lt;/code&gt;, we can check its performance. To do so, we use brute force and simulate the average reward under the optimal policy for a large number of episodes.&lt;/p&gt;
&lt;p&gt;To do so, I wrote a function &lt;code&gt;test_performance()&lt;/code&gt; by taking the &lt;code&gt;q_learning()&lt;/code&gt; function, removing the learning (Q-updating) part and setting epsilon to zero when selecting an action (i.e. a pure greedy policy based on a given Q-table).&lt;/p&gt;
&lt;p&gt;Playing around with the binomial density in &lt;code&gt;R&lt;/code&gt; (&lt;code&gt;summary(rbinom(1e5, 1e5, prob = 0.8)/1e5&lt;/code&gt;) tells me that choosing 100.000 episodes gives a precision of around three decimals in estimating the probability of succes. This is good enough for me.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Obtain Q for Gamma = 0.95 and convert to defaultdict 
Q_perfect = create_q_dict_from_v(env, V_pi_dc, gamma = 0.95)

fullrun = 0

if fullrun == 1:
    d = []
    runs = 100
    for i in range(runs):
        avg_scores = test_performance(Q_perfect, env, num_episodes = 1000, \
                                   plot_every = 1000, verbose = False)
        d.append({&amp;#39;run&amp;#39;: i,
                 &amp;#39;avg_score&amp;#39;: np.mean(avg_scores)})
        print(&amp;quot;\ri {}/{}&amp;quot;.format(i, runs), end=&amp;quot;&amp;quot;)
        sys.stdout.flush() 

    d_perfect = pd.DataFrame(d)
    d_perfect.to_pickle(&amp;#39;cached/scores_perfect_0.95.pkl&amp;#39;)
else:
    d_perfect = pd.read_pickle(&amp;#39;cached/scores_perfect_0.95.pkl&amp;#39;)

&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;round(np.mean(d_perfect.avg_score), 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.781&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thus, we find that with the true optimal policy for &lt;code&gt;gamma = 0.95&lt;/code&gt;, in the long run 78% of all episodes is succesful. This is therefore the expected plateau value for the learning curve in Q-learning, provided that the exploration rate has become sufficiently small.&lt;/p&gt;
&lt;p&gt;Let’s move on to Q-learning and convergence.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;solving-frozenlake-using-q-learning&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;“Solving” FrozenLake using Q-learning&lt;/h1&gt;
&lt;p&gt;The typical RL tutorial approach to solve a simple MDP as FrozenLake is to choose a constant learning rate, not too high, not too low, say &lt;span class=&#34;math inline&#34;&gt;\(\alpha = 0.1\)&lt;/span&gt;. Then, the exploration parameter &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; starts at 1 and is gradually reduced to a floor value of say &lt;span class=&#34;math inline&#34;&gt;\(\epsilon = 0.0001\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Lets solve FrozenLake this way, monitoring the learning curve (average reward per episode) as it learns, and compare the learned Q-values with the true Q-values found using DP.&lt;/p&gt;
&lt;p&gt;I wrote Python functions that generate a &lt;strong&gt;decay schedule&lt;/strong&gt;, a 1D numpy array of &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; values, with length equal to the total number of episode the Q-learning algorithm is to run. This array is passed on to the Q-learning algorithm, and used during learning.&lt;/p&gt;
&lt;p&gt;It is helpful to visualize the decay schedule of &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; to check that it is reasonable before we start to use them with our Q-learning algorithm. I played around with the decay rate until the “elbow” of the curve is around 20% of the number of episodes, and reaches the desired minimal end value (&lt;span class=&#34;math inline&#34;&gt;\(\epsilon = 0.0001\)&lt;/span&gt;).&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def create_epsilon_schedule(num_episodes,  \
                       epsilon_start=1.0, epsilon_decay=.9999, epsilon_min=1e-4):
    x = np.arange(num_episodes)+1
    y = np.full(num_episodes, epsilon_start)
    y = np.maximum((epsilon_decay**x)*epsilon_start, epsilon_min)
    return y

epsilon_schedule = create_epsilon_schedule(100_000)

plot_schedule(epsilon_schedule, ylab = &amp;#39;Epsilon&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_20_0.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;My version of the Q-learning algorithm has slowly evolved to include more and more lists of things to monitor during the execution of the algorithm. Every 100 episodes, a copy of &lt;span class=&#34;math inline&#34;&gt;\(Q^{*}_{s}\)&lt;/span&gt; and of &lt;span class=&#34;math inline&#34;&gt;\(Q_{s, a}\)&lt;/span&gt; is stored in a list, as well as the average reward over this episode, as well as the final &lt;span class=&#34;math inline&#34;&gt;\(Q_{s,a}\)&lt;/span&gt; table, and &lt;span class=&#34;math inline&#34;&gt;\(N_{s,a}\)&lt;/span&gt; that kept track of how often each state-action was chosen. The &lt;code&gt;dill&lt;/code&gt; package is used to store these datastructures on disk to avoid the need to rerun the algorithm every time the notebook is run.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# K / N + K decay learning rate schedule
# fully random policy

plot_every = 100

n_episodes = len(epsilon_schedule)

fullrun = 0

random.seed(123)
np.random.seed(123)

if fullrun == 1:
    Q_sarsamax, N_sarsamax, avg_scores, Qlist, Qtable_list = q_learning(env, num_episodes = n_episodes, \
                                                eps_schedule = epsilon_schedule,\
                                                alpha = 0.1, \
                                                gamma =  0.95, \
                                                plot_every = plot_every, \
                                                verbose = True, log_full = True)
    
    with open(&amp;#39;cached/es1_Q_sarsamax.pkl&amp;#39;, &amp;#39;wb&amp;#39;) as f:
        dill.dump(Q_sarsamax, f)
    with open(&amp;#39;cached/es1_avg_scores.pkl&amp;#39;, &amp;#39;wb&amp;#39;) as f:
        dill.dump(avg_scores, f)
    with open(&amp;#39;cached/es1_Qlist.pkl&amp;#39;, &amp;#39;wb&amp;#39;) as f:
        dill.dump(Qlist, f)   
    with open(&amp;#39;cached/es1_Qtable_list.pkl&amp;#39;, &amp;#39;wb&amp;#39;) as f:
        dill.dump(Qtable_list, f)           
else:
    with open(&amp;#39;cached/es1_Q_sarsamax.pkl&amp;#39;, &amp;#39;rb&amp;#39;) as f:
        Q_sarsamax = dill.load(f)
    with open(&amp;#39;cached/es1_avg_scores.pkl&amp;#39;, &amp;#39;rb&amp;#39;) as f:
        avg_scores = dill.load(f)
    with open(&amp;#39;cached/es1_Qlist.pkl&amp;#39;, &amp;#39;rb&amp;#39;) as f:
        Qlist = dill.load(f)   
    with open(&amp;#39;cached/es1_Qtable_list.pkl&amp;#39;, &amp;#39;rb&amp;#39;) as f:
        Qtable_list = dill.load(f)            

Q_es1_lasttable = [np.max(Q_sarsamax[key]) if key in Q_sarsamax \
     else 0 for key in np.arange(env.nS)]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lets plot the learning curve. Here &lt;code&gt;plotnine&lt;/code&gt;, a &lt;code&gt;ggplot&lt;/code&gt; clone in Python comes in handy.&lt;/p&gt;
&lt;p&gt;As we can see below, the “recipe” for solving FrozenLake has worked really well. The displayed red line gives the theoretical optimal performance for &lt;code&gt;gamma = 0.95&lt;/code&gt;, I used a &lt;code&gt;loess&lt;/code&gt; smoother so we can more easily compare the Q-learning results with the theoretical optimum.&lt;/p&gt;
&lt;p&gt;We can see that the Q-learning algorithm has indeed found a policy that performs optimal. This appears to happen at around 60.000 episodes. We return to this later.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# plot performance
df_scores = pd.DataFrame(
    {&amp;#39;episode_nr&amp;#39;: np.linspace(0,n_episodes,len(avg_scores),endpoint=False),
     &amp;#39;avg_score&amp;#39;: np.asarray(avg_scores)})

(p9.ggplot(data = df_scores.loc[df_scores.episode_nr &amp;gt; -1], mapping = p9.aes(x = &amp;#39;episode_nr&amp;#39;, y = &amp;#39;avg_score&amp;#39;))
    + p9.geom_point(colour = &amp;#39;gray&amp;#39;)
    + p9.geom_smooth(method = &amp;#39;loess&amp;#39;)
    + p9.geom_hline(yintercept = 0.781, colour = &amp;quot;red&amp;quot;)
    + p9.geom_vline(xintercept = 60_000, color = &amp;quot;blue&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_24_0.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;But now comes the big question: did the &lt;span class=&#34;math inline&#34;&gt;\(Q^{*}_{a}\)&lt;/span&gt; estimates converge onto the true values as well?&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;q_showdown = pd.DataFrame(
    {&amp;#39;Q_es1_lasttable&amp;#39;: Q_es1_lasttable,
     &amp;#39;q_true&amp;#39;: V_pi_dc})
q_showdown[&amp;#39;state&amp;#39;] = range(16)

q_showdown = pd.melt(q_showdown, [&amp;#39;state&amp;#39;])

(p9.ggplot(data = q_showdown, mapping = p9.aes(x = &amp;#39;state&amp;#39;, y = &amp;#39;value&amp;#39;, color = &amp;#39;factor(variable)&amp;#39;))
+ p9.geom_point(size = 5, shape = &amp;#39;x&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_26_0.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Nope, they did not. Ok, most learned Q-values are close to their true values, but they clearly did not converge exactly to their true value.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;plotting-the-learning-curves-for-all-the-state-action-values&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Plotting the learning curves for all the state-action values&lt;/h1&gt;
&lt;p&gt;To really understand what is going on, I found it helpful to plot the learning curves for all the 16 x 4 = 64 state-action values at the same time. Here &lt;code&gt;plotnine&lt;/code&gt; really shines. I leave out the actual estimates, and only plot a &lt;code&gt;loess&lt;/code&gt; smoothed curve for each of the state-action values over time.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# convert to pandas df
dfm = list_of_tables_to_df(Qtable_list)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;#10 s
(p9.ggplot(data = dfm.loc[(dfm.episode &amp;gt; -1)], mapping = p9.aes(x = &amp;#39;episode&amp;#39;, y = &amp;#39;value&amp;#39;, group = &amp;#39;action&amp;#39;, color = &amp;#39;factor(action)&amp;#39;))
    #+ p9.geom_point(shape = &amp;#39;x&amp;#39;) 
    + p9.geom_smooth(method = &amp;#39;loess&amp;#39;)
    + p9.geom_hline(data = df_true, mapping = p9.aes(yintercept = &amp;#39;value&amp;#39;), color = &amp;#39;red&amp;#39;)
    + p9.geom_vline(xintercept = 600, color = &amp;#39;blue&amp;#39;)
    + p9.facet_wrap(&amp;quot;variable&amp;quot;, ncol = 4)
    + p9.theme(subplots_adjust={&amp;#39;wspace&amp;#39;:0.4}) # fix a displaying issue
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_30_0.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;First the main question: what about convergence of &lt;span class=&#34;math inline&#34;&gt;\(Q^{*}_{a}\)&lt;/span&gt;? For this we need only to look, for each state, at the action with the highest value, and compare that to the true value (red horizontal lines). Most of the values appear to have converged to a value close to the true value, but Q3 is clearly still way off. Note that we using smoothing here to average out the fluctuations around the true value.&lt;/p&gt;
&lt;p&gt;We noted earlier that at around episode 60.000, the optimal policy emerges and the learning curve becomes flat. Now, the most obvious reason why performance increases is because the value of &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; is decaying, so the deleterious effects of exploration should go down, and performance should go up.&lt;/p&gt;
&lt;p&gt;Another reason that performance goes up could be that the greedy policy is improving. It is interesting to examine whether at this point, meaningfull changes in the greedy policy still occur. Meaningfull changes in policy are caused by changes in the estimated state-action values. For example, we might expect two or more state-action value lines crossing, with the “right” action becoming dominant over the “wrong” action. Is this indeed the case?&lt;/p&gt;
&lt;p&gt;Indeed, from the plot above, with the change point at around episode 600 (x 100),a change occurs in Q6, where the actions 0 and 2 cross. However, from the true Q-value table (not shown) we see that both actions are equally rewarding, so the crossing has no effect on performance. Note that only one of the two converges to the true value, because the exploration rate becomes so low that learning for the other action almost completely stops in the end.&lt;/p&gt;
&lt;p&gt;lets zoom in then at the states that have low expected reward, Q0, Q1, Q2, and Q3. These are difficult to examine in the plot above, and have actions with expected rewards that are similar and therefore more difficult to resolve (BUT: since the difference in expected reward is small, the effect of resolving them on the overall performance is small as well). For these we plot the actual state-actions values:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;(p9.ggplot(data = dfm[dfm.variable.isin([&amp;#39;Q0&amp;#39;, &amp;#39;Q1&amp;#39;,&amp;#39;Q2&amp;#39;, &amp;#39;Q3&amp;#39;])], mapping = p9.aes(x = &amp;#39;episode&amp;#39;, y = &amp;#39;value&amp;#39;, group = &amp;#39;action&amp;#39;, color = &amp;#39;factor(action)&amp;#39;))
    + p9.geom_point(shape = &amp;#39;x&amp;#39;) 
    #+ p9.geom_smooth(method = &amp;#39;loess&amp;#39;)
    + p9.geom_hline(data = df_true[df_true.variable.isin([&amp;#39;Q0&amp;#39;, &amp;#39;Q1&amp;#39;,&amp;#39;Q2&amp;#39;, &amp;#39;Q3&amp;#39;])], mapping = p9.aes(yintercept = &amp;#39;value&amp;#39;), color = &amp;#39;red&amp;#39;)
    + p9.geom_vline(xintercept = 600, color = &amp;#39;blue&amp;#39;)
    + p9.facet_wrap(&amp;quot;variable&amp;quot;, scales = &amp;#39;free_y&amp;#39;)
    + p9.theme(subplots_adjust={&amp;#39;wspace&amp;#39;:0.15})
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_33_0.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From Q1, Q2 and Q3, we can see that exploration really goes down at around episode 500 (x 100) (&lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; at this point is &lt;code&gt;0.007&lt;/code&gt;), and with the optimal action standing out already long before reaching this point.&lt;/p&gt;
&lt;p&gt;Only with Q2 there is a portion of the learning curve where action 1 has the highest value, and is chosen for quite some time before switching back to action 0 again at around episode 60.000. Let’s compare with the true values for Q2:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;Q_perfect[2]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([0.15347714, 0.14684971, 0.14644451, 0.13958106])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, the difference in expected reward between the actions in state 2 is really small, and because of the increasingly greedy action selection, only action 0 converges to its true values, with the other values “frozen” in place because of the low exploration rate.&lt;/p&gt;
&lt;p&gt;Now, after analyzing what happens if we apply the “cookbook” approach to solving problems using RL, let’s change our attention to getting convergence for preferably ALL the state-action values.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;q-learning-theoretical-sufficient-conditions-for-convergence&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Q-learning: Theoretical sufficient conditions for convergence&lt;/h1&gt;
&lt;p&gt;According to our RL bible (Sutton &amp;amp; Barto), to obtain &lt;strong&gt;exact&lt;/strong&gt; convergence, we need two conditions to hold.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The first is that all states continue to be visited, and&lt;/li&gt;
&lt;li&gt;the second is that the learning rate goes to zero.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;continuous-exploration-visiting-all-the-states&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Continuous exploration: visiting all the states&lt;/h2&gt;
&lt;p&gt;The nice thing of Q-learning is that it is an &lt;strong&gt;off-policy&lt;/strong&gt; learning algorithm. This means that no matter what the actual policy is used to explore the states, the Q-values we learn correspond to the expected reward when following the &lt;strong&gt;optimal policy&lt;/strong&gt;. Which is quite miraculous if you ask me.&lt;/p&gt;
&lt;p&gt;Now, our (admittedly, a bit academic) goal is getting &lt;strong&gt;all&lt;/strong&gt; of the learned state values &lt;strong&gt;as close as possible&lt;/strong&gt; to the true values. An epsilon-greedy policy with a low &lt;span class=&#34;math inline&#34;&gt;\(epsilon\)&lt;/span&gt; would spent a lot of time by choosing state-actions that are on the optimal path between start and goal state, and would only rarely visit low value states, or choose low value state-actions.&lt;/p&gt;
&lt;p&gt;Because we can choose any policy we like, I chose a completely &lt;strong&gt;random&lt;/strong&gt; policy. This way, the Agent is more likely to end up in low value states and estimate the Q-values of those state-actions accurately as well.&lt;/p&gt;
&lt;p&gt;Note that since theFrozen Lake enviroment has a lot of inherent randomness already because of the ice being slippery, a policy with a low &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; (most of the time exploiting and only rarely exploring) will still bring the agent in low values states, but this would require much more episodes.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;convergence-and-learning-rate-schedules&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Convergence and learning rate schedules&lt;/h2&gt;
&lt;p&gt;For a particular constant learning rate, provided that it is not too high, the estimated Q-values will converge to a situation where they start to fluctuate around their true values.
If we subsequently lower the learning rate, the scale of the fluctuations (their &lt;strong&gt;variance&lt;/strong&gt;) will decrease.
If the learning rate is gradually decayed to zero, the estimates will converge.
According to Sutton &amp;amp; Barto (2018), the &lt;strong&gt;decay schedule&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; must obey two constraints to assure convergence: (p 33).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;sum of increments must go to infinity&lt;/li&gt;
&lt;li&gt;sum of the square of increments must go to zero&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;On the interwebs, I found two formulas that are often used to decay hyperparameters of Reinforcement Learning algorithms:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\alpha_n = K / (K + n)\)&lt;/span&gt; (Eq. 1)&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\alpha_n = \Sigma^{\infty}_{n = 1} \delta^n \alpha_0\)&lt;/span&gt; (Eq. 2)&lt;/p&gt;
&lt;p&gt;Again, just as with the decay schedule for &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;, it is helpful to visualize the decay schedules to check that they are reasonable before we start to use them with our Q-learning algorithm.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Eq 1
def create_alpha_schedule(num_episodes, \
                     alpha_K = 100, alpha_min = 1e-3):
    x = np.arange(num_episodes)+1
    y = np.maximum(alpha_K/(x + alpha_K), alpha_min)
    return y

# Eq 2
def create_alpha_schedule2(num_episodes,  \
                       alpha_start=1.0, alpha_decay=.999, alpha_min=1e-3):
    x = np.arange(num_episodes)+1
    y = np.full(num_episodes, alpha_start)
    y = np.maximum((alpha_decay**x)*alpha_start, alpha_min)
    return y

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I played a bit with the shape parameter to get a curve with the “elbow” around 20% of the episodes.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;alpha_schedule = create_alpha_schedule(num_episodes = 500_000, \
                                       alpha_K = 5_000)

plot_schedule(alpha_schedule, ylab = &amp;#39;Alpha&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_44_0.png&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;round(min(alpha_schedule), 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.01&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This curve decays &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; in 500.000 episodes to &lt;code&gt;0.01&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The second formula (Equation 2) decays &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; even further, to &lt;code&gt;0.001&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;alpha_schedule2 = create_alpha_schedule2(num_episodes = 500_000, \
                                        alpha_decay = 0.99997)

plot_schedule(alpha_schedule2, ylab = &amp;#39;Alpha&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_47_0.png&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;min(alpha_schedule2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.001&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;running-the-q-learning-algorithm-for-different-learning-rate-schedules&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Running the Q-learning algorithm for different learning rate schedules&lt;/h1&gt;
&lt;p&gt;We start with the decay function that follows Equation 1. To get a full random policy, we set &lt;span class=&#34;math inline&#34;&gt;\(\epsilon = 1\)&lt;/span&gt;. Note that this gives awful performance where the learning curve suggests it is hardly learning anything at all. However, wait until we try out a fully exploiting policy on the Q-value table learned during this run!&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# K / N + K decay learning rate schedule
# fully random policy

plot_every = 100

alpha_schedule = create_alpha_schedule(num_episodes = 500_000, \
                                       alpha_K = 5_000)

n_episodes = len(alpha_schedule)

fullrun = 0

if fullrun == 1:
    Q_sarsamax, N_sarsamax, avg_scores, Qlist = q_learning(env, num_episodes = n_episodes, \
                                                eps = 1,\
                                                alpha_schedule = alpha_schedule, \
                                                gamma =  0.95, \
                                                plot_every = plot_every, \
                                                verbose = True)
    
    with open(&amp;#39;cached/as1_Q_sarsamax.pkl&amp;#39;, &amp;#39;wb&amp;#39;) as f:
        dill.dump(Q_sarsamax, f)
    with open(&amp;#39;cached/as1_avg_scores.pkl&amp;#39;, &amp;#39;wb&amp;#39;) as f:
        dill.dump(avg_scores, f)
    with open(&amp;#39;cached/as1_Qlist.pkl&amp;#39;, &amp;#39;wb&amp;#39;) as f:
        dill.dump(Qlist, f)        
else:
    with open(&amp;#39;cached/as1_Q_sarsamax.pkl&amp;#39;, &amp;#39;rb&amp;#39;) as f:
        Q_sarsamax = dill.load(f)
    with open(&amp;#39;cached/as1_avg_scores.pkl&amp;#39;, &amp;#39;rb&amp;#39;) as f:
        avg_scores = dill.load(f)
    with open(&amp;#39;cached/as1_Qlist.pkl&amp;#39;, &amp;#39;rb&amp;#39;) as f:
        Qlist = dill.load(f)      

Q_as1_lasttable = [np.max(Q_sarsamax[key]) if key in Q_sarsamax \
     else 0 for key in np.arange(env.nS)]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is the learning curve for this run of Q-learning:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# plot performance
plt.plot(np.linspace(0,n_episodes,len(avg_scores),endpoint=False), np.asarray(avg_scores))
plt.xlabel(&amp;#39;Episode Number&amp;#39;)
plt.ylabel(&amp;#39;Average Reward (Over Next %d Episodes)&amp;#39; % plot_every)
plt.show()

print((&amp;#39;Best Average Reward over %d Episodes: &amp;#39; % plot_every), np.max(avg_scores))    
 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_53_0.png&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Best Average Reward over 100 Episodes:  0.08&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Pretty awfull huh? Now let us check out the performance of the learned Q-table:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fullrun = 0
runs = 100

if fullrun == 1:
    d_q = []
    for i in range(runs):
        avg_scores = test_performance(Q_sarsamax, env, num_episodes = 1000, \
                                   plot_every = 1000, verbose = False)
        d_q.append({&amp;#39;run&amp;#39;: i,
                 &amp;#39;avg_score&amp;#39;: np.mean(avg_scores)})  
        print(&amp;quot;\ri = {}/{}&amp;quot;.format(i+1, runs), end=&amp;quot;&amp;quot;)
        sys.stdout.flush() 
        
    d_qlearn = pd.DataFrame(d_q)
    d_qlearn.to_pickle(&amp;#39;cached/scores_qlearn_0.95.pkl&amp;#39;)
else:
    d_qlearn = pd.read_pickle(&amp;#39;cached/scores_qlearn_0.95.pkl&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;round(np.mean(d_qlearn.avg_score), 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.778&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Bam! Equal to the performance of the optimal policy found using Dynamic programming (sampling error (2x SD) is +/- 0.008). The random policy has actual learned Q-values that for a greedy policy result in optimal performance!&lt;/p&gt;
&lt;div id=&#34;but-what-about-convergence&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;But what about convergence?&lt;/h2&gt;
&lt;p&gt;Ok, so Q-learning found an optimal policy. But did it converge?
Our &lt;code&gt;q_learning()&lt;/code&gt; function made a list of Q-tables while learning, adding a new table every 100 episodes. This gives us 5.000 datapoints for each Q-value, which we can plot to visually check for convergence.&lt;/p&gt;
&lt;p&gt;As with the list of state-action tables above, It takes some datawrangling to get the list of Q-tables in a nice long pandas DataFrame suitable for plotting. This is hidden away in the &lt;code&gt;list_to_df()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# 13s
dfm = list_to_df(Qlist)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;#10 s
(p9.ggplot(data = dfm.loc[(dfm.episode &amp;gt; -1)], mapping = p9.aes(x = &amp;#39;episode&amp;#39;, y = &amp;#39;value&amp;#39;))
    + p9.geom_point() 
    + p9.geom_smooth(method = &amp;quot;loess&amp;quot;, color = &amp;quot;yellow&amp;quot;)
    + p9.geom_hline(data = df_true, mapping = p9.aes(yintercept = &amp;#39;value&amp;#39;), color = &amp;#39;red&amp;#39;)
    + p9.facet_wrap(&amp;quot;variable&amp;quot;, scales = &amp;#39;free_y&amp;#39;)
    + p9.theme(subplots_adjust={&amp;#39;wspace&amp;#39;:0.4}) # fix plotting issue
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_60_0.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This looks good, lets zoom in at one of the more noisier Q-values, &lt;code&gt;Q14&lt;/code&gt;.
In the learning schedule used, the lowest value for &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; is &lt;code&gt;0.01&lt;/code&gt;.
At this value of the learning rate, there is still considerable variation around the true value.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;(p9.ggplot(data = dfm.loc[(dfm.variable == &amp;#39;Q10&amp;#39;) &amp;amp; (dfm.episode &amp;gt; -1)], mapping = p9.aes(x = &amp;#39;episode&amp;#39;, y = &amp;#39;value&amp;#39;))
    + p9.geom_point() 
    + p9.geom_smooth(method = &amp;quot;loess&amp;quot;, color = &amp;quot;yellow&amp;quot;)
    + p9.geom_hline(data = df_true[df_true.variable == &amp;#39;Q10&amp;#39;], mapping = p9.aes(yintercept = &amp;#39;value&amp;#39;), color = &amp;#39;red&amp;#39;)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_62_0.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Suppose we did not know the true value of Q(S = 14), and wanted to estimate it using Q-learning. From the plot above, an obvious strategy is to average all the values in the tail of the learning rate schedule, say after episode 400.000.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# drop burn-in, then average Q-vals by variable
Q_est = (dfm.loc[dfm.episode &amp;gt; 4000]
         .groupby([&amp;#39;variable&amp;#39;])
         .mean()
        )

# convert to a 1D array sorted by Q-value
Q_est[&amp;#39;sort_order&amp;#39;] = [int(str.replace(x, &amp;#39;Q&amp;#39;, &amp;#39;&amp;#39;)) \
                       for x in Q_est.index.values]

Q_est = Q_est.sort_values(by=[&amp;#39;sort_order&amp;#39;])

Q_est_as1 = Q_est[&amp;#39;value&amp;#39;].values
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lets compare these with the final estimated values, and with true values:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;Q_as1_lasttable = [np.max(Q_sarsamax[key]) if key in Q_sarsamax \
             else 0 for key in np.arange(env.nS)]

q_showdown = pd.DataFrame(
    {&amp;#39;q_est_as1&amp;#39;: Q_est_as1,
     &amp;#39;q_est_as1_lasttable&amp;#39;: Q_as1_lasttable,
     &amp;#39;q_true&amp;#39;: V_pi_dc})
q_showdown[&amp;#39;state&amp;#39;] = range(16)

q_showdown = pd.melt(q_showdown, [&amp;#39;state&amp;#39;])

(p9.ggplot(data = q_showdown, mapping = p9.aes(x = &amp;#39;state&amp;#39;, y = &amp;#39;value&amp;#39;, color = &amp;#39;factor(variable)&amp;#39;))
+ p9.geom_point(size = 5, shape = &amp;#39;x&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_66_0.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Pretty close! So here we see Q-learning finally delivering on its convergence promise.&lt;/p&gt;
&lt;p&gt;And what about final values vs averaging? Averaging seems to have improved the estimates a bit. Note that we had to choose an averaging window based on eyeballing the learning curves for the separate Q-values.&lt;/p&gt;
&lt;p&gt;Can we do even better? A learning rate schedule where alpha is lowered further would diminish the fluctuations around the true values, but at the risk of lowering it too fast and effectively freezing (or very slowly evolving) the Q-values at non-equilibrium values.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;decay-learning-rate-schedule-variant-ii&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;decay learning rate schedule variant II&lt;/h1&gt;
&lt;p&gt;The second formula decays &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; to &lt;code&gt;0.001&lt;/code&gt;, ten times lower than the previous decay schedule:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# fully random policy

plot_every = 100

alpha_schedule2 = create_alpha_schedule2(num_episodes = 500_000, \
                                        alpha_decay = 0.99997)
n_episodes = len(alpha_schedule2)

fullrun = 0

if fullrun == 1:
    Q_sarsamax, N_sarsamax, avg_scores, Qlist = q_learning(env, num_episodes = n_episodes, \
                                                eps = 1,\
                                                alpha_schedule = alpha_schedule2, \
                                                gamma =  0.95, \
                                                plot_every = plot_every, \
                                                verbose = True)
    
    with open(&amp;#39;cached/as2_Q_sarsamax.pkl&amp;#39;, &amp;#39;wb&amp;#39;) as f:
        dill.dump(Q_sarsamax, f)
    with open(&amp;#39;cached/as2_avg_scores.pkl&amp;#39;, &amp;#39;wb&amp;#39;) as f:
        dill.dump(avg_scores, f)
    with open(&amp;#39;cached/as2_Qlist.pkl&amp;#39;, &amp;#39;wb&amp;#39;) as f:
        dill.dump(Qlist, f)        
else:
    with open(&amp;#39;cached/as2_Q_sarsamax.pkl&amp;#39;, &amp;#39;rb&amp;#39;) as f:
        Q_sarsamax = dill.load(f)
    with open(&amp;#39;cached/as2_avg_scores.pkl&amp;#39;, &amp;#39;rb&amp;#39;) as f:
        avg_scores = dill.load(f)
    with open(&amp;#39;cached/as2_Qlist.pkl&amp;#39;, &amp;#39;rb&amp;#39;) as f:
        Qlist = dill.load(f)     &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;dfm = list_to_df(Qlist)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;(p9.ggplot(data = dfm.loc[(dfm.episode &amp;gt; -1)], mapping = p9.aes(x = &amp;#39;episode&amp;#39;, y = &amp;#39;value&amp;#39;))
    + p9.geom_point() 
    + p9.geom_smooth(method = &amp;quot;loess&amp;quot;, color = &amp;quot;yellow&amp;quot;)
    + p9.geom_hline(data = df_true, mapping = p9.aes(yintercept = &amp;#39;value&amp;#39;), color = &amp;#39;red&amp;#39;)
    + p9.facet_wrap(&amp;quot;variable&amp;quot;, scales = &amp;#39;free_y&amp;#39;)
    + p9.theme(subplots_adjust={&amp;#39;wspace&amp;#39;:0.4})
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_70_0.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Cleary, the fluctuations are reduced compared to the previous schedule. AND all Q-values still fluctuate around their true values, so it seems that this schedule is better with respect to finding the true values.&lt;/p&gt;
&lt;p&gt;Let’s see if the accuracy of the estimated Q-values is indeed higher:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# drop burn-in, then average Q-vals by variable
Q_est = (dfm.loc[dfm.episode &amp;gt; 2000]
         .groupby([&amp;#39;variable&amp;#39;])
         .mean()
        )
# convert to 1d array sorted by state nr
Q_est[&amp;#39;sort_order&amp;#39;] = [int(str.replace(x, &amp;#39;Q&amp;#39;, &amp;#39;&amp;#39;)) \
                       for x in Q_est.index.values]

Q_est = Q_est.sort_values(by=[&amp;#39;sort_order&amp;#39;])

Q_est_as2 = Q_est[&amp;#39;value&amp;#39;].values

Q_as2_lasttable = [np.max(Q_sarsamax[key]) if key in Q_sarsamax \
             else 0 for key in np.arange(env.nS)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;q_showdown = pd.DataFrame(
    {&amp;#39;Q_as2_lasttable&amp;#39;: Q_as2_lasttable,
     &amp;#39;q_est_as2&amp;#39;: Q_est_as2,
     &amp;#39;q_true&amp;#39;: V_pi_dc})
q_showdown[&amp;#39;state&amp;#39;] = range(16)

q_showdown = pd.melt(q_showdown, [&amp;#39;state&amp;#39;])

(p9.ggplot(data = q_showdown, mapping = p9.aes(x = &amp;#39;state&amp;#39;, y = &amp;#39;value&amp;#39;, color = &amp;#39;factor(variable)&amp;#39;))
+ p9.geom_point(size = 5, shape = &amp;#39;x&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_73_0.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Boom! Now our &lt;span class=&#34;math inline&#34;&gt;\(Q^{*}_{a}\)&lt;/span&gt; estimates are really getting close to the true values. Clearly, the second learning rate schedule is able to learn the true Q-values compared to the first rate schedule, given the fixed amount of computation, in this case 500.000 episodes each.&lt;/p&gt;
&lt;p&gt;Averaging out does not do much anymore, except for states 10 and 14, where it improves the estimates a tiny bit.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;wrapping-up&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Wrapping up&lt;/h1&gt;
&lt;p&gt;In conclusion, we have seen that the common approach of using Q-learning with a constant learning rate and gradually decreasing the exploration rate, given sensible values and rates, will indeed find the optimal policy. However, this approach does not necessary converge to the true state-values. We have to tune the algorithm exactly the other way around: keep the exploration rate constant and sufficiently high, and decay the learning rate. For sufficiently low learning rates, averaging out the fluctuations does not meaningfully increase accuracy of the learned Q-values.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Jacks Car Rental as a Gym Environment</title>
      <link>/post/jacks-car-rental-gym/</link>
      <pubDate>Wed, 30 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/post/jacks-car-rental-gym/</guid>
      <description>
&lt;link href=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;This blogpost is about &lt;a href=&#34;https://en.wikipedia.org/wiki/Reinforcement_learning&#34;&gt;reinforcement learning&lt;/a&gt;, part of the Machine Learning (ML) / AI family of computer algorithms and techniques.
Reinforcement learning is all about agents taking decisions in complex environments. The decisions (&lt;strong&gt;actions&lt;/strong&gt;) take the agent from a current &lt;strong&gt;state&lt;/strong&gt; or situation, to a new &lt;strong&gt;state&lt;/strong&gt;. When the probability of ending up in a new state is only dependent on the current state and the action the agent takes in that state, we are facing a so-called &lt;strong&gt;Markov Decision Problem&lt;/strong&gt;, or &lt;strong&gt;MDP&lt;/strong&gt; for short.&lt;/p&gt;
&lt;p&gt;Back in 2016, people at OpenAI, a startup company that specializes in AI/ML, created a Python library called &lt;strong&gt;Gym&lt;/strong&gt; that provides standardized access to a range of MDP environments. Using Gym means keeping a sharp separation between the RL algorithm (“The agent”) and the environment (or task) it tries to solve / optimize / control / achieve. Gym allows us to easily benchmark RL algorithms on a range of different environments. It also allows us to more easily build on others work, and share our own work (i.e. on Github). Because when I implement something as a Gym Environment, others can then immediately apply their algorithms on it, and vice versa.&lt;/p&gt;
&lt;p&gt;In this blogpost, we solve a famous decision problem called “Jack’s Car Rental” by first turning it into a Gym environment and then use a RL algorithm called “Policy Iteration” (a form of “Dynamic Programming”) to solve for the optimal decisions to take in this environment.&lt;/p&gt;
&lt;p&gt;The Gym environment for Jack’s Car Rental is called &lt;code&gt;gym_jcr&lt;/code&gt; and can be installed from &lt;a href=&#34;https://github.com/gsverhoeven/gym_jcr&#34;&gt;https://github.com/gsverhoeven/gym_jcr&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;jacks-car-rental-problem&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Jack’s Car Rental problem&lt;/h1&gt;
&lt;p&gt;Learning Reinforcement learning (RL) as a student, means working through the famous &lt;a href=&#34;http://incompleteideas.net/book/the-book.html&#34;&gt;book on RL by Sutton and Barto&lt;/a&gt;. In chapter 4, Example 4.2 (2018 edition), Jack’s Car Rental problem is presented:&lt;/p&gt;
&lt;pre class=&#34;plaintext&#34;&gt;&lt;code&gt;Jack’s Car Rental 

Jack manages two locations for a nationwide car rental company. 
Each day, some number of customers arrive at each location to rent cars. 
If Jack has a car available, he rents it out and is credited $10 by 
the national company. If he is out of cars at that location, then the 
business is lost. Cars become available for renting the day after they 
are returned. To help ensure that cars are available where they are 
needed, Jack can move them between the two locations overnight, at a cost 
of $2 per car moved. We assume that the number of cars requested and 
returned at each location are Poisson random variables. Suppose Lambda is
3 and 4 for rental requests at the first and second locations and 
3 and 2 for returns. 

To simplify the problem slightly, we assume that there can be no more than
20 cars at each location (any additional cars are returned to the 
nationwide company, and thus disappear from the problem) and a maximum 
of five cars can be moved from one location to the other in one night. 

We take the discount rate to be gamma = 0.9 and formulate this as a 
continuing finite MDP, where the time steps are days, the state is the 
number of cars at each location at the end of the day, and the actions 
are the net numbers of cars moved between the two locations overnight.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In order to implement this MDP in Gym and solving it using DP (Dynamic Programming), we need to calculate for each state - action combination the probability of transitioning to all other states. Here a state is defined as the number of cars at the two locations A and B. Since there can be between 0 and 20 cars at each location, we have in total 21 x 21 = 441 states. We have 11 actions, moving up to five cars from A to B, moving up to five cars from B to A, or moving no cars at all. We also need the rewards &lt;strong&gt;R&lt;/strong&gt; for taking action &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; in state &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Luckily for us, Christian Herta and Patrick Baumann, as part of their project &lt;a href=&#34;https://www.deep-teaching.org/&#34;&gt;“Deep.Teaching”&lt;/a&gt;, created a Jupyter Notebook containing a well explained Python code solution for calculating &lt;strong&gt;P&lt;/strong&gt;, and &lt;strong&gt;R&lt;/strong&gt;, and published it as open source under the MIT license. I extracted their functions and put them in &lt;code&gt;jcr_mdp.py&lt;/code&gt;, containing two top level functions &lt;code&gt;create_P_matrix()&lt;/code&gt; and &lt;code&gt;create_R_matrix()&lt;/code&gt;, these are used when the Gym environment is initialized.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;jackscarrentalenv&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;JacksCarRentalEnv&lt;/h1&gt;
&lt;p&gt;My approach to creating the Gym environment for Jack’s Car Rental was to take the &lt;a href=&#34;https://gym.openai.com/envs/FrozenLake-v0/&#34;&gt;Frozen Lake Gym environment&lt;/a&gt;, and rework it to become JacksCarRentalEnv. I chose this environment because it has a similar structure as JCR, having discrete states and discrete actions. In addition, it is one of the few environments that create and expose the complete transition matrix &lt;strong&gt;P&lt;/strong&gt; needed for the DP algorithm.&lt;/p&gt;
&lt;p&gt;There is actually not much to it at this point, as most functionality is provided by the &lt;code&gt;DiscreteEnv&lt;/code&gt; class that our environment builds on. We need only to specify four objects:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;nS: number of states&lt;/li&gt;
&lt;li&gt;nA: number of actions&lt;/li&gt;
&lt;li&gt;P: transitions&lt;/li&gt;
&lt;li&gt;isd: initial state distribution (list or array of length nS)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;nS&lt;/code&gt; and &lt;code&gt;nA&lt;/code&gt; were already discussed above, there are 441 and 11 respectively.
For the &lt;code&gt;isd&lt;/code&gt; we simply choose an equal probability to start in any of the 441 states.&lt;/p&gt;
&lt;p&gt;This leaves us with the transitions &lt;strong&gt;P&lt;/strong&gt;. This needs to be in a particular format, a &lt;code&gt;dictionary dict of dicts of lists, where P[s][a] == [(probability, nextstate, reward, done), ...]&lt;/code&gt; according to the help of this class. So we take the &lt;strong&gt;P&lt;/strong&gt; and &lt;strong&gt;R&lt;/strong&gt; arrays created by the python code in &lt;code&gt;jcr_mdp.py&lt;/code&gt; and use these to fill the dictionary in the proper way (drawing inspiration from the Frozen Lake &lt;strong&gt;P&lt;/strong&gt; object :)).&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;P = {s : {a : [] for a in range(nA)} for s in range(nS)}

# prob, next_state, reward, done
for s in range(nS):
    # need a state vec to extract correct probs from Ptrans
    state_vec = np.zeros(nS)
    state_vec[s] = 1
    for a in range(nA):
        prob_vec = np.dot(Ptrans[:,:,a], state_vec)
        li = P[s][a]
        # add rewards for all transitions
        for ns in range(nS):
            li.append((prob_vec[ns], ns, R[s][a], False))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And were done! Let’s try it out.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import matplotlib.pyplot as plt
import numpy as np
import pickle

# Gym environment
import gym
import gym_jcr
# RL algorithm
from dp import *&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# n.b. can take up to 15 s
env = gym.make(&amp;quot;JacksCarRentalEnv-v0&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So what we have?&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# print the state space and action space
print(env.observation_space)
print(env.action_space)

# print the total number of states and actions
print(env.nS)
print(env.nA)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Discrete(441)
Discrete(11)
441
11&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let us check for state &lt;code&gt;s= 0&lt;/code&gt;, for each action &lt;code&gt;a&lt;/code&gt;, if the probabilities of transitioning to a new state &lt;code&gt;new_state&lt;/code&gt; sum to one (we need to end up somewhere right?).&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# from state 0, for each action the probs for going to new state
s = 0

for a in range(env.nA):
    prob = 0.0
    for new_state in range(env.nS):
        prob += env.P[s][a][new_state][0]
    print(prob, end = &amp;#39; &amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.9999999999999992 0.9999999999999992 0.9999999999999992 0.9999999999999992 0.9999999999999992 0.9999999999999992 0.9999999999999992 0.9999999999999992 0.9999999999999992 0.9999999999999992 0.9999999999999992 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Close enough. Let’s run our Dynamic Programming algorithm on it!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;policy-iteration-on-jcr&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Policy iteration on JCR&lt;/h1&gt;
&lt;p&gt;The &lt;code&gt;policy_iteration()&lt;/code&gt; function used below is from &lt;a href=&#34;https://github.com/gsverhoeven/hugo_source/blob/master/content/post/dp.py&#34;&gt;dp.py&lt;/a&gt;. This exact same code was used in a Jupyter tutorial notebook to solve the &lt;a href=&#34;https://gym.openai.com/envs/FrozenLake-v0/&#34;&gt;Frozen-Lake Gym environment&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We reproduce the results from the Sutton &amp;amp; Barto book (p81), where the algorithm converges after four iterations. This takes about 30 min on my computer.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fullrun = False

if fullrun == True:
    policy, V = policy_iteration(env, gamma = 0.9)
    with open(&amp;#39;policy.bin&amp;#39;, &amp;#39;wb&amp;#39;) as f:
        pickle.dump(policy, f)
    with open(&amp;#39;values.bin&amp;#39;, &amp;#39;wb&amp;#39;) as f:
        pickle.dump(V, f)
else:
    with open(&amp;#39;policy.bin&amp;#39;, &amp;#39;rb&amp;#39;) as f:
        policy = pickle.load(f)
    with open(&amp;#39;values.bin&amp;#39;, &amp;#39;rb&amp;#39;) as f:
        V = pickle.load(f)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-optimal-policy-as-a-contour-map&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Plot optimal policy as a contour map&lt;/h1&gt;
&lt;p&gt;For easy plotting, we need to transform the policy from a 2d state-action matrix to a 2d state-A, state-B matrix with the action values in the cells.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;MAX_CARS = 20

def get_state_vector(a, b):
    s = np.zeros((MAX_CARS+1)**2)
    s[a*(MAX_CARS+1)+b] = 1
    return s

policy_map = np.zeros([MAX_CARS+1, MAX_CARS+1])

for a in range(MAX_CARS+1):
    for b in range(MAX_CARS+1):
        state = get_state_vector(a, b)
        s = state.argmax()
        policy_map[a, b] = np.argmax(policy[s,:]) - 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We visualize the optimal policy as a 2d heatmap using &lt;code&gt;matplotlib.pyplot.imshow()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;plt.figure(figsize=(7,6))
hmap = plt.imshow(policy_map, cmap=&amp;#39;viridis&amp;#39;, origin=&amp;#39;lower&amp;#39;)
cbar = plt.colorbar(hmap)
cbar.ax.set_ylabel(&amp;#39;actions&amp;#39;)
plt.title(&amp;#39;Policy&amp;#39;)
plt.xlabel(&amp;quot;cars at B&amp;quot;)
plt.ylabel(&amp;quot;cars at A&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/post/2020-12-30-jacks_car_rental_gym_files/2020-12-30-jacks_car_rental_gym_13_1.png&#34; alt=&#34;Optimal policy for all states of Jack’s Car Rental&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Optimal policy for all states of Jack’s Car Rental&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion-and-outlook&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion and outlook&lt;/h1&gt;
&lt;p&gt;Conclusion: yes we can turn JCR into a Gym environment and solve it using the exact same (policy iteration) code that I had earlier used to solve the Frozen-Lake Gym environment!&lt;/p&gt;
&lt;p&gt;So now what? One obvious area of improvement is speed: It takes too long to load the environment. Also the DP algorithm is slow, because it uses for loops instead of matrix operations.&lt;/p&gt;
&lt;p&gt;Another thing is that currently the rewards that the environment returns are &lt;strong&gt;average expected rewards&lt;/strong&gt; that are received when taking action &lt;em&gt;a&lt;/em&gt; in state &lt;em&gt;s&lt;/em&gt; . However, they do not match the actual amount of cars rented when transitioning from a particular state &lt;em&gt;s&lt;/em&gt; to a new state &lt;em&gt;s’&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Finally, adding the modifications to the problem from Exercise 4.7 in Sutton &amp;amp; Barto could also be implemented, but this complicates the calculation of &lt;strong&gt;P&lt;/strong&gt; and &lt;strong&gt;R&lt;/strong&gt; even further.
For me, this is the real takeaway from this exercise: it is really hard to (correctly) compute the complete set of transition probabilities and rewards for an MDP, but it is much easier if we just need to simulate single transitions according to the MDP specification. Wikipedia has a nice paragraph on it under &lt;a href=&#34;https://en.wikipedia.org/wiki/Markov_decision_process#Simulator_models&#34;&gt;simulator models for MDPs&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
