<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>cronbach&#39;s alpha, validity | Gertjan Verhoeven</title>
    <link>/tags/cronbachs-alpha-validity/</link>
      <atom:link href="/tags/cronbachs-alpha-validity/index.xml" rel="self" type="application/rss+xml" />
    <description>cronbach&#39;s alpha, validity</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2019, 2020</copyright><lastBuildDate>Thu, 20 Jun 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>cronbach&#39;s alpha, validity</title>
      <link>/tags/cronbachs-alpha-validity/</link>
    </image>
    
    <item>
      <title>Cronbach&#39;s alpha and its models</title>
      <link>/post/cronbach-alpha-measurement-theory/</link>
      <pubDate>Thu, 20 Jun 2019 00:00:00 +0000</pubDate>
      <guid>/post/cronbach-alpha-measurement-theory/</guid>
      <description>


&lt;div id=&#34;summary&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Summary&lt;/h1&gt;
&lt;p&gt;Working for the first time with questionnaire data, I felt i needed some basic understanding of measurement theory, and the various approaches regarding reliability and validity of questionnaires.&lt;/p&gt;
&lt;p&gt;Cronbach’s alpha was something I already associated with the social sciences before I knew any stats.
So I was curious to find out what it is, and how it can be useful.&lt;/p&gt;
&lt;p&gt;As always, we use simulated data. Fortunately, the &lt;code&gt;psych&lt;/code&gt; package by Hugh Revelle contains functions to simulate data with the appropriate structure.&lt;/p&gt;
&lt;p&gt;Before getting busy, let us explain the statistical models that underlie Cronbach ’s alpha.
In this I follow Cho (2016). This study proves that various reliability coefficients are generated from measurement models nested within the bi-factor measurement model.&lt;/p&gt;
&lt;p&gt;Unfortunately, with rare exceptions, we normally are faced with just one test, not two, three or four. How then to estimate the reliability of that one test?&lt;/p&gt;
&lt;p&gt;It has been proposed that α {} can be viewed as the expected correlation of two tests that measure the same construct. By using this definition, it is implicitly assumed that the average correlation of a set of items is an accurate estimate of the average correlation of all items that pertain to a certain construct.&lt;/p&gt;
&lt;p&gt;The term item is used throughout this article, but items could be anything—questions, raters, indicators- for all of which, one might ask, to what extent they “measure the same thing.” Items that are manipulated are commonly referred to as variables.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;revelle&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Revelle:&lt;/h1&gt;
&lt;p&gt;Although defined in terms of the correlation of a test with a test just like it, reliability can be estimated by the characteristics of the items &lt;em&gt;within the test&lt;/em&gt;. The desire for an easy to use “magic bullet” based upon the &lt;em&gt;domain sampling model&lt;/em&gt; has led to a number of solutions for estimating the reliability of a test based upon characteristics of the covariances of the items. All of these estimates are based upon &lt;em&gt;classical test theory&lt;/em&gt; and assume that the covariances between items represents true covariance, but that the variances of the items reflect an unknown sum of true and unique variance.&lt;/p&gt;
&lt;p&gt;A &lt;em&gt;tau-equivalent&lt;/em&gt; measurement model is a special case of a &lt;em&gt;congeneric&lt;/em&gt; measurement model, hereby assuming all factor loadings to be the same.&lt;/p&gt;
&lt;p&gt;The most important difference between CTT and IRT is that in CTT, one uses a common estimate of the measurement precision that is assumed to be equal for all individuals irrespective of their attribute levels. In IRT, however, the measurement precision depends on the latent-attribute value. This can result in differences between CTT and IRT with respect to their conclusions about statistical significance of change.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cronbach-s-alpha-and-its-measurement-models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Cronbach ’s alpha and its measurement models&lt;/h1&gt;
&lt;p&gt;Cronbach’s alpha is meant for single latent variables / factors / traits.&lt;/p&gt;
&lt;p&gt;Classical Test Theory (CTT) considers four or more tests to be congenerically equivalent if all tests may be expressed in terms of one factor and a residual error. (N.b. we need at least four tests to identify all free parameters of the model).&lt;/p&gt;
&lt;p&gt;To determine the scale, the variance of the latent variable is set to 1.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Congeneric tests may differ in both factor loading and error variances.
The congeneric model does not have additional constraints.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The tau-equivalent model is the same as the congeneric model, only with the constraint that all the factor loadings are equal, but allows the error variances to vary from item to item.
Tau equivalent tests have equal factor loadings but may have unequal errors.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example, an &lt;em&gt;essentially tau-equivalent model&lt;/em&gt; includes a constant, whereas a &lt;em&gt;strictly tau-equivalent model&lt;/em&gt; does not. Although the addition of a constant has an effect on the mean, it does not affect the
variances, covariances or the value of reliability.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The (unidimensional) parallel model is the tau-equivalent model with the constraint that the error variances are all equal. Parallel tests are the special case where (usually two) tests have equal factor loadings.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;step-one-simulate-data-for-a-congeneric-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Step one: simulate data for a congeneric model&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(psych)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;psych&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:ggplot2&amp;#39;:
## 
##     %+%, alpha&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(42) #keep the same starting values
#four congeneric measures

loadings &amp;lt;- rep(1, 4)
loadings &amp;lt;- c(0.8, 0.7, 0.6, 0.5)

errors &amp;lt;- c(0.8, 0.7, 0.6, 0.5)

errors &amp;lt;- sqrt(1 - loadings^2)
errors &amp;lt;- rep(0.1, 4)

r4 &amp;lt;- psych::sim.congeneric(loads = loadings,
                            err = errors, #A vector of error variances 
                            low = -3, # values less than low are forced to low (when cat = T)
                            high = 3, # values greater than high are forced to high
                            short = FALSE, 
                            categorical = FALSE,
                            N = 10000)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have less noise on the item with the lower factor loadings.
&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is the (common) latent variable.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-two-describe-simulated-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Step two: describe simulated data&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my.df &amp;lt;- data.frame(r4$latent, r4$observed)

# Check factor loadings
fit1 &amp;lt;- lm(V1 ~ theta, data = my.df)
fit2 &amp;lt;- lm(V2 ~ theta, data = my.df)
fit3 &amp;lt;- lm(V3 ~ theta, data = my.df)
fit4 &amp;lt;- lm(V4 ~ theta, data = my.df)

sd(fit1$residuals)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1007705&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd(fit2$residuals)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1013989&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd(fit3$residuals)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1008644&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd(fit4$residuals)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.09897803&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;errors &amp;lt;- my.df[, colnames(my.df) %in% c(&amp;quot;e1&amp;quot;, &amp;quot;e2&amp;quot;, &amp;quot;e3&amp;quot;, &amp;quot;e4&amp;quot;)]

cor(errors)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              e1           e2           e3           e4
## e1  1.000000000  0.010432462  0.002030510 -0.006050927
## e2  0.010432462  1.000000000 -0.008618315 -0.027381449
## e3  0.002030510 -0.008618315  1.000000000 -0.005340683
## e4 -0.006050927 -0.027381449 -0.005340683  1.000000000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# errors are uncorrelated with each other
cov(errors)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              e1          e2           e3           e4
## e1  1.015522554  0.01066020  0.002063898 -0.006035393
## e2  0.010660197  1.02817565 -0.008814430 -0.027480772
## e3  0.002063898 -0.00881443  1.017364269 -0.005331801
## e4 -0.006035393 -0.02748077 -0.005331801  0.979665221&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;var(my.df$e1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.015523&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;var(my.df$theta)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.012307&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;step-three-calculate-cronbachs-alpha&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Step three: Calculate Cronbach’s Alpha&lt;/h1&gt;
&lt;p&gt;Ok, let’s have it. The cronbach alpha.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# r4 population correlation matrix
psych::alpha(r4$observed)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Reliability analysis   
## Call: psych::alpha(x = r4$observed)
## 
##   raw_alpha std.alpha G6(smc) average_r S/N     ase    mean   sd median_r
##       0.98      0.99    0.99      0.97 154 0.00013 -0.0074 0.66     0.97
## 
##  lower alpha upper     95% confidence boundaries
## 0.98 0.98 0.98 
## 
##  Reliability if an item is dropped:
##    raw_alpha std.alpha G6(smc) average_r S/N alpha se   var.r med.r
## V1      0.98      0.99    0.99      0.97 102  0.00021 1.9e-05  0.97
## V2      0.97      0.99    0.99      0.97 109  0.00023 3.1e-05  0.97
## V3      0.98      0.99    0.99      0.98 119  0.00021 3.8e-05  0.97
## V4      0.99      0.99    0.99      0.98 140  0.00015 9.8e-06  0.98
## 
##  Item statistics 
##        n raw.r std.r r.cor r.drop    mean   sd
## V1 10000  0.99  0.99  0.99   0.99 -0.0090 0.81
## V2 10000  0.99  0.99  0.99   0.99 -0.0071 0.71
## V3 10000  0.99  0.99  0.99   0.98 -0.0070 0.61
## V4 10000  0.99  0.99  0.98   0.98 -0.0066 0.51&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check with ground truth:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# calculate total score
my.df$sumV &amp;lt;- with(my.df, V1+V2+V3+V4)

my.df$sumE &amp;lt;- with(my.df, e1+e2+e3+e4)

plot(my.df$sumV, my.df$theta)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-01-20-cronbach_alpha_blog_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# raw alpha
(1-sum(diag(cov(r4$observed)))/var(my.df$sumV))*4/3&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.984329&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cov(r4$observed)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           V1        V2        V3        V4
## V1 0.6568506 0.5665898 0.4855468 0.4045279
## V2 0.5665898 0.5065013 0.4252159 0.3541304
## V3 0.4855468 0.4252159 0.3746968 0.3037033
## V4 0.4045279 0.3541304 0.3037033 0.2629166&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;and-why-does-this-measure-reliability-of-a-test&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;And why does this measure “reliability” of a test?&lt;/h1&gt;
&lt;p&gt;The resulting α coefficient of reliability ranges from 0 to 1 in providing this overall assessment of a measure’s reliability. If all of the scale items are entirely independent from one another (i.e., are not correlated or share no covariance), then α = 0; and, if all of the items have high covariances, then α will approach 1 as the number of items in the scale approaches infinity.&lt;/p&gt;
&lt;p&gt;Cronbach’s α {} assumes that all factor loadings are equal. In reality this is rarely the case, and hence it systematically underestimates the reliability. An alternative to Cronbach’s α {} that does not rely on this assumption is congeneric reliability ( ρ C {&lt;em&gt;{C}} &lt;/em&gt;{C})&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;does-it-measure-unidimensionality&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Does it measure unidimensionality?&lt;/h1&gt;
&lt;p&gt;Let ’s simulate data that has TWO factors, that are fully independent (uncorrelated).
So that’s two independent (orthogonal) factors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# examples of two independent factors that produce reasonable alphas
#this is a case where alpha is a poor indicator of unidimensionality
set.seed(123)
two.f &amp;lt;- data.frame(sim.item(nvar = 8,
                             nsub = 10000))

with(two.f, plot(V1, V3))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-01-20-cronbach_alpha_blog_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#sim.item(nvar = 72, nsub = 500, circum = FALSE, xloading = 0.6, yloading = 0.6, 
# gloading = 0, xbias = 0, ybias = 0, categorical = FALSE, low = -3, high = 3, 
# truncate = FALSE, cutpoint = 0)

#specify which items to reverse key by name
alpha(two.f, keys = c(&amp;quot;V1&amp;quot;,&amp;quot;V2&amp;quot;,&amp;quot;V7&amp;quot;,&amp;quot;V8&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Reliability analysis   
## Call: alpha(x = two.f, keys = c(&amp;quot;V1&amp;quot;, &amp;quot;V2&amp;quot;, &amp;quot;V7&amp;quot;, &amp;quot;V8&amp;quot;))
## 
##   raw_alpha std.alpha G6(smc) average_r S/N    ase  mean   sd median_r
##        0.6       0.6    0.63      0.16 1.5 0.0062 0.053 0.51    0.017
## 
##  lower alpha upper     95% confidence boundaries
## 0.58 0.6 0.61 
## 
##  Reliability if an item is dropped:
##     raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r
## V1-      0.56      0.56    0.59      0.15 1.3   0.0068 0.032 0.019
## V2-      0.57      0.57    0.59      0.16 1.3   0.0068 0.031 0.019
## V3       0.56      0.56    0.59      0.16 1.3   0.0068 0.032 0.019
## V4       0.56      0.56    0.59      0.16 1.3   0.0068 0.033 0.014
## V5       0.56      0.56    0.59      0.15 1.3   0.0068 0.033 0.013
## V6       0.56      0.56    0.59      0.16 1.3   0.0068 0.032 0.019
## V7-      0.56      0.56    0.59      0.15 1.3   0.0068 0.033 0.014
## V8-      0.57      0.57    0.60      0.16 1.3   0.0067 0.032 0.019
## 
##  Item statistics 
##         n raw.r std.r r.cor r.drop    mean   sd
## V1- 10000  0.52  0.52  0.42   0.31  0.1112 1.00
## V2- 10000  0.51  0.50  0.40   0.29  0.0949 1.00
## V3  10000  0.51  0.51  0.40   0.29  0.0048 1.01
## V4  10000  0.51  0.51  0.40   0.30 -0.0029 0.99
## V5  10000  0.52  0.52  0.41   0.30  0.0104 1.00
## V6  10000  0.51  0.51  0.41   0.30  0.0143 1.00
## V7- 10000  0.51  0.52  0.41   0.30  0.1067 0.99
## V8- 10000  0.50  0.50  0.38   0.28  0.0837 1.01&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Cho, E (2016). “Making Reliability Reliable”. Organizational Research Methods. 19 (4): 651–682
&lt;a href=&#34;https://rameliaz.github.io/mg-sem-workshop/cho2016.pdf&#34; class=&#34;uri&#34;&gt;https://rameliaz.github.io/mg-sem-workshop/cho2016.pdf&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Revelle, W The Personality Project: &lt;a href=&#34;https://www.personality-project.org/r/book/Chapter7.pdf&#34; class=&#34;uri&#34;&gt;https://www.personality-project.org/r/book/Chapter7.pdf&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Measurement theories in psychometrics</title>
      <link>/post/cronbach-alpha-measurement-theory/</link>
      <pubDate>Thu, 20 Jun 2019 00:00:00 +0000</pubDate>
      <guid>/post/cronbach-alpha-measurement-theory/</guid>
      <description>


&lt;!-- https://www.personality-project.org/r/book/Chapter7.pdf --&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Summary&lt;/h1&gt;
&lt;p&gt;Working for the first time with questionnaire data, I felt i needed some basic understanding of measurement theory, and the various approaches regarding reliability and validity of questionnaires.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;measurement-theories&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Measurement Theories&lt;/h1&gt;
&lt;p&gt;Measurement theories concern themselves with the question: What does it mean to “measure” something? What types of measurement exist? And what statistical analysises are appropropriate for different types of measurement?&lt;/p&gt;
&lt;p&gt;According to a recent paper by Matt Williams, at least four measurement theories exist.
There also exists a book by &lt;strong&gt;Denny Borsboom&lt;/strong&gt; and &lt;strong&gt;Keith Markus&lt;/strong&gt;, called “Frontiers of Test Validity Theory: Measurement, Causation, and Meaning”, which contains an extended review of the various measurement theories, and their implications for pyschological measurement. Another book on the same topic is “Measuring the mind” by &lt;strong&gt;Denny Borsboom&lt;/strong&gt;. Borsboom has been influenced by &lt;strong&gt;Joel Michell&lt;/strong&gt;, who wrote a book in 1995 on the topic, and was reviewed by my hero Cosma Shalizi! &lt;a href=&#34;http://bactra.org/reviews/michell-measurement.html&#34; class=&#34;uri&#34;&gt;http://bactra.org/reviews/michell-measurement.html&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Classical test theory (Operationalism, is operationalist, prescribes many items to reduce random error)&lt;/li&gt;
&lt;li&gt;Latent variable theory (where do the scores come from, hypothesis, substantive theory that can be falsified, realist theory of science)&lt;/li&gt;
&lt;li&gt;Representationalism (constructivist / restricted, “the scale” )&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;measuring-the-mind&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Measuring the mind&lt;/h1&gt;
&lt;p&gt;Difference between measurement theories and scientific theories.
Scientific theory we can use experiments to make progress.
Measurement theory we only have “plausible” of “absurd” consequences.
Conceptual, theory of science.
CTT defective philosophical foundations.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;representationalism&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Representationalism&lt;/h1&gt;
&lt;p&gt;Representationalism is associated with the analysis of Stevens (1946), that resulted in the famous taxonomy of measurement &lt;em&gt;scales&lt;/em&gt; (nominal, ordinal, interval, and ratio), and from these scales, what type of statistical analysis is appropriate.&lt;/p&gt;
&lt;p&gt;Williams explains &lt;em&gt;Representationalism&lt;/em&gt; with an example, using a well-known scale of &lt;em&gt;Rock hardness&lt;/em&gt;, the Mohs scale (&lt;a href=&#34;https://en.wikipedia.org/wiki/Mohs_scale_of_mineral_hardness&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Mohs_scale_of_mineral_hardness&lt;/a&gt;).
Rocks exist, and we can measure a relationship between them, i.e. whether rocks can scratch each other.
If a piece of unknown rock can be scratched by diamond, but not by Quartz, it’s hardness must be somewhere inbetween. It is thus a purely ordinal scale.&lt;/p&gt;
&lt;p&gt;So what statistical analyses are appropriate on ordinal data? Stevens reasons that we can change the numbers on a purely ordinal scale without changing the order. We could transform a set of numbers {1,2,3,4} into {1, 5, 20, 396} and the order would still hold. However, some statistical procedures give different answers (i.e. a Pearson correlation), and therefore are not appropriate for an ordinal scale. Another example of an ordinal scale is the Likert scale.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Operationalism&lt;/em&gt; avoids assumptions by defining what we have measured by the operation we performed to measure it. So if we use a particular psychometric test, say the “Revised NEO Personality Inventory” (NEO-PI-R), to measure personality of a subject, if we subsequently talk about the subject’s personality, we mean nothing more than the score as measured with the NEO-PI-R.&lt;/p&gt;
&lt;p&gt;This has several consequences that make this measurement theory not very useful. It means that we cannot compare tests, even if they purport to measure the same thing. There is also no concept of measurement error, because there is no underlying “true attribute” to measure with error.&lt;/p&gt;
&lt;p&gt;Then we have the &lt;em&gt;classical theory of measurement&lt;/em&gt;. The classical theory of measurement should not be confused with classical test theory (a.k.a. true score theory). CTM is about concatenation, and requires a realist stance on the existence of attributes. This measurement theory also brings us trouble for psychometric tests, because it follows that there must exist some “unit” of personality, that allows us to compare “levels” of personality between subjects.&lt;/p&gt;
&lt;p&gt;From Wikipedia:
&amp;gt;Psychological attributes, like temperature, are considered to be intensive as no way of concatenating such
&amp;gt;attributes has been found. But this is not to say that such attributes are not quantifiable. The theory of
&amp;gt;conjoint measurement provides a theoretical means of doing this.&lt;/p&gt;
&lt;p&gt;PM apgar score.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;latent-variable-theory&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Latent variable theory&lt;/h1&gt;
&lt;p&gt;This leaves us with &lt;em&gt;latent variable theory&lt;/em&gt;. It appears that this is what all current/modern/standard psychometrics is about. For example, the starting sentence on measurement on the “personality-project.org” website by Prof. William Revelle states:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;All scientific theories require measurement of the constructs underlying the field.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;My impression: Both Classical Test Theory and Item response theory fall under the umbrella of latent variable theory. They vary in the amount and type of assumptions made.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;Measurement theory is part of the Philosophy of science.
We therefore cannot set out to “falsify” a measurement theory using some clever experiment.
Borsboom writes that it is still usefull to think about these things, because some measurement theories will result in inplausible conclusions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Williams, M 2019 Scales of measurement and statistical analyses, PsyArxiv.&lt;/li&gt;
&lt;li&gt;Denny Borsboom &amp;amp; Keith Markus, Frontiers of Test Validity Theory: Measurement, Causation, and Meaning&lt;/li&gt;
&lt;li&gt;Denny Borsboom 2005, Measuring the mind&amp;quot;&lt;/li&gt;
&lt;li&gt;Joel Michell a lot of papers&lt;/li&gt;
&lt;li&gt;Cosma Shalizi! &lt;a href=&#34;http://bactra.org/reviews/michell-measurement.html&#34; class=&#34;uri&#34;&gt;http://bactra.org/reviews/michell-measurement.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Open source psychometrics: The IPIP Big Five questionnaire</title>
      <link>/post/cronbach-alpha-measurement-theory/</link>
      <pubDate>Thu, 20 Jun 2019 00:00:00 +0000</pubDate>
      <guid>/post/cronbach-alpha-measurement-theory/</guid>
      <description>


&lt;!-- https://www.personality-project.org/r/book/Chapter7.pdf --&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Summary&lt;/h1&gt;
&lt;p&gt;Working for the first time with questionnaire data, I felt i needed some basic understanding of measurement theory, and the various approaches regarding reliability and validity of questionnaires.&lt;/p&gt;
&lt;p&gt;The big five personality traits are the best accepted and most commonly used model of personality in academic psychology. If you take a college course in personality psychology, this is what you will learn about.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;measurement-theories&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Measurement Theories&lt;/h1&gt;
&lt;p&gt;Measurement theories concern themselves with the question: What does it mean to “measure” something? What types of measurement exist? And what statistical analysises are appropropriate for different types of measurement? According to a recent paper by Matt Williams, at least four measurement theories exist.
There also exists a book by &lt;strong&gt;Denny Borsboom&lt;/strong&gt; and &lt;strong&gt;Keith Markus&lt;/strong&gt;, called “Frontiers of Test Validity Theory: Measurement, Causation, and Meaning”, which contains an extended review of the various measurement theories, and their implications for pyschological measurement. Another book on the same topic is “Measuring the mind” by &lt;strong&gt;Denny Borsboom&lt;/strong&gt;. Borsboom has been influenced by &lt;strong&gt;Joel Michell&lt;/strong&gt;, who wrote a book in 1995 on the topic, and was reviewed by my hero Cosma Shalizi! &lt;a href=&#34;http://bactra.org/reviews/michell-measurement.html&#34; class=&#34;uri&#34;&gt;http://bactra.org/reviews/michell-measurement.html&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Classical test theory (Operationalism, is operationalist, prescribes many items to reduce random error)&lt;/li&gt;
&lt;li&gt;Latent variable theory (where do the scores come from, hypothesis, substantive theory that can be falsified, realist theory of science)&lt;/li&gt;
&lt;li&gt;Representationalism (constructivist / restricted, “the scale” )&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;measuring-the-mind&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Measuring the mind&lt;/h1&gt;
&lt;p&gt;Difference between measurement theories and scientific theories.
Scientific theory we can use experiments to make progress.
Measurement theory we only have “plausible” of “absurd” consequences.
Conceptual, theory of science.
CTT defective philosophical foundations.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;representationalism&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Representationalism&lt;/h1&gt;
&lt;p&gt;Representationalism is associated with the analysis of Stevens (1946), that resulted in the famous taxonomy of measurement &lt;em&gt;scales&lt;/em&gt; (nominal, ordinal, interval, and ratio), and from these scales, what type of statistical analysis is appropriate.&lt;/p&gt;
&lt;p&gt;Williams explains &lt;em&gt;Representationalism&lt;/em&gt; with an example, using a well-known scale of &lt;em&gt;Rock hardness&lt;/em&gt;, the Mohs scale (&lt;a href=&#34;https://en.wikipedia.org/wiki/Mohs_scale_of_mineral_hardness&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Mohs_scale_of_mineral_hardness&lt;/a&gt;).
Rocks exist, and we can measure a relationship between them, i.e. whether rocks can scratch each other.
If a piece of unknown rock can be scratched by diamond, but not by Quartz, it’s hardness must be somewhere inbetween. It is thus a purely ordinal scale.&lt;/p&gt;
&lt;p&gt;So what statistical analyses are appropriate on ordinal data? Stevens reasons that we can change the numbers on a purely ordinal scale without changing the order. We could transform a set of numbers {1,2,3,4} into {1, 5, 20, 396} and the order would still hold. However, some statistical procedures give different answers (i.e. a Pearson correlation), and therefore are not appropriate for an ordinal scale. Another example of an ordinal scale is the Likert scale.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Operationalism&lt;/em&gt; avoids assumptions by defining what we have measured by the operation we performed to measure it. So if we use a particular psychometric test, say the “Revised NEO Personality Inventory” (NEO-PI-R), to measure personality of a subject, if we subsequently talk about the subject’s personality, we mean nothing more than the score as measured with the NEO-PI-R.&lt;/p&gt;
&lt;p&gt;This has several consequences that make this measurement theory not very useful. It means that we cannot compare tests, even if they purport to measure the same thing. There is also no concept of measurement error, because there is no underlying “true attribute” to measure with error.&lt;/p&gt;
&lt;p&gt;Then we have the &lt;em&gt;classical theory of measurement&lt;/em&gt;. The classical theory of measurement should not be confused with classical test theory (a.k.a. true score theory). CTM is about concatenation, and requires a realist stance on the existence of attributes. This measurement theory also brings us trouble for psychometric tests, because it follows that there must exist some “unit” of personality, that allows us to compare “levels” of personality between subjects.&lt;/p&gt;
&lt;p&gt;From Wikipedia:
&amp;gt;Psychological attributes, like temperature, are considered to be intensive as no way of concatenating such
&amp;gt;attributes has been found. But this is not to say that such attributes are not quantifiable. The theory of
&amp;gt;conjoint measurement provides a theoretical means of doing this.&lt;/p&gt;
&lt;p&gt;PM apgar score.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;latent-variable-theory&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Latent variable theory&lt;/h1&gt;
&lt;p&gt;This leaves us with &lt;em&gt;latent variable theory&lt;/em&gt;. It appears that this is what all current/modern/standard psychometrics is about. For example, the starting sentence on measurement on the “personality-project.org” website by Prof. William Revelle states:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;All scientific theories require measurement of the constructs underlying the field.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;My impression: Both Classical Test Theory and Item response theory fall under the umbrella of latent variable theory. They vary in the amount and type of assumptions made.&lt;/p&gt;
&lt;p&gt;Unfortunately, with rare exceptions, we normally are faced with just one test, not two, three or four. How then to estimate the reliability of that one test?&lt;/p&gt;
&lt;p&gt;It has been proposed that α {} can be viewed as the expected correlation of two tests that measure the same construct. By using this definition, it is implicitly assumed that the average correlation of a set of items is an accurate estimate of the average correlation of all items that pertain to a certain construct.&lt;/p&gt;
&lt;p&gt;The term item is used throughout this article, but items could be anything—questions, raters, indicators- for all of which, one might ask, to what extent they “measure the same thing.” Items that are manipulated are commonly referred to as variables.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;revelle&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Revelle:&lt;/h1&gt;
&lt;p&gt;Although defined in terms of the correlation of a test with a test just like it, reliability can be estimated by the characteristics of the items &lt;em&gt;within the test&lt;/em&gt;. The desire for an easy to use “magic bullet” based upon the &lt;em&gt;domain sampling model&lt;/em&gt; has led to a number of solutions for estimating the reliability of a test based upon characteristics of the covariances of the items. All of these estimates are based upon &lt;em&gt;classical test theory&lt;/em&gt; and assume that the covariances between items represents true covariance, but that the variances of the items reflect an unknown sum of true and unique variance.&lt;/p&gt;
&lt;p&gt;A &lt;em&gt;tau-equivalent&lt;/em&gt; measurement model is a special case of a &lt;em&gt;congeneric&lt;/em&gt; measurement model, hereby assuming all factor loadings to be the same.&lt;/p&gt;
&lt;p&gt;The most important difference between CTT and IRT is that in CTT, one uses a common estimate of the measurement precision that is assumed to be equal for all individuals irrespective of their attribute levels. In IRT, however, the measurement precision depends on the latent-attribute value. This can result in differences between CTT and IRT with respect to their conclusions about statistical significance of change.&lt;/p&gt;
&lt;p&gt;Cho 2016: This study proves that various reliability coefficients are generated from measurement
models nested within the bi-factor measurement model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;measurement-models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Measurement models&lt;/h1&gt;
&lt;p&gt;Before proceeding with the discussion, let us explain the measurement models used in this study
starting from unidimensional models (i.e. models that have a single latent variable).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;unidimensional parallel&lt;/li&gt;
&lt;li&gt;tau-equivalent&lt;/li&gt;
&lt;li&gt;congeneric&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggraph)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggplot2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidygraph)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;tidygraph&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:stats&amp;#39;:
## 
##     filter&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;param_nodes &amp;lt;- data.frame(name = c(&amp;quot;Item 1&amp;quot;, &amp;quot;Item 2&amp;quot;, &amp;quot;Item 3&amp;quot;, &amp;quot;Intelligence&amp;quot;))

param_nodes$latent &amp;lt;- c(F, F, F, T)

param_nodes$x &amp;lt;- c(0, 0, 0, 1)

param_nodes$y &amp;lt;- c(0, 1, 2, 1)

param_nodes$hjust &amp;lt;- c(1, 1, 1, 0)

param_edges &amp;lt;- data.frame(from = c( 4, 4, 4), 
                            to = c( 1, 2, 3))

param_graph &amp;lt;- tidygraph::tbl_graph(param_nodes, param_edges)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggraph(param_graph, 
       layout = &amp;quot;manual&amp;quot;, x = param_nodes$x, y = param_nodes$y) +
    # Observed Nodes
    geom_node_point(aes(filter = !latent),
                    shape = 0, 
                    size = 3, 
                    col = &amp;quot;black&amp;quot;) +
    geom_node_point(aes(filter = latent), 
                    shape = 1, 
                    size = 4, 
                    color = &amp;quot;black&amp;quot;) +
    # Regression Paths (and text)
    geom_edge_link(alpha = 1,
                   linetype = 1, 
                   angle_calc = &amp;quot;along&amp;quot;,
                   start_cap = circle(1.5, &amp;#39;mm&amp;#39;),
                   end_cap = circle(1, &amp;#39;mm&amp;#39;),
                   arrow = arrow(angle = 45, 
                                 length = unit(2, &amp;quot;mm&amp;quot;), 
                                 type = &amp;quot;closed&amp;quot;,
                                 ends = &amp;quot;last&amp;quot;)
                   ) +
    # Node names
    geom_node_text(aes(label = name, hjust = hjust), size = 3) +
  ggtitle(&amp;#39;An example&amp;#39;) +
    theme_graph()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-04-01-big_five_blog_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt; #theme_graph( plot_margin = margin(6, 6, 6, 6))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For example, an &lt;em&gt;essentially tau-equivalent model&lt;/em&gt; includes a constant, whereas a strictly tau-equivalent model does not. Although the addition of a constant has an effect on the mean, it does not affect the
variances, covariances or the value of reliability.&lt;/p&gt;
&lt;p&gt;To determine the scale, the variance of the latent variable is set to 1.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The congeneric model does not have additional constraints.&lt;/li&gt;
&lt;li&gt;The tau-equivalent model is the same as the congeneric model, only with the constraint that all the factor loadings are equal, but allows the error variances to vary from item to item.&lt;/li&gt;
&lt;li&gt;The parallel model is the tau-equivalent model with the constraint that the error variances are all equal.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;cronbachs-alpha&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Cronbach’s alpha&lt;/h1&gt;
&lt;/div&gt;
&lt;div id=&#34;calculate-cronbachs-alpha-for-subgroup-a&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Calculate Cronbach’s Alpha for subgroup A&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://rameliaz.github.io/mg-sem-workshop/cho2016.pdf&#34; class=&#34;uri&#34;&gt;https://rameliaz.github.io/mg-sem-workshop/cho2016.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Classical Test Theory (CTT) considers four or more tests to be congenerically equivalent if all tests may be expressed in terms of one factor and a residual error. (N.b. we need at least four tests to identify all free parameters of the model).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Parallel tests are the special case where (usually two) tests have equal factor loadings.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Tau equivalent tests have equal factor loadings but may have unequal errors.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Congeneric tests may differ in both factor loading and error variances.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(psych)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;psych&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:ggplot2&amp;#39;:
## 
##     %+%, alpha&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(42) #keep the same starting values
#four congeneric measures

loadings &amp;lt;- rep(1, 4)
loadings &amp;lt;- c(0.8, 0.7, 0.6, 0.5)

errors &amp;lt;- c(0.8, 0.7, 0.6, 0.5)

errors &amp;lt;- sqrt(1 - loadings^2)
errors &amp;lt;- rep(0.1, 4)

r4 &amp;lt;- psych::sim.congeneric(loads = loadings,
                            err = errors, #A vector of error variances 
                            low = -3, # values less than low are forced to low (when cat = T)
                            high = 3, # values greater than high are forced to high
                            short = FALSE, 
                            categorical = FALSE,
                            N = 10000)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have less noise on the item with the lower factor loadings.
&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is the (common) latent variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my.df &amp;lt;- data.frame(r4$latent, r4$observed)

# Check factor loadings
fit1 &amp;lt;- lm(V1 ~ theta, data = my.df)
fit2 &amp;lt;- lm(V2 ~ theta, data = my.df)
fit3 &amp;lt;- lm(V3 ~ theta, data = my.df)
fit4 &amp;lt;- lm(V4 ~ theta, data = my.df)

sd(fit1$residuals)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1007705&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd(fit2$residuals)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1013989&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd(fit3$residuals)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1008644&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd(fit4$residuals)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.09897803&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;errors &amp;lt;- my.df[, colnames(my.df) %in% c(&amp;quot;e1&amp;quot;, &amp;quot;e2&amp;quot;, &amp;quot;e3&amp;quot;, &amp;quot;e4&amp;quot;)]

cor(errors)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              e1           e2           e3           e4
## e1  1.000000000  0.010432462  0.002030510 -0.006050927
## e2  0.010432462  1.000000000 -0.008618315 -0.027381449
## e3  0.002030510 -0.008618315  1.000000000 -0.005340683
## e4 -0.006050927 -0.027381449 -0.005340683  1.000000000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# errors are uncorrelated with each other
cov(errors)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              e1          e2           e3           e4
## e1  1.015522554  0.01066020  0.002063898 -0.006035393
## e2  0.010660197  1.02817565 -0.008814430 -0.027480772
## e3  0.002063898 -0.00881443  1.017364269 -0.005331801
## e4 -0.006035393 -0.02748077 -0.005331801  0.979665221&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;var(my.df$e1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.015523&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;var(my.df$theta)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.012307&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ok, let’s have it. The cronbach alpha.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# r4 population correlation matrix
psych::alpha(r4$observed)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Reliability analysis   
## Call: psych::alpha(x = r4$observed)
## 
##   raw_alpha std.alpha G6(smc) average_r S/N     ase    mean   sd median_r
##       0.98      0.99    0.99      0.97 154 0.00013 -0.0074 0.66     0.97
## 
##  lower alpha upper     95% confidence boundaries
## 0.98 0.98 0.98 
## 
##  Reliability if an item is dropped:
##    raw_alpha std.alpha G6(smc) average_r S/N alpha se   var.r med.r
## V1      0.98      0.99    0.99      0.97 102  0.00021 1.9e-05  0.97
## V2      0.97      0.99    0.99      0.97 109  0.00023 3.1e-05  0.97
## V3      0.98      0.99    0.99      0.98 119  0.00021 3.8e-05  0.97
## V4      0.99      0.99    0.99      0.98 140  0.00015 9.8e-06  0.98
## 
##  Item statistics 
##        n raw.r std.r r.cor r.drop    mean   sd
## V1 10000  0.99  0.99  0.99   0.99 -0.0090 0.81
## V2 10000  0.99  0.99  0.99   0.99 -0.0071 0.71
## V3 10000  0.99  0.99  0.99   0.98 -0.0070 0.61
## V4 10000  0.99  0.99  0.98   0.98 -0.0066 0.51&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check with ground truth:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# calculate total score
my.df$sumV &amp;lt;- with(my.df, V1+V2+V3+V4)

my.df$sumE &amp;lt;- with(my.df, e1+e2+e3+e4)

plot(my.df$sumV, my.df$theta)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-04-01-big_five_blog_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# raw alpha
(1-sum(diag(cov(r4$observed)))/var(my.df$sumV))*4/3&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.984329&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cov(r4$observed)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           V1        V2        V3        V4
## V1 0.6568506 0.5665898 0.4855468 0.4045279
## V2 0.5665898 0.5065013 0.4252159 0.3541304
## V3 0.4855468 0.4252159 0.3746968 0.3037033
## V4 0.4045279 0.3541304 0.3037033 0.2629166&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And why does this measure “reliability” of a test?&lt;/p&gt;
&lt;p&gt;The resulting α coefficient of reliability ranges from 0 to 1 in providing this overall assessment of a measure’s reliability. If all of the scale items are entirely independent from one another (i.e., are not correlated or share no covariance), then α = 0; and, if all of the items have high covariances, then α will approach 1 as the number of items in the scale approaches infinity.&lt;/p&gt;
&lt;p&gt;Cronbach’s α {} assumes that all factor loadings are equal. In reality this is rarely the case, and hence it systematically underestimates the reliability. An alternative to Cronbach’s α {} that does not rely on this assumption is congeneric reliability ( ρ C {&lt;em&gt;{C}} &lt;/em&gt;{C})&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;two-independent-orthogonal-factors&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Two independent (orthogonal) factors&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# examples of two independent factors that produce reasonable alphas
#this is a case where alpha is a poor indicator of unidimensionality
set.seed(123)
two.f &amp;lt;- data.frame(sim.item(nvar = 8,
                             nsub = 10000))

with(two.f, plot(V1, V3))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-04-01-big_five_blog_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#sim.item(nvar = 72, nsub = 500, circum = FALSE, xloading = 0.6, yloading = 0.6, 
# gloading = 0, xbias = 0, ybias = 0, categorical = FALSE, low = -3, high = 3, 
# truncate = FALSE, cutpoint = 0)

#specify which items to reverse key by name
alpha(two.f, keys = c(&amp;quot;V1&amp;quot;,&amp;quot;V2&amp;quot;,&amp;quot;V7&amp;quot;,&amp;quot;V8&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Reliability analysis   
## Call: alpha(x = two.f, keys = c(&amp;quot;V1&amp;quot;, &amp;quot;V2&amp;quot;, &amp;quot;V7&amp;quot;, &amp;quot;V8&amp;quot;))
## 
##   raw_alpha std.alpha G6(smc) average_r S/N    ase  mean   sd median_r
##        0.6       0.6    0.63      0.16 1.5 0.0062 0.053 0.51    0.017
## 
##  lower alpha upper     95% confidence boundaries
## 0.58 0.6 0.61 
## 
##  Reliability if an item is dropped:
##     raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r
## V1-      0.56      0.56    0.59      0.15 1.3   0.0068 0.032 0.019
## V2-      0.57      0.57    0.59      0.16 1.3   0.0068 0.031 0.019
## V3       0.56      0.56    0.59      0.16 1.3   0.0068 0.032 0.019
## V4       0.56      0.56    0.59      0.16 1.3   0.0068 0.033 0.014
## V5       0.56      0.56    0.59      0.15 1.3   0.0068 0.033 0.013
## V6       0.56      0.56    0.59      0.16 1.3   0.0068 0.032 0.019
## V7-      0.56      0.56    0.59      0.15 1.3   0.0068 0.033 0.014
## V8-      0.57      0.57    0.60      0.16 1.3   0.0067 0.032 0.019
## 
##  Item statistics 
##         n raw.r std.r r.cor r.drop    mean   sd
## V1- 10000  0.52  0.52  0.42   0.31  0.1112 1.00
## V2- 10000  0.51  0.50  0.40   0.29  0.0949 1.00
## V3  10000  0.51  0.51  0.40   0.29  0.0048 1.01
## V4  10000  0.51  0.51  0.40   0.30 -0.0029 0.99
## V5  10000  0.52  0.52  0.41   0.30  0.0104 1.00
## V6  10000  0.51  0.51  0.41   0.30  0.0143 1.00
## V7- 10000  0.51  0.52  0.41   0.30  0.1067 0.99
## V8- 10000  0.50  0.50  0.38   0.28  0.0837 1.01&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;intermezzo-scientific-progress-vs-proprietary-instruments&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Intermezzo: Scientific progress vs proprietary instruments&lt;/h1&gt;
&lt;p&gt;Something that always puzzled me is that psychometric research often makes use of non-free measurement instruments (questionnaires), with access controlled by test publishers. How can science progress optimally when such barriers to reproducibility exist? It appears I am not alone in this viewpoint.&lt;/p&gt;
&lt;p&gt;Exactly because of this reason, the &lt;em&gt;International Personality Item Pool&lt;/em&gt; was created.&lt;/p&gt;
&lt;p&gt;From &lt;a href=&#34;https://ipip.ori.org/newRationale.htm&#34; class=&#34;uri&#34;&gt;https://ipip.ori.org/newRationale.htm&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;On the other hand, most broad-bandwidth personality inventories (like the MMPI, CPI, 16PF, and NEO-PI) are &amp;gt; proprietary instruments, whose items are copyrighted by the test authors. As a consequence, the
instruments cannot be used freely by other scientists, who thus cannot contribute to their further
development and refinement. Indeed, broad-bandwidth inventories are rarely revised. At most, after many
decades of commercial use, some of the most dated items might be changed and/or new norms established. For
many inventories, nothing is ever done at all.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;And from Goldberg 2006:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In regard to personality-trait measurement however, Goldberg (1999a) attributed the seeming lack of
progress in part to the policies and practices of commercial inventory publishers who could regard certain
scientific activities as detrimental to their pecuniary interests (an issue also raised by Eber,2005).
There are at least four kinds of scientific activities that are disallowed or discouraged by test
publishers […]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;application-big-five-personality-tests&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Application: Big Five personality tests&lt;/h1&gt;
&lt;p&gt;We use a Big Five personality traits dataset. It has 50 items and was developed by Goldberg (1992).&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://ipip.ori.org/newBigFive5broadTable.htm&#34; class=&#34;uri&#34;&gt;https://ipip.ori.org/newBigFive5broadTable.htm&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The documentation provides the following information:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This data was collected (c. 2012) through an interactive online personality test. Participants were
informed that their responses would be recorded and used for research at the begining of the test and
asked to confirm their consent at the end of the test.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The following items were rated on a five point scale where 1=Disagree, 3=Neutral, 5=Agree (0=missed). All were presented on one page in the order E1, N2, A1, C1, O1, E2……&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ──────────────────────────────────────────────────────────────────────── tidyverse 1.3.0 ──&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ✓ tibble  2.1.3     ✓ dplyr   0.8.4
## ✓ tidyr   1.0.2     ✓ stringr 1.4.0
## ✓ readr   1.3.1     ✓ forcats 0.4.0
## ✓ purrr   0.3.3&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Conflicts ─────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──
## x psych::%+%()    masks ggplot2::%+%()
## x psych::alpha()  masks ggplot2::alpha()
## x dplyr::filter() masks tidygraph::filter(), stats::filter()
## x dplyr::lag()    masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(data.table)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;data.table&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:dplyr&amp;#39;:
## 
##     between, first, last&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:purrr&amp;#39;:
## 
##     transpose&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(psych)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;read-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Read data&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;big5 &amp;lt;- read.csv(&amp;quot;BIG5/data.csv&amp;quot;, sep = &amp;quot;\t&amp;quot;)

big5 &amp;lt;- big5[, c(8:ncol(big5))]

big5$id &amp;lt;- 1:nrow(big5)

big5 &amp;lt;- data.table(big5)

mbig5 &amp;lt;- melt(big5, id.vars = &amp;quot;id&amp;quot;)

mbig5 &amp;lt;- mbig5[, factor_label := substr(variable, 1, 1)]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;make-some-plots&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Make some plots&lt;/h1&gt;
&lt;p&gt;Let us plot the variation for each separate item.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(mbig5, aes(x = variable, y = value, col = factor_label)) +
  geom_boxplot() + coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-04-01-big_five_blog_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Lets sort them by value.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(mbig5, aes(x = reorder(variable, value), y = value, col = factor_label)) +
  geom_boxplot() + coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-04-01-big_five_blog_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;make-network-plot-of-all-correlations&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Make network plot of ALL correlations&lt;/h1&gt;
&lt;p&gt;Output a network plot of a correlation data frame in which variables that are more highly correlated appear closer together and are joined by stronger paths. Paths are also colored by their sign (blue for positive and red for negative). The proximity of the points are determined using multidimensional clustering.
It uses the &lt;code&gt;cmdscale&lt;/code&gt; function. Multidimensional scaling takes a set of dissimilarities and returns a set of points such that the distances between the points are approximately equal to the dissimilarities. This function follows the analysis of Mardia (1978), and returns the best-fitting 2-dimensional representation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(corrr)

rdf &amp;lt;- correlate(big5[, -c(&amp;quot;id&amp;quot;)],  method = &amp;quot;spearman&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Correlation method: &amp;#39;spearman&amp;#39;
## Missing treated using: &amp;#39;pairwise.complete.obs&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;corrr::network_plot(rdf, curved = FALSE, min_cor = 0.3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-04-01-big_five_blog_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note that we do NOT calculate a partial correlation plot, i.e. the correlation between two items after “controlling” for all the other items.&lt;/p&gt;
&lt;p&gt;Surprisingly, “factor alpha” (N, A and C) and “factor beta” (E and O) are not visible here.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-a-few-individual-correlations&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Plot a few individual correlations&lt;/h1&gt;
&lt;p&gt;lets pick two questions from “Agreeableness”. These are close together, so highly correlated.&lt;/p&gt;
&lt;p&gt;A4 I sympathize with others’ feelings.
A6 I have a soft heart.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(big5, aes(x = A4, y = A6)) + geom_jitter(alpha = 0.1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-04-01-big_five_blog_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A1 I feel little concern for others.
A3 I insult people.&lt;/p&gt;
&lt;p&gt;These are questions to which the answer other than 1 are socially undesirable.
We would expect most answers to lie at (1,1). Indeed this is the case.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(big5, aes(x = A1, y = A3)) + geom_jitter(alpha = 0.1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-04-01-big_five_blog_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let ’ s move on to Extraversion.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;E1 I am the life of the party.&lt;/li&gt;
&lt;li&gt;E2 I don’t talk a lot.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(big5, aes(x = E1, y = E2)) + geom_jitter(alpha = 0.1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-04-01-big_five_blog_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;E9 I don’t mind being the center of attention.
E10 I am quiet around strangers.&lt;/p&gt;
&lt;p&gt;If you are quiet around strangers, it is difficult to imagine someone liking to be the center of attention.
If you are NOT quiet around strangers, it is still possible to not liking to be the center of attention.&lt;/p&gt;
&lt;p&gt;Interestingly, it is the other way around. So if you are very quiet around strangers, you can still like to be the center of attention!
But if you are not quiet around strangers (so you are very talkative) you have a high probability of wanting to be at the center of attention.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(big5, aes(x = E9, y = E10)) + geom_jitter(alpha = 0.1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-04-01-big_five_blog_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;These two questions appear highly correlated, although they “belong” to different latent factors.&lt;/p&gt;
&lt;p&gt;E10 I am quiet around strangers.
N10 I often feel blue.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(big5, aes(x = E10, y = N10)) + geom_jitter(alpha = 0.1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-04-01-big_five_blog_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Ok that makes sense. If you often feel blue, you are very likely to be quiet around strangers.
However, if you do not often feel blue, you can still have widly varying attitudes towards talking to strangers. So it makes sense here that N10 provides a lower bound for E10.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;calculate-cronbachs-alpha-for-the-five-factors&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Calculate Cronbach’s alpha for the five factors&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;factor_labels &amp;lt;- unique(mbig5$factor_label)

c_alphas &amp;lt;- c()
i &amp;lt;- 1

for(l in factor_labels){
  sub_big &amp;lt;- mbig5[factor_label == l]
  
  sub_big &amp;lt;- dcast(sub_big, id ~ variable, value.var = &amp;quot;value&amp;quot;)
  sub_big$id &amp;lt;- NULL
  
  tmp &amp;lt;- psych::alpha(sub_big, check.keys = T)
  c_alphas[i] &amp;lt;- tmp[[1]]$raw_alpha
  i &amp;lt;- i + 1
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in psych::alpha(sub_big, check.keys = T): Some items were negatively correlated with total scale and were automatically reversed.
##  This is indicated by a negative sign for the variable name.

## Warning in psych::alpha(sub_big, check.keys = T): Some items were negatively correlated with total scale and were automatically reversed.
##  This is indicated by a negative sign for the variable name.

## Warning in psych::alpha(sub_big, check.keys = T): Some items were negatively correlated with total scale and were automatically reversed.
##  This is indicated by a negative sign for the variable name.

## Warning in psych::alpha(sub_big, check.keys = T): Some items were negatively correlated with total scale and were automatically reversed.
##  This is indicated by a negative sign for the variable name.

## Warning in psych::alpha(sub_big, check.keys = T): Some items were negatively correlated with total scale and were automatically reversed.
##  This is indicated by a negative sign for the variable name.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res &amp;lt;- data.frame(factor_labels, raw_alpha = c_alphas)

res&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   factor_labels raw_alpha
## 1             E 0.8921807
## 2             N 0.8691398
## 3             A 0.8318745
## 4             C 0.8126381
## 5             O 0.7938856&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we have summed (after reversing) all scores per label.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;International Personality Item Pool: A Scientific Collaboratory for the Development of Advanced Measures of Personality Traits and Other Individual Differences (&lt;a href=&#34;http://ipip.ori.org/&#34; class=&#34;uri&#34;&gt;http://ipip.ori.org/&lt;/a&gt;). Internet Web Site.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Goldberg, L. R. (1992). The development of markers for the Big-Five factor structure. Psychological Assessment, 4, 26-42.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Goldberg, L. R. (1999). A broad-bandwidth, public domain, personality inventory measuring the lower-level facets of several five-factor models. In I. Mervielde, I. Deary, F. De Fruyt, &amp;amp; F. Ostendorf (Eds.), Personality Psychology in Europe, Vol. 7 (pp. 7-28). Tilburg, The Netherlands: Tilburg University Press.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Goldberg, L. R., Johnson, J. A., Eber, H. W., Hogan, R., Ashton, M. C., Cloninger, C. R., &amp;amp; Gough, H. C. (2006). The International Personality Item Pool and the future of public-domain personality measures. Journal of Research in Personality, 40, 84-96.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Johnson 2014, Measuring thirty facets of the Five Factor Model with a 120-item public domain inventory: Development of the IPIP-NEO-120 &lt;a href=&#34;http://www.personal.psu.edu/faculty/j/5/j5j/papers/JRP2014.pdf&#34; class=&#34;uri&#34;&gt;http://www.personal.psu.edu/faculty/j/5/j5j/papers/JRP2014.pdf&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Cho, E (2016). “Making Reliability Reliable”. Organizational Research Methods. 19 (4): 651–682&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
