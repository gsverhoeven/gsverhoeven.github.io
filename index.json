[{"authors":null,"categories":["R"],"content":"\rIn this post, we’ll explore the BupaR suite of Process Mining packages created by Gert Janssenswillen from Hasselt University.\nWe start with exploring the patients dataset contained in the eventdataR package. According to the documentation, this is an “Artifical eventlog about patients”.\nGetting started\rAfter installing all required packages, we can load the whole “bupaverse” by loading the bupaR package.\nlibrary(ggplot2)\rlibrary(bupaR)\r#library(eventdataR)\rdf \u0026lt;- eventdataR::patients\rThe data is already in event log format, let’s check it out.\nsummary(df)\r## Number of events: 5442\r## Number of cases: 500\r## Number of traces: 7\r## Number of distinct activities: 7\r## Average trace length: 10.884\r## ## Start eventlog: 2017-01-02 11:41:53\r## End eventlog: 2018-05-05 07:16:02\r## handling patient employee ## Blood test : 474 Length:5442 r1:1000 ## Check-out : 984 Class :character r2:1000 ## Discuss Results : 990 Mode :character r3: 474 ## MRI SCAN : 472 r4: 472 ## Registration :1000 r5: 522 ## Triage and Assessment:1000 r6: 990 ## X-Ray : 522 r7: 984 ## handling_id registration_type time ## Length:5442 complete:2721 Min. :2017-01-02 11:41:53 ## Class :character start :2721 1st Qu.:2017-05-06 17:15:18 ## Mode :character Median :2017-09-08 04:16:50 ## Mean :2017-09-02 20:52:34 ## 3rd Qu.:2017-12-22 15:44:11 ## Max. :2018-05-05 07:16:02 ## ## .order ## Min. : 1 ## 1st Qu.:1361 ## Median :2722 ## Mean :2722 ## 3rd Qu.:4082 ## Max. :5442 ## \rSo we learn that there are 500 “cases”, i.e. patients. There are 7 different activities.\nLet’s check out the data for a single patient:\ndf %\u0026gt;% filter(patient == 1) %\u0026gt;% arrange(handling_id) #%\u0026gt;% \r## Event log consisting of:\r## 12 events\r## 1 traces\r## 1 cases\r## 6 activities\r## 6 activity instances\r## ## # A tibble: 12 x 7\r## handling patient employee handling_id registration_ty~\r## \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; ## 1 Registr~ 1 r1 1 start ## 2 Registr~ 1 r1 1 complete ## 3 Blood t~ 1 r3 1001 start ## 4 Blood t~ 1 r3 1001 complete ## 5 MRI SCAN 1 r4 1238 start ## 6 MRI SCAN 1 r4 1238 complete ## 7 Discuss~ 1 r6 1735 start ## 8 Discuss~ 1 r6 1735 complete ## 9 Check-o~ 1 r7 2230 start ## 10 Check-o~ 1 r7 2230 complete ## 11 Triage ~ 1 r2 501 start ## 12 Triage ~ 1 r2 501 complete ## # ... with 2 more variables: time \u0026lt;dttm\u0026gt;, .order \u0026lt;int\u0026gt;\r # select(handling, handling_id, registration_type) # does not work\rWe learn that each “handling” has a separate start and complete timestamp.\n\rTraces\rThe summary info of the event log also counts so-called “traces”. A trace is defined a unique sequence of events in the event log. Apparently, there are only seven different traces (possible sequences). Let’s visualize them.\nTo visualize all traces, we set coverage to 1.0.\ndf %\u0026gt;% trace_explorer(type = \u0026quot;frequent\u0026quot;, coverage = 1.0)\rSo there are a few traces (0.6%) that do not end with a check-out. Ignoring these rare cases, we find that there are two types of cases:\n\rCases that get an X-ray\rCases that get a blood test followed by an MRI scan\r\r\rThe dotted chart\rA really powerful visualization in process mining comes in the form of a “dotted chart”. The dotted chart function produces a ggplot graph, which is nice, because so we can actually tweak the graph as we can with regular ggplot objects.\nIt has two nice use cases. The first is when we plot actual time on the x-axis, and sort the cases by starting date.\ndf %\u0026gt;% dotted_chart(x = \u0026quot;absolute\u0026quot;, sort = \u0026quot;start\u0026quot;) + ggtitle(\u0026quot;All cases\u0026quot;) +\rtheme_gray()\rThe slope of this graphs learns us the rate of new cases, and if this changes over time. Here it appears constant, with 500 cases divided over five quarter years.\nThe second is to align all cases relative to the first event, and sort on duration of the whole sequence of events.\ndf %\u0026gt;% dotted_chart(x = \u0026quot;relative\u0026quot;, sort = \u0026quot;duration\u0026quot;) + ggtitle(\u0026quot;All cases\u0026quot;) +\rtheme_gray()\rA nice pattern emerges, where all cases start with registration, then quickly proceed to triage and assessment, after that, a time varying period of 1-10 days follows where either the blood test + MRI scan, or the X-ray is performed, followed by discussing the results. Finally, check out occurs.\n\rConclusion\rTo conclude, the process mining approach to analyze time series event data appears highly promising. The dotted chart is a great addition to my data visualization repertoire, and the process mining folks appear to have at lot more goodies, such as Trace Alignment.\n\r","date":1560988800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560988800,"objectID":"f36148e4bf390595d95a45aa28e76282","permalink":"https://gsverhoeven.github.io/post/exploring-process-mining/","publishdate":"2019-06-20T00:00:00Z","relpermalink":"/post/exploring-process-mining/","section":"post","summary":"In this post, we'll explore the BupaR suite of Process Mining packages created by Gert Janssenswillen of Hasselt University.","tags":["process mining"],"title":"Exploring Process Mining in R","type":"post"},{"authors":null,"categories":["R"],"content":"\rIntroduction\r(Short intro) This is me learning causal inference (CI) by self-study together with colleagues using online resources.\n(Longer intro) A modern data scientist needs to become skilled in at least three topics (I left out visualization):\n\r(Bayesian) Statistical modeling\rMachine Learning\rCausal inference\r\rFor the first two topics, great introductory books exist that\n\rfocus on learning-by-doing and\rare low on math and high on simulation / programming in R\rare fun / well written\r\rFor Bayesian statistical modeling, we have the awesome textbook “Statistical Rethinking” by Richard mcElreath.\nFor Machine Learning, we have the (free) book “Introduction to Statistical Learning” by James, Witten, Hastie \u0026amp; Tibshirani.\nHowever, for Causal Inference, such a book does not exist yet AFAIK. Therefore, I tried to piece together a Causal Inference course based on the criteria mentioned above.\n\rDesigning an introductory causal inference course\rExplicit goal was to contrast/combine the causal graph (DAG) approach with what some call “Quasi-experimental designs”, i.e. the econometric causal effects toolkit (Regression Discontinuity Design, matching, instrumental variables etc).\nIn the end, I decided to combine the two causal chapters from Gelman \u0026amp; Hill (2007) (freely available on Gelman’s website) with the introductory chapter on Causal Graphical Models by Felix Elwert (freely available on Elwert’s website).\nThe Gelman \u0026amp; Hill chapters already come with a set of exercises. However, for DAGs, i could not find a suitable set of exercises.\nSo I created two R markdown notebooks with exercises in R, that make use of the DAGitty tool, created by Johannes Textor and freely available as R package.\nSome exercises are taken from Causal inference in statistics: A Primer by Pearl, Glymour \u0026amp; Jewell. (I probably should own this book. So I just ordered it :))\nAll materials are available in a GitHub repository\n\rOutline of the course\rThe course has four parts.\nGeneral introduction\rThe course starts with the first causal chapter of Gelman \u0026amp; Hill’s book, “Causal inference using regression on the treatment variable”. This creates a first complete experience with identifying and estimating causal effects. However, there are no causal diagrams, which is unfortunate.\n\rIdentification of Causal effects using DAGs\rNext we dive into causal identification using the causal diagram approach. For this we use the chapter “Causal Graphical Models” by Felix Elwert. Two R markdown Notebooks with exercises using Dagitty complete this part.\n\rIdentification and estimation strategies\rWe then continue with the second causal chapter of Gelman \u0026amp; Hill “Causal inference using more advanced models”. This covers matching, regression discontinuity design, and instrumental variables. This material is combined with a paper by Scheiner et al, that contains DAGs for these methods. In our study group DAGs greatly facilitated discussion of the various designs.\n\rVarying treatment effects using Machine Learning\rFinally, and this part of the course has yet to take place, is the topic of estimating heterogeneous (i.e. subgroup, or even individual) treatment effects. This covers recent developements based on (forests of) regression trees. The plan is to cover both bayesian (BART, Chipman \u0026amp; mcCullough) and non-bayesian (GRF, Athey \u0026amp; Wager) methods.\n\r\rLooking back so far\rThe causal diagram / DAG approach is nonparametric and its purpose is to\n\rMake assumptions on the data generating process explicit\rFormalize identification of causal effects\r\rThus, it is separate from, and complements statistical estimation. The distinction between identification and estimation is not so explicitly made in the Gelman \u0026amp; Hill chapters, at least this is my impression. It would really benefit from adding DAGs, as Richard mcElreath is doing in his upcoming second edition of Statistical Rethinking.\nAfter having worked through these materials, I think reading Shalizi’s chapters on Causal Effects would be a smart move. This is part III of his book “Advanced Data Analysis from an Elementary Point of View”, which is awesome in its clarity, practical remarks a.k.a. normative statements by the author, and breadth.\nIf you have a question, would like to comment or share ideas feel free to contact me.\n\r","date":1557532800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557532800,"objectID":"71aeaecf1bbcd7d5f52d2141de1743f4","permalink":"https://gsverhoeven.github.io/post/causal-inference-course/","publishdate":"2019-05-11T00:00:00Z","relpermalink":"/post/causal-inference-course/","section":"post","summary":"In this blog post, I describe the introductory course on Causal Inference I pieced together using various materials available online. It combines Pearl's Causal Graph approach with statistics Gelman/mcElreath style.","tags":["R, causal inference, DAG, causal graph"],"title":"Designing an introductory course on Causal Inference","type":"post"},{"authors":null,"categories":["R"],"content":"\rIn this blog post, we explore how to implement the validation set approach in caret. This is the most basic form of the train/test machine learning concept. For example, the classic machine learning textbook “An introduction to Statistical Learning” uses the validation set approach to introduce resampling methods.\nIn practice, one likes to use k-fold Cross validation, or Leave-one-out cross validation, as they make better use of the data. This is probably the reason that the validation set approach is not one of caret’s preset methods.\nBut for teaching purposes it would be very nice to have a caret implementation.\nThis would allow for an easy demonstration of the variability one gets when choosing different partionings. It also allows direct demonstration of why k-fold CV is superior to the validation set approach with respect to bias/variance.\nWe pick the BostonHousing dataset for our example code.\n# Boston Housing knitr::kable(head(Boston))\r\r\rcrim\rzn\rindus\rchas\rnox\rrm\rage\rdis\rrad\rtax\rptratio\rblack\rlstat\rmedv\r\r\r\r0.00632\r18\r2.31\r0\r0.538\r6.575\r65.2\r4.0900\r1\r296\r15.3\r396.90\r4.98\r24.0\r\r0.02731\r0\r7.07\r0\r0.469\r6.421\r78.9\r4.9671\r2\r242\r17.8\r396.90\r9.14\r21.6\r\r0.02729\r0\r7.07\r0\r0.469\r7.185\r61.1\r4.9671\r2\r242\r17.8\r392.83\r4.03\r34.7\r\r0.03237\r0\r2.18\r0\r0.458\r6.998\r45.8\r6.0622\r3\r222\r18.7\r394.63\r2.94\r33.4\r\r0.06905\r0\r2.18\r0\r0.458\r7.147\r54.2\r6.0622\r3\r222\r18.7\r396.90\r5.33\r36.2\r\r0.02985\r0\r2.18\r0\r0.458\r6.430\r58.7\r6.0622\r3\r222\r18.7\r394.12\r5.21\r28.7\r\r\r\rOur model is predicting medv (Median house value) using predictors indus and chas in a multiple linear regression. We split the data in half, 50% for fitting the model, and 50% to use as a validation set.\nStratified sampling vs random sampling\rTo check if we understand what caret does, we first implement the validation set approach ourselves. To be able to compare, we need exactly the same data partitions for our manual approach and the caret approach. As caret requires a particular format (a named list of sets of train indices) we conform to this standard. However, all caret partitioning functions seem to perform stratified random sampling. This means that it first partitions the data in equal sized groups based on the outcome variable, and then samples at random within those groups to partitions that have similar distributions for the outcome variable.\nThis not desirable for teaching, as it adds more complexity. In addition, it would be nice to be able to compare stratified vs. random sampling.\nWe therefore write a function that generates truly random partitions of the data. We let it generate partitions in the format that trainControl likes.\n# internal function from caret package, needed to play nice with resamples()\rprettySeq \u0026lt;- function(x) paste(\u0026quot;Resample\u0026quot;, gsub(\u0026quot; \u0026quot;, \u0026quot;0\u0026quot;, format(seq(along = x))), sep = \u0026quot;\u0026quot;)\rcreateRandomDataPartition \u0026lt;- function(y, times, p) {\rvec \u0026lt;- 1:length(y)\rn_samples \u0026lt;- round(p * length(y))\rresult \u0026lt;- list()\rfor(t in 1:times){\rindices \u0026lt;- sample(vec, n_samples, replace = FALSE)\rresult[[t]] \u0026lt;- indices\r#names(result)[t] \u0026lt;- paste0(\u0026quot;Resample\u0026quot;, t)\r}\rnames(result) \u0026lt;- prettySeq(result)\rresult\r}\rcreateRandomDataPartition(1:10, times = 2, p = 0.5)\r## $Resample1\r## [1] 1 7 3 6 10\r## ## $Resample2\r## [1] 7 6 8 9 4\r\rThe validation set approach without caret\rHere is the validation set approach without using caret. We create a single random partition of the data in train and validation set, fit the model on the training data, predict on the validation data, and calculate the RMSE error on the test predictions.\nset.seed(1234)\rparts \u0026lt;- createRandomDataPartition(Boston$medv, times = 1, p = 0.5)\rtrain \u0026lt;- parts$Resample1\r# fit ols on train data\rlm.fit \u0026lt;- lm(medv ~ indus + chas , data = Boston[train,])\r# predict on held out data\rpreds \u0026lt;- predict(lm.fit, newdata = Boston[-train,])\r# calculate RMSE validation error\rsqrt(mean((preds - Boston[-train,]$medv)^2))\r## [1] 8.217625\rIf we feed caret the same data partition, we expect exactly the same test error for the held-out data. Let’s find out!\n\rThe validation set approach in caret\rNow we use the caret package. Regular usage requires two function calls, one to trainControl to control the resampling behavior, and one to train to do the actual model fitting and prediction generation.\nAs the validation set approach is not one of the predefined methods, we need to make use of the index argument to explicitely define the train partitions outside of caret. It automatically predicts on the records that are not contained in the train partitions.\nThe index argument plays well with the createDataPartition (Stratfied sampling) and createRandomDataPartition (our own custom function that performs truly random sampling) functions, as these functions both generate partitions in precisely the format that index wants: lists of training set indices.\nIn the code below, we generate four different 50/50 partitions of the data.\nWe set savePredictions to TRUE to be able to verify the calculated metrics such as the test RMSE.\nset.seed(1234)\r# create four partitions\rparts \u0026lt;- createRandomDataPartition(Boston$medv, times = 4, p = 0.5)\rctrl \u0026lt;- trainControl(method = \u0026quot;repeatedcv\u0026quot;, ## The method doesn\u0026#39;t matter\r## since we are defining the resamples\rindex= parts, ##verboseIter = TRUE, ##repeats = 1,\rsavePredictions = TRUE\r##returnResamp = \u0026quot;final\u0026quot;\r) \rNow we can run caret and fit the model four times:\nres \u0026lt;- train(medv ~ indus + chas, data = Boston, method = \u0026quot;lm\u0026quot;,\rtrControl = ctrl)\rres\r## Linear Regression ## ## 506 samples\r## 2 predictor\r## ## No pre-processing\r## Resampling: Cross-Validated (10 fold, repeated 1 times) ## Summary of sample sizes: 253, 253, 253, 253 ## Resampling results:\r## ## RMSE Rsquared MAE ## 8.140461 0.221556 5.970956\r## ## Tuning parameter \u0026#39;intercept\u0026#39; was held constant at a value of TRUE\rFrom the result returned by train we can verify that it has fitted a model on four different datasets, each of size 253. By default it reports the average test error over the four validation sets. We can also extract the four individual test errors:\n# strangely enough, resamples() always wants at least two train() results\r# see also the man page for resamples()\rresamples \u0026lt;- resamples(list(MOD1 = res, MOD2 = res))\rresamples$values$`MOD1~RMSE`\r## [1] 8.217625 7.960800 8.244937 8.138481\r# check that we recover the RMSE reported by train() in the Resampling results\rmean(resamples$values$`MOD1~RMSE`)\r## [1] 8.140461\rsummary(resamples)\r## ## Call:\r## summary.resamples(object = resamples)\r## ## Models: MOD1, MOD2 ## Number of resamples: 4 ## ## MAE ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA\u0026#39;s\r## MOD1 5.772957 5.949938 6.011035 5.970956 6.032053 6.088796 0\r## MOD2 5.772957 5.949938 6.011035 5.970956 6.032053 6.088796 0\r## ## RMSE ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA\u0026#39;s\r## MOD1 7.9608 8.094061 8.178053 8.140461 8.224453 8.244937 0\r## MOD2 7.9608 8.094061 8.178053 8.140461 8.224453 8.244937 0\r## ## Rsquared ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA\u0026#39;s\r## MOD1 0.2017943 0.2069602 0.2189169 0.221556 0.2335127 0.2465958 0\r## MOD2 0.2017943 0.2069602 0.2189169 0.221556 0.2335127 0.2465958 0\rNote that the RMSE value for the first train/test partition is exactly equal to our own implementation of the validation set approach. Awesome.\n\rValidation set approach: stratified sampling versus random sampling\rSince we now know what we are doing, let’s perform a simulation study to compare stratified random sampling with truly random sampling, using the validation set approach, and repeating this proces say a few thousand times to get a nice distribution of test errors.\n# simulation settings\rn_repeats \u0026lt;- 3000\rtrain_fraction \u0026lt;- 0.8\rFirst we fit the models on the random sampling data partitions:\nset.seed(1234)\rparts \u0026lt;- createRandomDataPartition(Boston$medv, times = n_repeats, p = train_fraction)\rctrl \u0026lt;- trainControl(method = \u0026quot;repeatedcv\u0026quot;, ## The method doesn\u0026#39;t matter\rindex= parts, savePredictions = TRUE\r) rand_sampl_res \u0026lt;- train(medv ~ indus + chas, data = Boston, method = \u0026quot;lm\u0026quot;,\rtrControl = ctrl)\rrand_sampl_res\r## Linear Regression ## ## 506 samples\r## 2 predictor\r## ## No pre-processing\r## Resampling: Cross-Validated (10 fold, repeated 1 times) ## Summary of sample sizes: 405, 405, 405, 405, 405, 405, ... ## Resampling results:\r## ## RMSE Rsquared MAE ## 7.872387 0.2755166 5.806044\r## ## Tuning parameter \u0026#39;intercept\u0026#39; was held constant at a value of TRUE\rNext, we fit the models on the stratified sampling data partitions:\nset.seed(1234)\rparts \u0026lt;- createDataPartition(Boston$medv, times = n_repeats, p = train_fraction, list = T)\rctrl \u0026lt;- trainControl(method = \u0026quot;repeatedcv\u0026quot;, ## The method doesn\u0026#39;t matter\rindex= parts, savePredictions = TRUE\r) strat_sampl_res \u0026lt;- train(medv ~ indus + chas, data = Boston, method = \u0026quot;lm\u0026quot;,\rtrControl = ctrl)\rstrat_sampl_res\r## Linear Regression ## ## 506 samples\r## 2 predictor\r## ## No pre-processing\r## Resampling: Cross-Validated (10 fold, repeated 1 times) ## Summary of sample sizes: 407, 407, 407, 407, 407, 407, ... ## Resampling results:\r## ## RMSE Rsquared MAE ## 7.818498 0.280648 5.759168\r## ## Tuning parameter \u0026#39;intercept\u0026#39; was held constant at a value of TRUE\rThen, we merge the two results to compare the distributions:\nresamples \u0026lt;- resamples(list(RAND = rand_sampl_res, STRAT = strat_sampl_res))\r\rAnalyzing caret resampling results\rWe now analyse our resampling results. We can use the summary method on our resamples object:\nsummary(resamples)\r## ## Call:\r## summary.resamples(object = resamples)\r## ## Models: RAND, STRAT ## Number of resamples: 3000 ## ## MAE ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA\u0026#39;s\r## RAND 4.100652 5.492730 5.790283 5.806044 6.104647 7.663345 0\r## STRAT 4.438645 5.464873 5.745391 5.759168 6.044985 7.493055 0\r## ## RMSE ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA\u0026#39;s\r## RAND 5.119472 7.331857 7.857935 7.872387 8.403391 11.17930 0\r## STRAT 5.634478 7.307348 7.815735 7.818498 8.314292 10.43092 0\r## ## Rsquared ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA\u0026#39;s\r## RAND 0.05006335 0.2244762 0.2743783 0.2755166 0.3260239 0.5129500 0\r## STRAT 0.08044329 0.2311793 0.2792260 0.2806480 0.3286326 0.5242075 0\rWe can also use the plot function provided by the caret package. It plots the mean of our performance metric (RMSE), as well as estimation uncertainty of this mean. Note that the confidence intervals here are based on a normal approximation (One sample t-test).\n# caret:::ggplot.resamples\r# t.test(resamples$values$`RAND~RMSE`)\rggplot(resamples, metric = \u0026quot;RMSE\u0026quot;) \rMy personal preference is to more directly display both distributions. This is done by bwplot() (caret does not have ggplot version of this function).\nbwplot(resamples, metric = \u0026quot;RMSE\u0026quot;)\rIt does seems that stratified sampling paints a slightly more optimistic picture of the test error when compared to truly random sampling. However, we can also see that random sampling has somewhat higher variance when compared to stratified sampling.\nBased on these results, it seems like stratified sampling is indeed a reasonable default setting for caret.\n\rUpdate: LGOCV\rset.seed(1234)\rctrl \u0026lt;- trainControl(method = \u0026quot;LGOCV\u0026quot;, ## The method doesn\u0026#39;t matter\rrepeats = n_repeats,\rnumber = 1,\rp = 0.5,\rsavePredictions = TRUE\r) \r## Warning: `repeats` has no meaning for this resampling method.\rlgocv_res \u0026lt;- train(medv ~ indus + chas, data = Boston, method = \u0026quot;lm\u0026quot;,\rtrControl = ctrl)\rlgocv_res\r## Linear Regression ## ## 506 samples\r## 2 predictor\r## ## No pre-processing\r## Resampling: Repeated Train/Test Splits Estimated (1 reps, 50%) ## Summary of sample sizes: 254 ## Resampling results:\r## ## RMSE Rsquared MAE ## 7.73795 0.2942766 5.606969\r## ## Tuning parameter \u0026#39;intercept\u0026#39; was held constant at a value of TRUE\r\r","date":1553126400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553126400,"objectID":"faf4add31be0ffd08e307b2b0c4b5704","permalink":"https://gsverhoeven.github.io/post/validation-set-approach-in-caret/","publishdate":"2019-03-21T00:00:00Z","relpermalink":"/post/validation-set-approach-in-caret/","section":"post","summary":"In this blog post, we explore how to implement the validation set approach in caret. This is the most basic form of the train/test machine learning concept.","tags":["R, caret"],"title":"The validation set approach in caret","type":"post"},{"authors":null,"categories":["Arduino"],"content":"\rIn this post, I show how to create a Arduino-based atmospheric sensor circuit capable of storing large amounts of data on a microSD card.\nNowadays, one can buy a commercial Thermo/Hygro datalogger for 50 Euro online (i.e. https://www.vitalitools.nl/lascar-electronics-el-usb-2-datalogger). However, I decided that it would be a nice project to learn more about Arduino, in particular how to interface it with a microSD card. So i made one myself. Working with SD cards has the advantage of having a huge storage capacity. To give you an impression: Below we analyse 10K measurements stored in a 60 Kb file, the SD card can hold 4 Gb!\nComponents\rAfter some research I ordered:\n\rA microSD card reader/writer with SPI interface (Catalex card)\rA Bosch BME-280 temperature/pressure/humidity sensor with I2C interface\r\rAs the BME-280 sensor operates at 3.3V and my Arduino Nano at 5V, I also ordered a four channel Logic Level Converter to convert the 5V I2C on the Arduino side of the LLC to 3.3V on the BME-280 side.\nTo make the circuit Mains powered, i took an old Samsung mobile phone Charger (5V 0.7A), cutoff the plug and attached it to the breadboard.\n\rCircuit \u0026amp; Programming\rThe breadboard layout (created using Fritzing) is shown below:\n\rAt first i was using the Arduino 5V pin (with Arduino connected to USB at the front of my Desktop PC, these USB ports might have lower current) to power both the SD card and the Level converter. Separately they would work fine, but together in one circuit the SD card gave erratic results. I guessed that current consumption was too high, and during testing I used the 5V charger as power supply for the SD card. During actual usage I used the 5V charger to power both the SD card AND the Arduino Nano, which worked nicely.\nCoding was simple, i just combined the example code and libraries for a SPI SD card and for a BME-280 I2C sensor. I put the code on GitHub anyway as a reference.\n\rData collection and preparation\rI ended up testing the device by letting it collect measurements in four different places within the house. In the following order:\n\rThe living room\rThe basement\rFirst floor bedroom\rFirst floor bathroom\r\rAfter collecting the data I put the microSD card in a microSD card reader and copied the DATALOG.TXT CSV file to my pc for analysis in R.\ndf \u0026lt;- read.csv2(\u0026quot;DATALOG.TXT\u0026quot;, header = F)\rcolnames(df) \u0026lt;- c(\u0026quot;Time\u0026quot;, \u0026quot;Temp\u0026quot;, \u0026quot;Hum\u0026quot;, \u0026quot;Pressure\u0026quot;)\r# give the four traces a unique ID\rdf$start_trace \u0026lt;- ifelse(df$Time == 0, 1, 0)\rdf$trace_id \u0026lt;- cumsum(df$start_trace)\rmdf \u0026lt;- melt(df, id.vars = c(\u0026quot;Time\u0026quot;, \u0026quot;trace_id\u0026quot;))\r## Warning: attributes are not identical across measure variables; they will\r## be dropped\r# label the four traces\rtrace_id \u0026lt;- 1:4\rtrace_name \u0026lt;- c(\u0026quot;Living room\u0026quot;, \u0026quot;Basement\u0026quot;, \u0026quot;Bedroom 1st floor\u0026quot;, \u0026quot;Bathroom 1st floor\u0026quot;)\rcod \u0026lt;- data.table(trace_id, trace_name = factor(trace_name, levels = trace_name))\rmdf \u0026lt;- data.table(merge(mdf, cod, by = \u0026quot;trace_id\u0026quot;))\rmdf \u0026lt;- mdf[, value := as.numeric(value)]\r\rAnalysis\rPressure\rWe start with the pressure measurements. This is supposed to be a proxy for altitude.\nggplot(mdf[mdf$variable == \u0026quot;Pressure\u0026quot; \u0026amp; Time \u0026gt; 1], aes(x = Time, y = value, color = variable, group = variable)) +\rgeom_point(col = \u0026quot;grey\u0026quot;) + facet_grid(~ trace_name) + geom_smooth(size = 1)\r## `geom_smooth()` using method = \u0026#39;loess\u0026#39; and formula \u0026#39;y ~ x\u0026#39;\rThe basement, which is the lowest, has the highest pressure. But the difference between living room (ground floor) and the two rooms at the first floor is less pronounced. What is not so clear is what drives the changes in pressure WHILE the sensor is at a particular location, i.e. in the basement, or on the 1st floor. But no time to dwell on that, let’s move on to the temperature!\n\rTemperature\rggplot(mdf[mdf$variable == \u0026quot;Temp\u0026quot; \u0026amp; Time \u0026gt; 1], aes(x = Time, y = value, color = variable, group = variable)) +\rgeom_point() + facet_grid(~ trace_name)\rHere, it appears that the sequence of the rooms can explain the slowly changing patterns of temperature. We started out in the Living room at 21C (The thermostat was set at 20C at that time). Then towards the cold basement. It appears that temperature needed some time to equilibrate, possibly because the breadboard was placed on an elevated plastic box, insulating it from below. In the bedroom it was placed on the (cold) floor, and it was already cold from the basement. Then in the bathroom, the final location, it went up, probably due to the floor being heated to keep the bathroom at 18C.\n\rRelative Humidity\rFinally, the relative humidity. This appears super strongly correlated with the temperature.\nggplot(mdf[mdf$variable == \u0026quot;Hum\u0026quot; \u0026amp; Time \u0026gt; 1], aes(x = Time, y = value, color = variable, group = variable)) +\rgeom_point() + facet_grid(~ trace_name)\rHere we see that the living room is at a agreeable 45% RH. The basement has a higher RH percentage, expected because it’s colder.\nAccording to Wikipedia:\nHumans can be comfortable within a wide range of humidities depending on the temperature—from 30% to 70%[14]—but ideally between 50%[15] and 60%.[16] Very low humidity can create discomfort, respiratory problems, and aggravate allergies in some individuals.\nThe bedroom is also at a nice humidity level of 55% RH. The bathroom floor was being heated, and this unsurprisingly reduces the local RH to below 40%.\n\r\rConclusion\rIt all seems to work pretty well. Measurement quality appears reasonable, with temperature and humidity consistent and with little noise, whereas the pressure reading needs some averaging / smoothing to get a stable signal.\nI had great fun making this device!\n\r","date":1551744000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551744000,"objectID":"8fec54b618880788adf021e0480e50fc","permalink":"https://gsverhoeven.github.io/post/arduino-atmospheric-datalogger/","publishdate":"2019-03-05T00:00:00Z","relpermalink":"/post/arduino-atmospheric-datalogger/","section":"post","summary":"In this post, I show how to create a Arduino-based atmospheric sensor prototype capable of storing large amounts of data on a microSD card.","tags":["arduino"],"title":"Arduino Weather Station with datalogging","type":"post"},{"authors":null,"categories":["R"],"content":"\rLoad packages\r# library(devtools)\r#devtools::install_github(\u0026quot;vdorie/dbarts\u0026quot;)\rlibrary(dbarts)\rlibrary(ggplot2)\rlibrary(tidyverse)\r## -- Attaching packages ----------------------------------------------- tidyverse 1.2.1 --\r## v tibble 2.0.1 v purrr 0.2.5\r## v tidyr 0.8.2 v dplyr 0.7.8\r## v readr 1.3.1 v stringr 1.3.1\r## v tibble 2.0.1 v forcats 0.3.0\r## -- Conflicts -------------------------------------------------- tidyverse_conflicts() --\r## x dplyr::filter() masks stats::filter()\r## x dplyr::lag() masks stats::lag()\rlibrary(grf)\r#devtools::install_github(\u0026quot;vdorie/aciccomp/2017\u0026quot;)\rlibrary(aciccomp2017)\rlibrary(cowplot)\r## ## Attaching package: \u0026#39;cowplot\u0026#39;\r## The following object is masked from \u0026#39;package:ggplot2\u0026#39;:\r## ## ggsave\rsource(\u0026quot;calcPosteriors.R\u0026quot;)\rfullrun \u0026lt;- 0\r\rDataset 1: Simulated dataset from Friedman MARS paper\rThis is not a causal problem but a prediction problem.\n## y = f(x) + epsilon , epsilon ~ N(0, sigma)\r## x consists of 10 variables, only first 5 matter\rf \u0026lt;- function(x) {\r10 * sin(pi * x[,1] * x[,2]) + 20 * (x[,3] - 0.5)^2 +\r10 * x[,4] + 5 * x[,5]\r}\rset.seed(99)\rsigma \u0026lt;- 1.0\rn \u0026lt;- 100\rx \u0026lt;- matrix(runif(n * 10), n, 10)\rEy \u0026lt;- f(x)\ry \u0026lt;- rnorm(n, Ey, sigma)\rdf \u0026lt;- data.frame(x, y, y_true = Ey)\rfit BART model on simulated Friedman data\rif(fullrun){\r## run BART\rset.seed(99)\rbartFit \u0026lt;- bart(x, y)\rsaveRDS(bartFit, \u0026quot;s1.rds\u0026quot;)\r} else { bartFit \u0026lt;- readRDS(\u0026quot;s1.rds\u0026quot;)}\rplot(bartFit)\rMCMC or sigma looks ok.\ncompare BART fit to true values\rdf2 \u0026lt;- data.frame(df, ql = apply(bartFit$yhat.train, length(dim(bartFit$yhat.train)), quantile,probs=0.05),\rqm = apply(bartFit$yhat.train, length(dim(bartFit$yhat.train)), quantile,probs=.5),\rqu \u0026lt;- apply(bartFit$yhat.train, length(dim(bartFit$yhat.train)), quantile,probs=0.95)\r)\rbartp \u0026lt;- ggplot(df2, aes(x= y, y = qm)) + geom_linerange(aes(ymin = ql, ymax = qu), col = \u0026quot;grey\u0026quot;) +\rgeom_point() + geom_smooth() +\rgeom_abline(intercept = 0, slope = 1, col = \u0026quot;red\u0026quot;, size = 1)\rbartp\r## `geom_smooth()` using method = \u0026#39;loess\u0026#39; and formula \u0026#39;y ~ x\u0026#39;\rThis looks nice.\n\r\rFit Grf regression forest on Friedman data\rFrom the manual: Trains a regression forest that can be used to estimate the conditional mean function mu(x) = E[Y | X = x]\nif(fullrun){\rreg.forest = regression_forest(x, y, num.trees = 2000)\rsaveRDS(reg.forest, \u0026quot;s00.rds\u0026quot;)\r} else {reg.forest \u0026lt;- readRDS(\u0026quot;s00.rds\u0026quot;)}\rdf3 \u0026lt;- CalcPredictionsGRF(x, reg.forest)\rdf3 \u0026lt;- data.frame(df3, y)\rggplot(df3, aes(x= y, y = qm)) + geom_linerange(aes(ymin = ql, ymax = qu), col = \u0026quot;grey\u0026quot;) +\rgeom_point() + geom_abline(intercept = 0, slope = 1, col = \u0026quot;red\u0026quot;, size = 1)\rThis is pretty bad compared to BART. What’s wrong here?\nFrom reference.md: GRF isn’t working well on a small dataset\nIf you observe poor performance on a dataset with a small number of examples, it may be worth trying out two changes:\n\rDisabling honesty. As noted in the section on honesty above, when honesty is enabled, the training subsample is further split in half before performing splitting. This may not leave enough information for the algorithm to determine high-quality splits.\rSkipping the variance estimate computation, by setting ci.group.size to 1 during training, then increasing sample.fraction. Because of how variance estimation is implemented, sample.fraction cannot be greater than 0.5 when it is enabled. If variance estimates are not needed, it may help to disable this computation and use a larger subsample size for training.\r\rDataset is pretty small (n=100). Maybe turn of honesty? We cannot turn off variance estimate computation, because we want the CI’s\nif(fullrun){\rreg.forest2 = regression_forest(x, y, num.trees = 2000,\rhonesty = FALSE)\rsaveRDS(reg.forest2, \u0026quot;s001.rds\u0026quot;)\r} else {reg.forest2 \u0026lt;- readRDS(\u0026quot;s001.rds\u0026quot;)}\rdf2 \u0026lt;- CalcPredictionsGRF(x, reg.forest2)\rdf2 \u0026lt;- data.frame(df2, y)\rgrfp \u0026lt;- ggplot(df2, aes(x= y, y = qm)) + geom_linerange(aes(ymin = ql, ymax = qu), col = \u0026quot;grey\u0026quot;) +\rgeom_point() + geom_smooth() +\rgeom_abline(intercept = 0, slope = 1, col = \u0026quot;red\u0026quot;, size = 1)\rgrfp\r## `geom_smooth()` using method = \u0026#39;loess\u0026#39; and formula \u0026#39;y ~ x\u0026#39;\rAh! better now. But Grf still worse than BART. We ran with 2000 trees and turned of honesty. Perhaps dataset too small? Maybe check out the sample.fraction parameter? Sample.fraction is set by default at 0.5, so only half of data is used to grow tree. OR use tune.parameters = TRUE\n\rCompare methods\rgp \u0026lt;- plot_grid(bartp, grfp)\r## `geom_smooth()` using method = \u0026#39;loess\u0026#39; and formula \u0026#39;y ~ x\u0026#39;\r## `geom_smooth()` using method = \u0026#39;loess\u0026#39; and formula \u0026#39;y ~ x\u0026#39;\rgp \r\r\rDataset 2: Simulated data from ACIC 2017\rThis is a bigger dataset, N=4302.\n\rTreatment effect \\(\\tau\\) is a function of covariates x3, x24, x14, x15\rProbability of treatment \\(\\pi\\) is a function of covariates x1, x43, x10.\rOutcome is a function of x43\rNoise is a function of x21\r\rhead(input_2017[, c(3,24,14,15)])\r## x_3 x_24 x_14 x_15\r## 1 20 white 0 2\r## 2 0 black 0 0\r## 3 0 white 0 1\r## 4 10 white 0 0\r## 5 0 black 0 0\r## 6 1 white 0 0\rCheck transformed covariates used to create simulated datasets.\n# zit hidden in package\rhead(aciccomp2017:::transformedData_2017)\r## x_1 x_3 x_10 x_14 x_15 x_21 x_24 x_43\r## 2665 -1.18689448 gt_0 leq_0 leq_0 gt_0 J E -1.0897971\r## 22 -0.04543705 leq_0 leq_0 leq_0 leq_0 J B 1.1223750\r## 2416 0.13675482 leq_0 leq_0 leq_0 gt_0 J E 0.6136700\r## 1350 -0.24062700 gt_0 leq_0 leq_0 leq_0 J E -0.2995632\r## 3850 1.02054653 leq_0 leq_0 leq_0 leq_0 I B 0.6136700\r## 4167 -1.18689448 gt_0 leq_0 leq_0 leq_0 K E -1.5961206\rSo we find that we should not take the functions in Dorie 2018 (debrief.pdf) literately. x_3 used to calculate the treatment effect is derived from x_3 in the input data. x_24 used to calculate the treatment effect is derived from x_24 in the input data. Both have become binary variables.\nTurns out that this was a feature of the 2016 ACIC and IS mentioned in the debrief.pdf\nWe pick the iid, strong signal, low noise, low confounding first. Actually from estimated PS (W.hat) it seems that every obs has probability of treatment 50%.\nparameters_2017[21,]\r## errors magnitude noise confounding\r## 21 iid 1 0 0\r# easiest?\rGrab the first replicate.\nsim \u0026lt;- dgp_2017(21, 1)\rFit BART to ACIC 2017 dataset\rNeed also counterfactual predictions. Most efficient seems to create x.test with Z reversed. This will give use a y.test as well as y.train in the output. We expect draws for both. Plotting a histogram of the difference gives us the treatment effect with uncertainty.\nFrom the MCMC draws for sigma we infer that we need to drop more “burn in” samples.\nPrepare data for BART, including x.test with treatment reversed:\n# combine x and y\ry \u0026lt;- sim$y\rx \u0026lt;- model.matrix(~. ,cbind(z = sim$z, input_2017))\r# flip z for counterfactual predictions (needed for BART)\rx.test \u0026lt;- model.matrix(~. ,cbind(z = 1 - sim$z, input_2017))\r## run BART\rfullrun \u0026lt;- 0\rif(fullrun){\rset.seed(99)\rbartFit \u0026lt;- bart(x, y, x.test, nskip = 350, ntree = 1000)\rsaveRDS(bartFit, \u0026quot;s2.rds\u0026quot;)\r} else { bartFit \u0026lt;- readRDS(\u0026quot;s2.rds\u0026quot;)}\rplot(bartFit)\rExtract individual treatment effect (ITE / CATE) plus uncertainty from bartfit\rThis means switching z from 0 to 1 and looking at difference in y + uncertainty in y.\nsource(\u0026quot;calcPosteriors.R\u0026quot;)\rsim \u0026lt;- CalcPosteriorsBART(sim, bartFit, \u0026quot;z\u0026quot;)\rsim \u0026lt;- sim %\u0026gt;% arrange(alpha)\rbartp \u0026lt;- ggplot(sim, aes(x = 1:nrow(sim), qm)) + geom_linerange(aes(ymin = ql, ymax = qu), col = \u0026quot;grey\u0026quot;) +\rgeom_point() + geom_smooth() + geom_point(aes(y = alpha), col = \u0026quot;red\u0026quot;) + ylim(-2.5, 4.5)\rbartp\r## `geom_smooth()` using method = \u0026#39;gam\u0026#39; and formula \u0026#39;y ~ s(x, bs = \u0026quot;cs\u0026quot;)\u0026#39;\r## Warning: Removed 1 rows containing missing values (geom_smooth).\rThis looks sort of ok, but still weird. Some points it gets REALLY wrong.\n\rCalculate coverage\rsim \u0026lt;- sim %\u0026gt;% mutate(in_ci = ql \u0026lt; alpha \u0026amp; qu \u0026gt; alpha) mean(sim$in_ci)\r## [1] 0.4363087\rPretty bad coverage. Look into whats going on here. Here it should be 0.9\nThe iid plot for method 2 gives coverage 0.7 (where it should be 0.95)\n\rCalculate RMSE of CATE\rsqrt(mean((sim$alpha - sim$ite)^2))\r## [1] 0.1587338\rFor All i.i.d. (averaged over 250 replicates averaged over 8 scenarios) method 2 (BART should have RMSE of CATE of 0.35-0.4)\n\r\rFit grf to ACIC 2017 dataset\rneed large num.trees for CI.\n# prep data for Grf\r# combine x and y\rsim \u0026lt;- dgp_2017(21, 1)\rY \u0026lt;- sim$y\rX \u0026lt;- model.matrix(~. ,input_2017)\rW = sim$z\r# Train a causal forest.\rfullrun \u0026lt;- 0\rif(fullrun){\rgrf.fit_alt \u0026lt;- causal_forest(X, Y, W, num.trees = 500)\rsaveRDS(grf.fit_alt, \u0026quot;s3.rds\u0026quot;)\r} else{grf.fit_alt \u0026lt;- readRDS(\u0026quot;s3.rds\u0026quot;)}\rIt appears that using 4000 trees consumes too much memory (bad_alloc)\n\rCompare predictions vs true value\rdf_sep2 \u0026lt;- CalcPredictionsGRF(X, grf.fit_alt)\rdf_sep2 \u0026lt;- data.frame(df_sep2, Y, W, TAU = sim$alpha)\rdf_sep2 \u0026lt;- df_sep2 %\u0026gt;% arrange(TAU)\rgrfp \u0026lt;- ggplot(df_sep2, aes(x = 1:nrow(df_sep2), y = qm)) +\rgeom_linerange(aes(ymin = ql, ymax = qu), col = \u0026quot;grey\u0026quot;) + geom_point() + geom_smooth() + geom_point(aes(y = TAU), col = \u0026quot;red\u0026quot;) + ylim(-2.5, 4.5)\rgrfp\r## `geom_smooth()` using method = \u0026#39;gam\u0026#39; and formula \u0026#39;y ~ s(x, bs = \u0026quot;cs\u0026quot;)\u0026#39;\rThis works ok now.\n\rCompare both methods\rgp \u0026lt;- plot_grid(bartp, grfp)\r## `geom_smooth()` using method = \u0026#39;gam\u0026#39; and formula \u0026#39;y ~ s(x, bs = \u0026quot;cs\u0026quot;)\u0026#39;\r## Warning: Removed 1 rows containing missing values (geom_smooth).\r## `geom_smooth()` using method = \u0026#39;gam\u0026#39; and formula \u0026#39;y ~ s(x, bs = \u0026quot;cs\u0026quot;)\u0026#39;\rgp\r\r\rDataset 3: simulated data used by grf example\rTHis dataset is used in the Grf manual page. Size N = 2000. Probability of treatment function of X1. Treatment effect function of X1.\n# Generate data.\rset.seed(123)\rn = 2000; p = 10\rX = matrix(rnorm(n*p), n, p)\r# treatment\rW = rbinom(n, 1, 0.4 + 0.2 * (X[,1] \u0026gt; 0))\r# outcome (parallel max)\rY = pmax(X[,1], 0) * W + X[,2] + pmin(X[,3], 0) + rnorm(n)\r# TAU is true treatment effect\rdf \u0026lt;- data.frame(X, W, Y, TAU = pmax(X[,1], 0))\rFit GRF\rDefault settings are honesty = TRUE.\n# Train a causal forest.\rif(fullrun){\rtau.forest = causal_forest(X, Y, W, num.trees = 2000)\rsaveRDS(tau.forest, \u0026quot;s4.rds\u0026quot;)\r} else {tau.forest \u0026lt;- readRDS(\u0026quot;s4.rds\u0026quot;)}\r\rOOB predictions\rFrom the GRF manual:\nGiven a test example, the GRF algorithm computes a prediction as follows:\nFor each tree, the test example is \u0026#39;pushed down\u0026#39; to determine what leaf it falls in.\rGiven this information, we create a list of neighboring training examples, weighted by how many times the example fell in the same leaf as the test example.\rA prediction is made using this weighted list of neighbors, using the relevant approach for the type of forest. In causal prediction, we calculate the treatment effect using the outcomes and treatment status of the neighbor examples.\rThose familiar with classic random forests might note that this approach differs from the way forest prediction is usually described. The traditional view is that to predict for a test example, each tree makes a prediction on that example. To make a final prediction, the tree predictions are combined in some way, for example through averaging or through ‘majority voting’. It’s worth noting that for regression forests, the GRF algorithm described above is identical this ‘ensemble’ approach, where each tree predicts by averaging the outcomes in each leaf, and predictions are combined through a weighted average.\n# Estimate treatment effects for the training data using out-of-bag prediction.\rtau.hat.oob = predict(tau.forest)\rres \u0026lt;- data.frame(df, pred = tau.hat.oob$predictions)\rggplot(res, aes(x = X1, y = pred)) + geom_point() + geom_smooth() + geom_abline(intercept = 0, slope = 1) +\rgeom_line(aes(y = TAU), col = \u0026quot;red\u0026quot;, size = 1)\r## `geom_smooth()` using method = \u0026#39;gam\u0026#39; and formula \u0026#39;y ~ s(x, bs = \u0026quot;cs\u0026quot;)\u0026#39;\r\rATE \u0026amp; ATT\r# Estimate the conditional average treatment effect on the full sample (CATE).\raverage_treatment_effect(tau.forest, target.sample = \u0026quot;all\u0026quot;)\r## estimate std.err ## 0.36664140 0.04796884\rmean(res$TAU)\r## [1] 0.4138061\r# Estimate the conditional average treatment effect on the treated sample (CATT).\r# Here, we don\u0026#39;t expect much difference between the CATE and the CATT, since\r# treatment assignment was randomized.\raverage_treatment_effect(tau.forest, target.sample = \u0026quot;treated\u0026quot;)\r## estimate std.err ## 0.45860274 0.04852209\rmean(res[res$W == 1,]$TAU)\r## [1] 0.5010723\r\rFit more trees for CI’s\r# Add confidence intervals for heterogeneous treatment effects; growing more\r# trees is now recommended.\rif(fullrun){\rtau.forest_big = causal_forest(X, Y, W, num.trees = 4000)\rsaveRDS(tau.forest_big, \u0026quot;s5.rds\u0026quot;)\r} else {tau.forest_big \u0026lt;- readRDS(\u0026quot;s5.rds\u0026quot;)}\r\rPlot CI’s\r## PM\rsource(\u0026quot;CalcPosteriors.R\u0026quot;)\rdf_res \u0026lt;- CalcPredictionsGRF(df, tau.forest_big)\rgrfp \u0026lt;- ggplot(df_res, aes(x = X1, y = qm)) + geom_linerange(aes(ymin = ql, ymax = qu), col = \u0026quot;grey\u0026quot;) +\rgeom_point() + geom_smooth() + geom_line(aes(y = TAU), col = \u0026quot;red\u0026quot;, size = 1) +\rylim(-1,3.5)\rgrfp\r## `geom_smooth()` using method = \u0026#39;gam\u0026#39; and formula \u0026#39;y ~ s(x, bs = \u0026quot;cs\u0026quot;)\u0026#39;\r\rFit BART on this dataset\rx.train \u0026lt;- model.matrix(~. ,data.frame(W, X))\rx.test \u0026lt;- model.matrix(~. ,data.frame(W = 1 - W, X))\ry.train \u0026lt;- Y\rif(fullrun){\rbartFit \u0026lt;- bart(x.train, y.train, x.test, ntree = 2000, ndpost = 1000, nskip = 100)\rsaveRDS(bartFit, \u0026quot;s10.rds\u0026quot;)\r} else {bartFit \u0026lt;- readRDS(\u0026quot;s10.rds\u0026quot;)}\rplot(bartFit)\r\rBART: Check fit and CI’s\rsource(\u0026quot;calcPosteriors.R\u0026quot;)\rsim \u0026lt;- CalcPosteriorsBART(df, bartFit, treatname = \u0026quot;W\u0026quot;)\rbartp \u0026lt;- ggplot(sim, aes(x = X1, qm)) + geom_linerange(aes(ymin = ql, ymax = qu), col = \u0026quot;grey\u0026quot;) +\rgeom_point() + geom_smooth() +\rgeom_line(aes(y = TAU), col = \u0026quot;red\u0026quot;, size = 1) + ylim(-1,3.5)\rbartp\r## `geom_smooth()` using method = \u0026#39;gam\u0026#39; and formula \u0026#39;y ~ s(x, bs = \u0026quot;cs\u0026quot;)\u0026#39;\r\rCompare\rgp \u0026lt;- plot_grid(bartp, grfp)\r## `geom_smooth()` using method = \u0026#39;gam\u0026#39; and formula \u0026#39;y ~ s(x, bs = \u0026quot;cs\u0026quot;)\u0026#39;\r## `geom_smooth()` using method = \u0026#39;gam\u0026#39; and formula \u0026#39;y ~ s(x, bs = \u0026quot;cs\u0026quot;)\u0026#39;\rgp\rHere Grf appears more accurate. Mental note: Both W and TAU function of X1.\n\r\rDataset 4: Fit separate grf forests for Y and W\rThis dataset has a complex propensity of treatment function (Exponential of X1 and X2), as well as hetergeneous treatment effect that is exponential function of X3. It has size N=4000.\n# In some examples, pre-fitting models for Y and W separately may\r# be helpful (e.g., if different models use different covariates).\r# In some applications, one may even want to get Y.hat and W.hat\r# using a completely different method (e.g., boosting).\rset.seed(123)\r# Generate new data.\rn = 4000; p = 20\rX = matrix(rnorm(n * p), n, p)\rTAU = 1 / (1 + exp(-X[, 3]))\rW = rbinom(n ,1, 1 / (1 + exp(-X[, 1] - X[, 2])))\rY = pmax(X[, 2] + X[, 3], 0) + rowMeans(X[, 4:6]) / 2 + W * TAU + rnorm(n)\rdf_sep4 \u0026lt;- data.frame(X, TAU, W, Y)\rGrf two-step: First fit model for W (PS)\rRegression forest to predict W from X. This is a propensity score.\nif(fullrun){\rforest.W = regression_forest(X, W, tune.parameters = TRUE, num.trees = 2000)\rsaveRDS(forest.W, \u0026quot;s6.rds\u0026quot;)\r} else {forest.W \u0026lt;- readRDS(\u0026quot;s6.rds\u0026quot;)}\rW.hat = predict(forest.W)$predictions\rGrf:Then Fit model for Y, selecting covariates\rThis predict Y from X, ignoring treatment.\nif(fullrun){\rforest.Y = regression_forest(X, Y, tune.parameters = TRUE, num.trees = 2000)\rsaveRDS(forest.Y, \u0026quot;s7.rds\u0026quot;)\r} else {forest.Y \u0026lt;- readRDS(\u0026quot;s7.rds\u0026quot;)}\rY.hat = predict(forest.Y)$predictions\r\rGrf:Select variables that predict Y.\rforest.Y.varimp = variable_importance(forest.Y)\r# Note: Forests may have a hard time when trained on very few variables\r# (e.g., ncol(X) = 1, 2, or 3). We recommend not being too aggressive\r# in selection.\rselected.vars = which(forest.Y.varimp / mean(forest.Y.varimp) \u0026gt; 0.2)\rThis selects five variables of 20. Indeed these are the variables that were used to simulate Y.\n\rGrf: Finally, Fit causal forest using PS and selected covariates\rif(fullrun){\rtau.forest2 = causal_forest(X[, selected.vars], Y, W,\rW.hat = W.hat, Y.hat = Y.hat,\rtune.parameters = TRUE, num.trees = 4000)\rsaveRDS(tau.forest2, \u0026quot;s8.rds\u0026quot;)\r} else {tau.forest2 \u0026lt;- readRDS(\u0026quot;s8.rds\u0026quot;)}\r\rGrf: Check fit and CI’s\rdf_sep2 \u0026lt;- CalcPredictionsGRF(df_sep4, tau.forest2)\rgrfp \u0026lt;- ggplot(df_sep2, aes(x = X3, y = qm)) +\rgeom_linerange(aes(ymin = ql, ymax = qu), col = \u0026quot;grey\u0026quot;) + geom_point() + geom_smooth() + geom_line(aes(y = TAU), col = \u0026quot;red\u0026quot;, size = 1) + ylim(-0.7,2)\rgrfp\r## `geom_smooth()` using method = \u0026#39;gam\u0026#39; and formula \u0026#39;y ~ s(x, bs = \u0026quot;cs\u0026quot;)\u0026#39;\rThis looks ok.\n\r\rFit BART on this dataset\rx.train \u0026lt;- model.matrix(~. ,data.frame(W, X))\rx.test \u0026lt;- model.matrix(~. ,data.frame(W = 1 - W, X))\ry.train \u0026lt;- Y\rif(fullrun){\rbartFit \u0026lt;- bart(x.train, y.train, x.test, ntree = 4000)\rsaveRDS(bartFit, \u0026quot;s9.rds\u0026quot;)\r} else {bartFit \u0026lt;- readRDS(\u0026quot;s9.rds\u0026quot;)}\rplot(bartFit)\r\rBART: Check fit and CI’s\rsource(\u0026quot;calcPosteriors.R\u0026quot;)\rsim \u0026lt;- CalcPosteriorsBART(df_sep4, bartFit, treatname = \u0026quot;W\u0026quot;)\rbartp \u0026lt;- ggplot(sim, aes(x = X3, qm)) + geom_linerange(aes(ymin = ql, ymax = qu), col = \u0026quot;grey\u0026quot;) +\rgeom_point() + geom_smooth() +\rgeom_line(aes(y = TAU), col = \u0026quot;red\u0026quot;, size = 1) + ylim(-0.7,2)\rbartp\r## `geom_smooth()` using method = \u0026#39;gam\u0026#39; and formula \u0026#39;y ~ s(x, bs = \u0026quot;cs\u0026quot;)\u0026#39;\r\rCompare BART and grf\rgp \u0026lt;- plot_grid(bartp, grfp)\r## `geom_smooth()` using method = \u0026#39;gam\u0026#39; and formula \u0026#39;y ~ s(x, bs = \u0026quot;cs\u0026quot;)\u0026#39;\r## `geom_smooth()` using method = \u0026#39;gam\u0026#39; and formula \u0026#39;y ~ s(x, bs = \u0026quot;cs\u0026quot;)\u0026#39;\rgp\rVery similar results. BART appears slightly more accurate, especially for low values of X3.\n\r\r","date":1544832000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544832000,"objectID":"5e61a0767e44f41cc719faa8702e5dae","permalink":"https://gsverhoeven.github.io/post/bart_vs_grf/bart-vs-grf-showdown/","publishdate":"2018-12-15T00:00:00Z","relpermalink":"/post/bart_vs_grf/bart-vs-grf-showdown/","section":"post","summary":"In this post, we test both `Bayesian Additive Regression Trees (BART)` and `Causal forests (grf)` on four simulated datasets of increasing complexity. May the best method win!","tags":["causal inference, BART, grf"],"title":"BART vs Causal forests showdown","type":"post"},{"authors":null,"categories":["R"],"content":"\rThe idea is that comparing the predictions of an RF model with the predictions of an OLS model can inform us in what ways the OLS model fails to capture all non-linearities and interactions between the predictors. Subsequently, using partial dependence plots of the RF model can guide the modelling of the non-linearities in the OLS model. After this step, the discrepancies between the RF predictions and the OLS predictions should be caused by non-modeled interactions. Using an RF to predict the discrepancy itself can then be used to discover which predictors are involved in these interactions. We test this method on the classic Boston Housing dataset to predict median house values (medv). We indeed recover interactions that, as it turns, have already been found and documented in the literature.\nLoad packages\rrm(list=ls())\r#library(randomForest)\r#library(party)\rlibrary(ranger)\rlibrary(data.table)\rlibrary(ggplot2)\rlibrary(MASS)\rrdunif \u0026lt;- function(n,k) sample(1:k, n, replace = T)\r\rStep 1: Run a RF on the Boston Housing set\rmy_ranger \u0026lt;- ranger(medv ~ ., data = Boston,\rimportance = \u0026quot;permutation\u0026quot;, num.trees = 500,\rmtry = 5, replace = TRUE)\rExtract the permutation importance measure.\nmyres_tmp \u0026lt;- ranger::importance(my_ranger);\rmyres \u0026lt;- cbind(names(myres_tmp), myres_tmp, i = 1)\r#my_rownames \u0026lt;- row.names(myres)\rmyres \u0026lt;- data.table(myres)\rsetnames(myres, \u0026quot;V1\u0026quot;, \u0026quot;varname\u0026quot;)\rsetnames(myres, \u0026quot;myres_tmp\u0026quot;, \u0026quot;MeanDecreaseAccuracy\u0026quot;)\rmyres \u0026lt;- myres[, varname := as.factor(varname)]\rmyres \u0026lt;- myres[, MeanDecreaseAccuracy := as.numeric(MeanDecreaseAccuracy)]\rmyres \u0026lt;- myres[, i := as.integer(i)]\rggplot(myres, aes(x = reorder(factor(varname), MeanDecreaseAccuracy), y = MeanDecreaseAccuracy)) + geom_point() + coord_flip()\r\rFit an OLS to the Boston Housing\rmy_glm \u0026lt;- glm(medv ~., data = Boston, family = \u0026quot;gaussian\u0026quot;)\r\rCompare predictions of both models\rpred_RF \u0026lt;- predict(my_ranger, data = Boston)\r#pred_RF$predictions\rpred_GLM \u0026lt;- predict(my_glm, data = Boston)\rplot(pred_RF$predictions, pred_GLM)\rabline(0, 1)\r# Run a RF on the discrepancy\nDiscrepancy is defined as the difference between the predictions of both models for each observation.\npred_diff \u0026lt;- pred_RF$predictions - pred_GLM\rmy_ranger_diff \u0026lt;- ranger(Ydiff ~ . - medv, data = data.table(Ydiff = pred_diff, Boston),\rimportance = \u0026quot;permutation\u0026quot;, num.trees = 500,\rmtry = 5, replace = TRUE)\rmy_ranger_diff\r## Ranger result\r## ## Call:\r## ranger(Ydiff ~ . - medv, data = data.table(Ydiff = pred_diff, Boston), importance = \u0026quot;permutation\u0026quot;, num.trees = 500, mtry = 5, replace = TRUE) ## ## Type: Regression ## Number of trees: 500 ## Sample size: 506 ## Number of independent variables: 13 ## Mtry: 5 ## Target node size: 5 ## Variable importance mode: permutation ## Splitrule: variance ## OOB prediction error (MSE): 5.215528 ## R squared (OOB): 0.6607762\rIt turns out the RF can “explain” 67% of these discrepancies.\nmyres_tmp \u0026lt;- ranger::importance(my_ranger_diff)\rmyres \u0026lt;- cbind(names(myres_tmp), myres_tmp, i = 1)\r#my_rownames \u0026lt;- row.names(myres)\rmyres \u0026lt;- data.table(myres)\rsetnames(myres, \u0026quot;V1\u0026quot;, \u0026quot;varname\u0026quot;)\rsetnames(myres, \u0026quot;myres_tmp\u0026quot;, \u0026quot;MeanDecreaseAccuracy\u0026quot;)\rmyres \u0026lt;- myres[, varname := as.factor(varname)]\rmyres \u0026lt;- myres[, MeanDecreaseAccuracy := as.numeric(MeanDecreaseAccuracy)]\rmyres \u0026lt;- myres[, i := as.integer(i)]\rggplot(myres, aes(x = reorder(factor(varname), MeanDecreaseAccuracy), y = MeanDecreaseAccuracy)) + geom_point() + coord_flip()\rIt turns out that rm and lstat are the variables that best predict the discrepancy.\nmy_glm_int \u0026lt;- glm(medv ~. + rm:lstat, data = Boston, family = \u0026quot;gaussian\u0026quot;)\rsummary(my_glm_int)\r## ## Call:\r## glm(formula = medv ~ . + rm:lstat, family = \u0026quot;gaussian\u0026quot;, data = Boston)\r## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -21.5738 -2.3319 -0.3584 1.8149 27.9558 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 6.073638 5.038175 1.206 0.228582 ## crim -0.157100 0.028808 -5.453 7.85e-08 ***\r## zn 0.027199 0.012020 2.263 0.024083 * ## indus 0.052272 0.053475 0.978 0.328798 ## chas 2.051584 0.750060 2.735 0.006459 ** ## nox -15.051627 3.324807 -4.527 7.51e-06 ***\r## rm 7.958907 0.488520 16.292 \u0026lt; 2e-16 ***\r## age 0.013466 0.011518 1.169 0.242918 ## dis -1.120269 0.175498 -6.383 4.02e-10 ***\r## rad 0.320355 0.057641 5.558 4.49e-08 ***\r## tax -0.011968 0.003267 -3.664 0.000276 ***\r## ptratio -0.721302 0.115093 -6.267 8.06e-10 ***\r## black 0.003985 0.002371 1.681 0.093385 . ## lstat 1.844883 0.191833 9.617 \u0026lt; 2e-16 ***\r## rm:lstat -0.418259 0.032955 -12.692 \u0026lt; 2e-16 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## (Dispersion parameter for gaussian family taken to be 16.98987)\r## ## Null deviance: 42716 on 505 degrees of freedom\r## Residual deviance: 8342 on 491 degrees of freedom\r## AIC: 2886\r## ## Number of Fisher Scoring iterations: 2\rThe interaction we have added is indeed highly significant.\nCompare approximate out-of-sample prediction accuracy using AIC:\nAIC(my_glm)\r## [1] 3027.609\rAIC(my_glm_int)\r## [1] 2886.043\rIndeed, the addition of the interaction greatly increases the prediction accuracy.\n\rRepeat this process\rpred_RF \u0026lt;- predict(my_ranger, data = Boston)\r#pred_RF$predictions\rpred_GLM \u0026lt;- predict(my_glm_int, data = Boston)\rplot(pred_RF$predictions, pred_GLM)\rabline(0, 1)\rpred_diff \u0026lt;- pred_RF$predictions - pred_GLM\rmy_ranger_diff2 \u0026lt;- ranger(Ydiff ~ . - medv, data = data.table(Ydiff = pred_diff, Boston),\rimportance = \u0026quot;permutation\u0026quot;, num.trees = 500,\rmtry = 5, replace = TRUE)\rmy_ranger_diff2\r## Ranger result\r## ## Call:\r## ranger(Ydiff ~ . - medv, data = data.table(Ydiff = pred_diff, Boston), importance = \u0026quot;permutation\u0026quot;, num.trees = 500, mtry = 5, replace = TRUE) ## ## Type: Regression ## Number of trees: 500 ## Sample size: 506 ## Number of independent variables: 13 ## Mtry: 5 ## Target node size: 5 ## Variable importance mode: permutation ## Splitrule: variance ## OOB prediction error (MSE): 5.585358 ## R squared (OOB): 0.438492\rmyres_tmp \u0026lt;- ranger::importance(my_ranger_diff2)\rmyres \u0026lt;- cbind(names(myres_tmp), myres_tmp, i = 1)\r#my_rownames \u0026lt;- row.names(myres)\rmyres \u0026lt;- data.table(myres)\rsetnames(myres, \u0026quot;V1\u0026quot;, \u0026quot;varname\u0026quot;)\rsetnames(myres, \u0026quot;myres_tmp\u0026quot;, \u0026quot;MeanDecreaseAccuracy\u0026quot;)\rmyres \u0026lt;- myres[, varname := as.factor(varname)]\rmyres \u0026lt;- myres[, MeanDecreaseAccuracy := as.numeric(MeanDecreaseAccuracy)]\rmyres \u0026lt;- myres[, i := as.integer(i)]\rggplot(myres, aes(x = reorder(factor(varname), MeanDecreaseAccuracy), y = MeanDecreaseAccuracy)) + geom_point() + coord_flip()\rNow the variables that best predict the discrepancy are lstat and dis. Add these two variables as an interaction.\nmy_glm_int2 \u0026lt;- glm(medv ~. + rm:lstat + lstat:dis, data = Boston, family = \u0026quot;gaussian\u0026quot;)\rsummary(my_glm_int2)\r## ## Call:\r## glm(formula = medv ~ . + rm:lstat + lstat:dis, family = \u0026quot;gaussian\u0026quot;, ## data = Boston)\r## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -23.3918 -2.2997 -0.4077 1.6475 27.6766 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 1.552991 5.107295 0.304 0.761201 ## crim -0.139370 0.028788 -4.841 1.73e-06 ***\r## zn 0.042984 0.012550 3.425 0.000667 ***\r## indus 0.066690 0.052878 1.261 0.207834 ## chas 1.760779 0.743688 2.368 0.018290 * ## nox -11.544280 3.404577 -3.391 0.000753 ***\r## rm 8.640503 0.513593 16.824 \u0026lt; 2e-16 ***\r## age -0.002127 0.012067 -0.176 0.860140 ## dis -1.904982 0.268056 -7.107 4.22e-12 ***\r## rad 0.304689 0.057000 5.345 1.39e-07 ***\r## tax -0.011220 0.003228 -3.476 0.000554 ***\r## ptratio -0.641380 0.115418 -5.557 4.51e-08 ***\r## black 0.003756 0.002339 1.606 0.108924 ## lstat 1.925223 0.190368 10.113 \u0026lt; 2e-16 ***\r## rm:lstat -0.466947 0.034897 -13.381 \u0026lt; 2e-16 ***\r## dis:lstat 0.076716 0.020009 3.834 0.000143 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## (Dispersion parameter for gaussian family taken to be 16.52869)\r## ## Null deviance: 42716.3 on 505 degrees of freedom\r## Residual deviance: 8099.1 on 490 degrees of freedom\r## AIC: 2873.1\r## ## Number of Fisher Scoring iterations: 2\rAIC(my_glm_int2)\r## [1] 2873.087\rAIC(my_glm_int)\r## [1] 2886.043\rWe conclude that the second interaction also results in significant model improvement.\n\rA more ambitious goal: Try and improve Harrison \u0026amp; Rubinfeld’s model formula for Boston housing\rSo far, we assumed that all relationships are linear. Harrison and Rubinfeld have created a model without interactions, but with transformations to correct for skewness, heteroskedasticity etc. Let’s see if we can improve upon this model equation by applying our method to search for interactions. Their formula predicts log(medv).\n# Harrison and Rubinfeld (1978) model\rmy_glm_hr \u0026lt;- glm(log(medv) ~ I(rm^2) + age + log(dis) + log(rad) + tax + ptratio + black + I(black^2) + log(lstat) + crim + zn + indus + chas + I(nox^2), data = Boston, family = \u0026quot;gaussian\u0026quot;)\rsummary(my_glm_hr)\r## ## Call:\r## glm(formula = log(medv) ~ I(rm^2) + age + log(dis) + log(rad) + ## tax + ptratio + black + I(black^2) + log(lstat) + crim + ## zn + indus + chas + I(nox^2), family = \u0026quot;gaussian\u0026quot;, data = Boston)\r## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.73091 -0.09274 -0.00710 0.09800 0.78607 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 4.474e+00 1.579e-01 28.343 \u0026lt; 2e-16 ***\r## I(rm^2) 6.634e-03 1.313e-03 5.053 6.15e-07 ***\r## age 3.491e-05 5.245e-04 0.067 0.946950 ## log(dis) -1.927e-01 3.325e-02 -5.796 1.22e-08 ***\r## log(rad) 9.613e-02 1.905e-02 5.047 6.35e-07 ***\r## tax -4.295e-04 1.222e-04 -3.515 0.000481 ***\r## ptratio -2.977e-02 5.024e-03 -5.926 5.85e-09 ***\r## black 1.520e-03 5.068e-04 3.000 0.002833 ** ## I(black^2) -2.597e-06 1.114e-06 -2.331 0.020153 * ## log(lstat) -3.695e-01 2.491e-02 -14.833 \u0026lt; 2e-16 ***\r## crim -1.157e-02 1.246e-03 -9.286 \u0026lt; 2e-16 ***\r## zn 7.257e-05 5.034e-04 0.144 0.885430 ## indus -1.943e-04 2.360e-03 -0.082 0.934424 ## chas 9.180e-02 3.305e-02 2.777 0.005690 ** ## I(nox^2) -6.566e-01 1.129e-01 -5.815 1.09e-08 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## (Dispersion parameter for gaussian family taken to be 0.03299176)\r## ## Null deviance: 84.376 on 505 degrees of freedom\r## Residual deviance: 16.199 on 491 degrees of freedom\r## AIC: -273.48\r## ## Number of Fisher Scoring iterations: 2\rmy_ranger_log \u0026lt;- ranger(log(medv) ~ ., data = Boston,\rimportance = \u0026quot;permutation\u0026quot;, num.trees = 500,\rmtry = 5, replace = TRUE)\rpred_RF \u0026lt;- predict(my_ranger_log, data = Boston)\r#pred_RF$predictions\rpred_GLM \u0026lt;- predict(my_glm_hr, data = Boston)\rplot(pred_RF$predictions, pred_GLM)\rabline(0, 1)\rFor low predicted values both models differ in a systematic way. This suggests that there exists a remaining pattern that is picked up by RF but not by the OLS model.\npred_diff \u0026lt;- pred_RF$predictions - pred_GLM\rmy_ranger_log_diff \u0026lt;- ranger(Ydiff ~ . - medv, data = data.table(Ydiff = pred_diff, Boston),\rimportance = \u0026quot;permutation\u0026quot;, num.trees = 500,\rmtry = 5, replace = TRUE)\rmy_ranger_log_diff\r## Ranger result\r## ## Call:\r## ranger(Ydiff ~ . - medv, data = data.table(Ydiff = pred_diff, Boston), importance = \u0026quot;permutation\u0026quot;, num.trees = 500, mtry = 5, replace = TRUE) ## ## Type: Regression ## Number of trees: 500 ## Sample size: 506 ## Number of independent variables: 13 ## Mtry: 5 ## Target node size: 5 ## Variable importance mode: permutation ## Splitrule: variance ## OOB prediction error (MSE): 0.009248139 ## R squared (OOB): 0.5271383\rThe RF indicates that 54% of the discrepancy can be “explained” by RF.\nmyres_tmp \u0026lt;- ranger::importance(my_ranger_log_diff)\rmyres \u0026lt;- cbind(names(myres_tmp), myres_tmp, i = 1)\r#my_rownames \u0026lt;- row.names(myres)\rmyres \u0026lt;- data.table(myres)\rsetnames(myres, \u0026quot;V1\u0026quot;, \u0026quot;varname\u0026quot;)\rsetnames(myres, \u0026quot;myres_tmp\u0026quot;, \u0026quot;MeanDecreaseAccuracy\u0026quot;)\rmyres \u0026lt;- myres[, varname := as.factor(varname)]\rmyres \u0026lt;- myres[, MeanDecreaseAccuracy := as.numeric(MeanDecreaseAccuracy)]\rmyres \u0026lt;- myres[, i := as.integer(i)]\rggplot(myres, aes(x = reorder(factor(varname), MeanDecreaseAccuracy), y = MeanDecreaseAccuracy)) + geom_point() + coord_flip()\rAdd the top 2 vars as an interaction to their model equation.\nmy_glm_hr_int \u0026lt;- glm(log(medv) ~ I(rm^2) + age + log(dis) + log(rad) + tax + ptratio + black + I(black^2) + log(lstat) + crim + zn + indus + chas + I(nox^2) +\rlstat:nox, data = Boston, family = \u0026quot;gaussian\u0026quot;)\rsummary(my_glm_hr_int)\r## ## Call:\r## glm(formula = log(medv) ~ I(rm^2) + age + log(dis) + log(rad) + ## tax + ptratio + black + I(black^2) + log(lstat) + crim + ## zn + indus + chas + I(nox^2) + lstat:nox, family = \u0026quot;gaussian\u0026quot;, ## data = Boston)\r## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.70340 -0.09274 -0.00665 0.10068 0.75004 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 4.243e+00 1.613e-01 26.304 \u0026lt; 2e-16 ***\r## I(rm^2) 7.053e-03 1.286e-03 5.484 6.66e-08 ***\r## age -3.146e-04 5.174e-04 -0.608 0.54354 ## log(dis) -2.254e-01 3.317e-02 -6.795 3.15e-11 ***\r## log(rad) 9.829e-02 1.862e-02 5.278 1.96e-07 ***\r## tax -4.589e-04 1.196e-04 -3.838 0.00014 ***\r## ptratio -2.990e-02 4.910e-03 -6.089 2.30e-09 ***\r## black 1.445e-03 4.955e-04 2.917 0.00370 ** ## I(black^2) -2.470e-06 1.089e-06 -2.268 0.02376 * ## log(lstat) -2.143e-01 3.989e-02 -5.373 1.20e-07 ***\r## crim -1.046e-02 1.238e-03 -8.448 3.40e-16 ***\r## zn 7.309e-04 5.099e-04 1.434 0.15234 ## indus -8.166e-05 2.307e-03 -0.035 0.97178 ## chas 8.746e-02 3.231e-02 2.707 0.00704 ** ## I(nox^2) -3.618e-01 1.256e-01 -2.880 0.00415 ** ## lstat:nox -2.367e-02 4.819e-03 -4.911 1.24e-06 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## (Dispersion parameter for gaussian family taken to be 0.03150809)\r## ## Null deviance: 84.376 on 505 degrees of freedom\r## Residual deviance: 15.439 on 490 degrees of freedom\r## AIC: -295.79\r## ## Number of Fisher Scoring iterations: 2\rAIC(my_glm_hr)\r## [1] -273.4788\rAIC(my_glm_hr_int)\r## [1] -295.7931\rThis results in a significant improvement!\n\rRepeat this procedure\rpred_RF \u0026lt;- predict(my_ranger_log, data = Boston)\r#pred_RF$predictions\rpred_GLM \u0026lt;- predict(my_glm_hr_int, data = Boston)\rplot(pred_RF$predictions, pred_GLM)\rabline(0, 1)\rpred_diff \u0026lt;- pred_RF$predictions - pred_GLM\rmy_ranger_log_diff2 \u0026lt;- ranger(Ydiff ~ . - medv, data = data.table(Ydiff = pred_diff, Boston),\rimportance = \u0026quot;permutation\u0026quot;, num.trees = 500,\rmtry = 5, replace = TRUE)\rmy_ranger_log_diff2\r## Ranger result\r## ## Call:\r## ranger(Ydiff ~ . - medv, data = data.table(Ydiff = pred_diff, Boston), importance = \u0026quot;permutation\u0026quot;, num.trees = 500, mtry = 5, replace = TRUE) ## ## Type: Regression ## Number of trees: 500 ## Sample size: 506 ## Number of independent variables: 13 ## Mtry: 5 ## Target node size: 5 ## Variable importance mode: permutation ## Splitrule: variance ## OOB prediction error (MSE): 0.008679761 ## R squared (OOB): 0.5241287\rmyres_tmp \u0026lt;- ranger::importance(my_ranger_log_diff2)\rmyres \u0026lt;- cbind(names(myres_tmp), myres_tmp, i = 1)\r#my_rownames \u0026lt;- row.names(myres)\rmyres \u0026lt;- data.table(myres)\rsetnames(myres, \u0026quot;V1\u0026quot;, \u0026quot;varname\u0026quot;)\rsetnames(myres, \u0026quot;myres_tmp\u0026quot;, \u0026quot;MeanDecreaseAccuracy\u0026quot;)\rmyres \u0026lt;- myres[, varname := as.factor(varname)]\rmyres \u0026lt;- myres[, MeanDecreaseAccuracy := as.numeric(MeanDecreaseAccuracy)]\rmyres \u0026lt;- myres[, i := as.integer(i)]\rggplot(myres, aes(x = reorder(factor(varname), MeanDecreaseAccuracy), y = MeanDecreaseAccuracy)) + geom_point() + coord_flip()\rNow we add lstat and dis as an interaction.\nmy_glm_hr_int2 \u0026lt;- glm(log(medv) ~ I(rm^2) + age + log(dis) + log(rad) + tax + ptratio + black + I(black^2) + log(lstat) + crim + zn + indus + chas + I(nox^2) +\rlstat:nox + lstat:dis, data = Boston, family = \u0026quot;gaussian\u0026quot;)\rsummary(my_glm_hr_int2)\r## ## Call:\r## glm(formula = log(medv) ~ I(rm^2) + age + log(dis) + log(rad) + ## tax + ptratio + black + I(black^2) + log(lstat) + crim + ## zn + indus + chas + I(nox^2) + lstat:nox + lstat:dis, family = \u0026quot;gaussian\u0026quot;, ## data = Boston)\r## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.70136 -0.08746 -0.00589 0.08857 0.76349 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 4.535e+00 1.712e-01 26.481 \u0026lt; 2e-16 ***\r## I(rm^2) 7.498e-03 1.266e-03 5.924 5.94e-09 ***\r## age -1.262e-03 5.504e-04 -2.293 0.02226 * ## log(dis) -4.065e-01 5.203e-02 -7.813 3.43e-14 ***\r## log(rad) 9.668e-02 1.828e-02 5.290 1.85e-07 ***\r## tax -4.622e-04 1.173e-04 -3.940 9.35e-05 ***\r## ptratio -2.640e-02 4.881e-03 -5.409 9.93e-08 ***\r## black 1.313e-03 4.871e-04 2.696 0.00727 ** ## I(black^2) -2.172e-06 1.071e-06 -2.029 0.04303 * ## log(lstat) -3.181e-01 4.553e-02 -6.987 9.23e-12 ***\r## crim -1.049e-02 1.215e-03 -8.635 \u0026lt; 2e-16 ***\r## zn 9.078e-04 5.019e-04 1.809 0.07108 . ## indus -2.733e-04 2.264e-03 -0.121 0.90395 ## chas 7.166e-02 3.191e-02 2.246 0.02515 * ## I(nox^2) -2.569e-01 1.255e-01 -2.048 0.04113 * ## lstat:nox -2.729e-02 4.798e-03 -5.689 2.21e-08 ***\r## lstat:dis 3.906e-03 8.754e-04 4.462 1.01e-05 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## (Dispersion parameter for gaussian family taken to be 0.03033711)\r## ## Null deviance: 84.376 on 505 degrees of freedom\r## Residual deviance: 14.835 on 489 degrees of freedom\r## AIC: -313.99\r## ## Number of Fisher Scoring iterations: 2\rAIC(my_glm_hr_int2)\r## [1] -313.9904\rAIC(my_glm_hr_int)\r## [1] -295.7931\rAnd again we find an improvement in model fit.\n\rHave these interactions already been reported on in the literature?\rTom Minka reports on his website an analysis of interactions in the Boston Housing set:\n(http://alumni.media.mit.edu/~tpminka/courses/36-350.2001/lectures/day30/) \u0026gt; summary(fit3) Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) -227.5485 49.2363 -4.622 4.87e-06 *** lstat 50.8553 20.3184 2.503 0.012639 * rm 38.1245 7.0987 5.371 1.21e-07 *** dis -16.8163 2.9174 -5.764 1.45e-08 *** ptratio 14.9592 2.5847 5.788 1.27e-08 *** lstat:rm -6.8143 3.1209 -2.183 0.029475 * lstat:dis 4.8736 1.3940 3.496 0.000514 *** lstat:ptratio -3.3209 1.0345 -3.210 0.001412 ** rm:dis 2.0295 0.4435 4.576 5.99e-06 *** rm:ptratio -1.9911 0.3757 -5.299 1.76e-07 *** lstat:rm:dis -0.5216 0.2242 -2.327 0.020364 * lstat:rm:ptratio 0.3368 0.1588 2.121 0.034423 *\nRob mcCulloch, using BART (bayesian additive regression trees) also examines interactions in the Boston Housing data. There the co-occurence within trees is used to discover interactions:\nThe second, interaction detection, uncovers which pairs of variables interact in analogous fashion by keeping track of the percentage of trees in the sum in which both variables occur. This exploits the fact that a sum-of-trees model captures an interaction between xi and xj by using them both for splitting rules in the same tree.\nhttp://www.rob-mcculloch.org/some_papers_and_talks/papers/working/cgm_as.pdf\n\r\rConclusion\rWe conclude that this appears a fruitfull approach to at least discovering where a regression model can be improved.\n\r","date":1504224000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504224000,"objectID":"f6e1d7cb765b378e9f12194e8cd7fa31","permalink":"https://gsverhoeven.github.io/post/interaction_detection/interaction-detection/","publishdate":"2017-09-01T00:00:00Z","relpermalink":"/post/interaction_detection/interaction-detection/","section":"post","summary":"In this post, I explore how we can improve a parametric regression model by comparing its predictions to those of a Random Forest model. This might informs us in what ways the OLS model fails to capture all non-linearities and interactions between the predictors.","tags":["interaction detection, random forest, ranger"],"title":"Improving a parametric regression model using machine learning","type":"post"}]