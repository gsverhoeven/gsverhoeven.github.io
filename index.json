[{"authors":null,"categories":null,"content":"\rLoad packages\r# library(devtools)\r#devtools::install_github(\u0026quot;vdorie/dbarts\u0026quot;)\rlibrary(dbarts)\rlibrary(ggplot2)\rlibrary(tidyverse)\r## -- Attaching packages ------------------------------------------------------------- tidyverse 1.2.1 --\r## v tibble 2.0.1 v purrr 0.2.5\r## v tidyr 0.8.2 v dplyr 0.7.8\r## v readr 1.3.1 v stringr 1.3.1\r## v tibble 2.0.1 v forcats 0.3.0\r## -- Conflicts ---------------------------------------------------------------- tidyverse_conflicts() --\r## x dplyr::filter() masks stats::filter()\r## x dplyr::lag() masks stats::lag()\rlibrary(grf)\r#devtools::install_github(\u0026quot;vdorie/aciccomp/2017\u0026quot;)\rlibrary(aciccomp2017)\rlibrary(cowplot)\r## ## Attaching package: \u0026#39;cowplot\u0026#39;\r## The following object is masked from \u0026#39;package:ggplot2\u0026#39;:\r## ## ggsave\rsource(\u0026quot;calcPosteriors.R\u0026quot;)\rfullrun \u0026lt;- 0\r\rDataset 1: Simulated dataset from Friedman MARS paper\rThis is not a causal problem but a prediction problem.\n## y = f(x) + epsilon , epsilon ~ N(0, sigma)\r## x consists of 10 variables, only first 5 matter\rf \u0026lt;- function(x) {\r10 * sin(pi * x[,1] * x[,2]) + 20 * (x[,3] - 0.5)^2 +\r10 * x[,4] + 5 * x[,5]\r}\rset.seed(99)\rsigma \u0026lt;- 1.0\rn \u0026lt;- 100\rx \u0026lt;- matrix(runif(n * 10), n, 10)\rEy \u0026lt;- f(x)\ry \u0026lt;- rnorm(n, Ey, sigma)\rdf \u0026lt;- data.frame(x, y, y_true = Ey)\rfit BART model on simulated Friedman data\rif(fullrun){\r## run BART\rset.seed(99)\rbartFit \u0026lt;- bart(x, y)\rsaveRDS(bartFit, \u0026quot;s1.rds\u0026quot;)\r} else { bartFit \u0026lt;- readRDS(\u0026quot;s1.rds\u0026quot;)}\rplot(bartFit)\rMCMC or sigma looks ok.\ncompare BART fit to true values\rdf2 \u0026lt;- data.frame(df, ql = apply(bartFit$yhat.train, length(dim(bartFit$yhat.train)), quantile,probs=0.05),\rqm = apply(bartFit$yhat.train, length(dim(bartFit$yhat.train)), quantile,probs=.5),\rqu \u0026lt;- apply(bartFit$yhat.train, length(dim(bartFit$yhat.train)), quantile,probs=0.95)\r)\rbartp \u0026lt;- ggplot(df2, aes(x= y, y = qm)) + geom_linerange(aes(ymin = ql, ymax = qu), col = \u0026quot;grey\u0026quot;) +\rgeom_point() + geom_smooth() +\rgeom_abline(intercept = 0, slope = 1, col = \u0026quot;red\u0026quot;, size = 1)\rbartp\r## `geom_smooth()` using method = \u0026#39;loess\u0026#39; and formula \u0026#39;y ~ x\u0026#39;\rThis looks nice.\n\r\rFit Grf regression forest on Friedman data\rFrom the manual: Trains a regression forest that can be used to estimate the conditional mean function mu(x) = E[Y | X = x]\nif(fullrun){\rreg.forest = regression_forest(x, y, num.trees = 2000)\rsaveRDS(reg.forest, \u0026quot;s00.rds\u0026quot;)\r} else {reg.forest \u0026lt;- readRDS(\u0026quot;s00.rds\u0026quot;)}\rdf3 \u0026lt;- CalcPredictionsGRF(x, reg.forest)\rdf3 \u0026lt;- data.frame(df3, y)\rggplot(df3, aes(x= y, y = qm)) + geom_linerange(aes(ymin = ql, ymax = qu), col = \u0026quot;grey\u0026quot;) +\rgeom_point() + geom_abline(intercept = 0, slope = 1, col = \u0026quot;red\u0026quot;, size = 1)\rThis is pretty bad compared to BART. What’s wrong here?\nFrom reference.md: GRF isn’t working well on a small dataset\nIf you observe poor performance on a dataset with a small number of examples, it may be worth trying out two changes:\n\rDisabling honesty. As noted in the section on honesty above, when honesty is enabled, the training subsample is further split in half before performing splitting. This may not leave enough information for the algorithm to determine high-quality splits.\rSkipping the variance estimate computation, by setting ci.group.size to 1 during training, then increasing sample.fraction. Because of how variance estimation is implemented, sample.fraction cannot be greater than 0.5 when it is enabled. If variance estimates are not needed, it may help to disable this computation and use a larger subsample size for training.\r\rDataset is pretty small (n=100). Maybe turn of honesty? We cannot turn off variance estimate computation, because we want the CI’s\nif(fullrun){\rreg.forest2 = regression_forest(x, y, num.trees = 2000,\rhonesty = FALSE)\rsaveRDS(reg.forest2, \u0026quot;s001.rds\u0026quot;)\r} else {reg.forest2 \u0026lt;- readRDS(\u0026quot;s001.rds\u0026quot;)}\rdf2 \u0026lt;- CalcPredictionsGRF(x, reg.forest2)\rdf2 \u0026lt;- data.frame(df2, y)\rgrfp \u0026lt;- ggplot(df2, aes(x= y, y = qm)) + geom_linerange(aes(ymin = ql, ymax = qu), col = \u0026quot;grey\u0026quot;) +\rgeom_point() + geom_smooth() +\rgeom_abline(intercept = 0, slope = 1, col = \u0026quot;red\u0026quot;, size = 1)\rgrfp\r## `geom_smooth()` using method = \u0026#39;loess\u0026#39; and formula \u0026#39;y ~ x\u0026#39;\rAh! better now. But Grf still worse than BART. We ran with 2000 trees and turned of honesty. Perhaps dataset too small? Maybe check out the sample.fraction parameter? Sample.fraction is set by default at 0.5, so only half of data is used to grow tree. OR use tune.parameters = TRUE\n\rCompare methods\rgp \u0026lt;- plot_grid(bartp, grfp)\r## `geom_smooth()` using method = \u0026#39;loess\u0026#39; and formula \u0026#39;y ~ x\u0026#39;\r## `geom_smooth()` using method = \u0026#39;loess\u0026#39; and formula \u0026#39;y ~ x\u0026#39;\rgp \r\r\rDataset 2: Simulated data from ACIC 2017\rThis is a bigger dataset, N=4302.\n\rTreatment effect \\(\\tau\\) is a function of covariates x3, x24, x14, x15\rProbability of treatment \\(\\pi\\) is a function of covariates x1, x43, x10.\rOutcome is a function of x43\rNoise is a function of x21\r\rhead(input_2017[, c(3,24,14,15)])\r## x_3 x_24 x_14 x_15\r## 1 20 white 0 2\r## 2 0 black 0 0\r## 3 0 white 0 1\r## 4 10 white 0 0\r## 5 0 black 0 0\r## 6 1 white 0 0\rCheck transformed covariates used to create simulated datasets.\n# zit hidden in package\rhead(aciccomp2017:::transformedData_2017)\r## x_1 x_3 x_10 x_14 x_15 x_21 x_24 x_43\r## 2665 -1.18689448 gt_0 leq_0 leq_0 gt_0 J E -1.0897971\r## 22 -0.04543705 leq_0 leq_0 leq_0 leq_0 J B 1.1223750\r## 2416 0.13675482 leq_0 leq_0 leq_0 gt_0 J E 0.6136700\r## 1350 -0.24062700 gt_0 leq_0 leq_0 leq_0 J E -0.2995632\r## 3850 1.02054653 leq_0 leq_0 leq_0 leq_0 I B 0.6136700\r## 4167 -1.18689448 gt_0 leq_0 leq_0 leq_0 K E -1.5961206\rSo we find that we should not take the functions in Dorie 2018 (debrief.pdf) literately. x_3 used to calculate the treatment effect is derived from x_3 in the input data. x_24 used to calculate the treatment effect is derived from x_24 in the input data. Both have become binary variables.\nTurns out that this was a feature of the 2016 ACIC and IS mentioned in the debrief.pdf\nWe pick the iid, strong signal, low noise, low confounding first. Actually from estimated PS (W.hat) it seems that every obs has probability of treatment 50%.\nparameters_2017[21,]\r## errors magnitude noise confounding\r## 21 iid 1 0 0\r# easiest?\rGrab the first replicate.\nsim \u0026lt;- dgp_2017(21, 1)\rFit BART to ACIC 2017 dataset\rNeed also counterfactual predictions. Most efficient seems to create x.test with Z reversed. This will give use a y.test as well as y.train in the output. We expect draws for both. Plotting a histogram of the difference gives us the treatment effect with uncertainty.\nFrom the MCMC draws for sigma we infer that we need to drop more “burn in” samples.\nPrepare data for BART, including x.test with treatment reversed:\n# combine x and y\ry \u0026lt;- sim$y\rx \u0026lt;- model.matrix(~. ,cbind(z = sim$z, input_2017))\r# flip z for counterfactual predictions (needed for BART)\rx.test \u0026lt;- model.matrix(~. ,cbind(z = 1 - sim$z, input_2017))\r## run BART\rfullrun \u0026lt;- 0\rif(fullrun){\rset.seed(99)\rbartFit \u0026lt;- bart(x, y, x.test, nskip = 350, ntree = 1000)\rsaveRDS(bartFit, \u0026quot;s2.rds\u0026quot;)\r} else { bartFit \u0026lt;- readRDS(\u0026quot;s2.rds\u0026quot;)}\rplot(bartFit)\rExtract individual treatment effect (ITE / CATE) plus uncertainty from bartfit\rThis means switching z from 0 to 1 and looking at difference in y + uncertainty in y.\nsource(\u0026quot;calcPosteriors.R\u0026quot;)\rsim \u0026lt;- CalcPosteriorsBART(sim, bartFit, \u0026quot;z\u0026quot;)\rsim \u0026lt;- sim %\u0026gt;% arrange(alpha)\rbartp \u0026lt;- ggplot(sim, aes(x = 1:nrow(sim), qm)) + geom_linerange(aes(ymin = ql, ymax = qu), col = \u0026quot;grey\u0026quot;) +\rgeom_point() + geom_smooth() + geom_point(aes(y = alpha), col = \u0026quot;red\u0026quot;) + ylim(-2.5, 4.5)\rbartp\r## `geom_smooth()` using method = \u0026#39;gam\u0026#39; and formula \u0026#39;y ~ s(x, bs = \u0026quot;cs\u0026quot;)\u0026#39;\r## Warning: Removed 1 rows containing missing values (geom_smooth).\rThis looks sort of ok, but still weird. Some points it gets REALLY wrong.\n\rCalculate coverage\rsim \u0026lt;- sim %\u0026gt;% mutate(in_ci = ql \u0026lt; alpha \u0026amp; qu \u0026gt; alpha) mean(sim$in_ci)\r## [1] 0.4363087\rPretty bad coverage. Look into whats going on here. Here it should be 0.9\nThe iid plot for method 2 gives coverage 0.7 (where it should be 0.95)\n\rCalculate RMSE of CATE\rsqrt(mean((sim$alpha - sim$ite)^2))\r## [1] 0.1587338\rFor All i.i.d. (averaged over 250 replicates averaged over 8 scenarios) method 2 (BART should have RMSE of CATE of 0.35-0.4)\n\r\rFit grf to ACIC 2017 dataset\rneed large num.trees for CI.\n# prep data for Grf\r# combine x and y\rsim \u0026lt;- dgp_2017(21, 1)\rY \u0026lt;- sim$y\rX \u0026lt;- model.matrix(~. ,input_2017)\rW = sim$z\r# Train a causal forest.\rfullrun \u0026lt;- 0\rif(fullrun){\rgrf.fit_alt \u0026lt;- causal_forest(X, Y, W, num.trees = 500)\rsaveRDS(grf.fit_alt, \u0026quot;s3.rds\u0026quot;)\r} else{grf.fit_alt \u0026lt;- readRDS(\u0026quot;s3.rds\u0026quot;)}\rIt appears that using 4000 trees consumes too much memory (bad_alloc)\n\rCompare predictions vs true value\rdf_sep2 \u0026lt;- CalcPredictionsGRF(X, grf.fit_alt)\rdf_sep2 \u0026lt;- data.frame(df_sep2, Y, W, TAU = sim$alpha)\rdf_sep2 \u0026lt;- df_sep2 %\u0026gt;% arrange(TAU)\rgrfp \u0026lt;- ggplot(df_sep2, aes(x = 1:nrow(df_sep2), y = qm)) +\rgeom_linerange(aes(ymin = ql, ymax = qu), col = \u0026quot;grey\u0026quot;) + geom_point() + geom_smooth() + geom_point(aes(y = TAU), col = \u0026quot;red\u0026quot;) + ylim(-2.5, 4.5)\rgrfp\r## `geom_smooth()` using method = \u0026#39;gam\u0026#39; and formula \u0026#39;y ~ s(x, bs = \u0026quot;cs\u0026quot;)\u0026#39;\rThis works ok now.\n\rCompare both methods\rgp \u0026lt;- plot_grid(bartp, grfp)\r## `geom_smooth()` using method = \u0026#39;gam\u0026#39; and formula \u0026#39;y ~ s(x, bs = \u0026quot;cs\u0026quot;)\u0026#39;\r## Warning: Removed 1 rows containing missing values (geom_smooth).\r## `geom_smooth()` using method = \u0026#39;gam\u0026#39; and formula \u0026#39;y ~ s(x, bs = \u0026quot;cs\u0026quot;)\u0026#39;\rgp\r\r\rDataset 3: simulated data used by grf example\rTHis dataset is used in the Grf manual page. Size N = 2000. Probability of treatment function of X1. Treatment effect function of X1.\n# Generate data.\rset.seed(123)\rn = 2000; p = 10\rX = matrix(rnorm(n*p), n, p)\r# treatment\rW = rbinom(n, 1, 0.4 + 0.2 * (X[,1] \u0026gt; 0))\r# outcome (parallel max)\rY = pmax(X[,1], 0) * W + X[,2] + pmin(X[,3], 0) + rnorm(n)\r# TAU is true treatment effect\rdf \u0026lt;- data.frame(X, W, Y, TAU = pmax(X[,1], 0))\rFit GRF\rDefault settings are honesty = TRUE.\n# Train a causal forest.\rif(fullrun){\rtau.forest = causal_forest(X, Y, W, num.trees = 2000)\rsaveRDS(tau.forest, \u0026quot;s4.rds\u0026quot;)\r} else {tau.forest \u0026lt;- readRDS(\u0026quot;s4.rds\u0026quot;)}\r\rOOB predictions\rFrom the GRF manual:\nGiven a test example, the GRF algorithm computes a prediction as follows:\nFor each tree, the test example is \u0026#39;pushed down\u0026#39; to determine what leaf it falls in.\rGiven this information, we create a list of neighboring training examples, weighted by how many times the example fell in the same leaf as the test example.\rA prediction is made using this weighted list of neighbors, using the relevant approach for the type of forest. In causal prediction, we calculate the treatment effect using the outcomes and treatment status of the neighbor examples.\rThose familiar with classic random forests might note that this approach differs from the way forest prediction is usually described. The traditional view is that to predict for a test example, each tree makes a prediction on that example. To make a final prediction, the tree predictions are combined in some way, for example through averaging or through ‘majority voting’. It’s worth noting that for regression forests, the GRF algorithm described above is identical this ‘ensemble’ approach, where each tree predicts by averaging the outcomes in each leaf, and predictions are combined through a weighted average.\n# Estimate treatment effects for the training data using out-of-bag prediction.\rtau.hat.oob = predict(tau.forest)\rres \u0026lt;- data.frame(df, pred = tau.hat.oob$predictions)\rggplot(res, aes(x = X1, y = pred)) + geom_point() + geom_smooth() + geom_abline(intercept = 0, slope = 1) +\rgeom_line(aes(y = TAU), col = \u0026quot;red\u0026quot;, size = 1)\r## `geom_smooth()` using method = \u0026#39;gam\u0026#39; and formula \u0026#39;y ~ s(x, bs = \u0026quot;cs\u0026quot;)\u0026#39;\r\rATE \u0026amp; ATT\r# Estimate the conditional average treatment effect on the full sample (CATE).\raverage_treatment_effect(tau.forest, target.sample = \u0026quot;all\u0026quot;)\r## estimate std.err ## 0.36664140 0.04796884\rmean(res$TAU)\r## [1] 0.4138061\r# Estimate the conditional average treatment effect on the treated sample (CATT).\r# Here, we don\u0026#39;t expect much difference between the CATE and the CATT, since\r# treatment assignment was randomized.\raverage_treatment_effect(tau.forest, target.sample = \u0026quot;treated\u0026quot;)\r## estimate std.err ## 0.45860274 0.04852209\rmean(res[res$W == 1,]$TAU)\r## [1] 0.5010723\r\rFit more trees for CI’s\r# Add confidence intervals for heterogeneous treatment effects; growing more\r# trees is now recommended.\rif(fullrun){\rtau.forest_big = causal_forest(X, Y, W, num.trees = 4000)\rsaveRDS(tau.forest_big, \u0026quot;s5.rds\u0026quot;)\r} else {tau.forest_big \u0026lt;- readRDS(\u0026quot;s5.rds\u0026quot;)}\r\rPlot CI’s\r## PM\rsource(\u0026quot;CalcPosteriors.R\u0026quot;)\rdf_res \u0026lt;- CalcPredictionsGRF(df, tau.forest_big)\rgrfp \u0026lt;- ggplot(df_res, aes(x = X1, y = qm)) + geom_linerange(aes(ymin = ql, ymax = qu), col = \u0026quot;grey\u0026quot;) +\rgeom_point() + geom_smooth() + geom_line(aes(y = TAU), col = \u0026quot;red\u0026quot;, size = 1) +\rylim(-1,3.5)\rgrfp\r## `geom_smooth()` using method = \u0026#39;gam\u0026#39; and formula \u0026#39;y ~ s(x, bs = \u0026quot;cs\u0026quot;)\u0026#39;\r\rFit BART on this dataset\rx.train \u0026lt;- model.matrix(~. ,data.frame(W, X))\rx.test \u0026lt;- model.matrix(~. ,data.frame(W = 1 - W, X))\ry.train \u0026lt;- Y\rif(fullrun){\rbartFit \u0026lt;- bart(x.train, y.train, x.test, ntree = 2000, ndpost = 1000, nskip = 100)\rsaveRDS(bartFit, \u0026quot;s10.rds\u0026quot;)\r} else {bartFit \u0026lt;- readRDS(\u0026quot;s10.rds\u0026quot;)}\rplot(bartFit)\r\rBART: Check fit and CI’s\rsource(\u0026quot;calcPosteriors.R\u0026quot;)\rsim \u0026lt;- CalcPosteriorsBART(df, bartFit, treatname = \u0026quot;W\u0026quot;)\rbartp \u0026lt;- ggplot(sim, aes(x = X1, qm)) + geom_linerange(aes(ymin = ql, ymax = qu), col = \u0026quot;grey\u0026quot;) +\rgeom_point() + geom_smooth() +\rgeom_line(aes(y = TAU), col = \u0026quot;red\u0026quot;, size = 1) + ylim(-1,3.5)\rbartp\r## `geom_smooth()` using method = \u0026#39;gam\u0026#39; and formula \u0026#39;y ~ s(x, bs = \u0026quot;cs\u0026quot;)\u0026#39;\r\rCompare\rgp \u0026lt;- plot_grid(bartp, grfp)\r## `geom_smooth()` using method = \u0026#39;gam\u0026#39; and formula \u0026#39;y ~ s(x, bs = \u0026quot;cs\u0026quot;)\u0026#39;\r## `geom_smooth()` using method = \u0026#39;gam\u0026#39; and formula \u0026#39;y ~ s(x, bs = \u0026quot;cs\u0026quot;)\u0026#39;\rgp\rHere Grf appears more accurate. Mental note: Both W and TAU function of X1.\n\r\rDataset 4: Fit separate grf forests for Y and W\rThis dataset has a complex propensity of treatment function (Exponential of X1 and X2), as well as hetergeneous treatment effect that is exponential function of X3. It has size N=4000.\n# In some examples, pre-fitting models for Y and W separately may\r# be helpful (e.g., if different models use different covariates).\r# In some applications, one may even want to get Y.hat and W.hat\r# using a completely different method (e.g., boosting).\rset.seed(123)\r# Generate new data.\rn = 4000; p = 20\rX = matrix(rnorm(n * p), n, p)\rTAU = 1 / (1 + exp(-X[, 3]))\rW = rbinom(n ,1, 1 / (1 + exp(-X[, 1] - X[, 2])))\rY = pmax(X[, 2] + X[, 3], 0) + rowMeans(X[, 4:6]) / 2 + W * TAU + rnorm(n)\rdf_sep4 \u0026lt;- data.frame(X, TAU, W, Y)\rGrf two-step: First fit model for W (PS)\rRegression forest to predict W from X. This is a propensity score.\nif(fullrun){\rforest.W = regression_forest(X, W, tune.parameters = TRUE, num.trees = 2000)\rsaveRDS(forest.W, \u0026quot;s6.rds\u0026quot;)\r} else {forest.W \u0026lt;- readRDS(\u0026quot;s6.rds\u0026quot;)}\rW.hat = predict(forest.W)$predictions\rGrf:Then Fit model for Y, selecting covariates\rThis predict Y from X, ignoring treatment.\nif(fullrun){\rforest.Y = regression_forest(X, Y, tune.parameters = TRUE, num.trees = 2000)\rsaveRDS(forest.Y, \u0026quot;s7.rds\u0026quot;)\r} else {forest.Y \u0026lt;- readRDS(\u0026quot;s7.rds\u0026quot;)}\rY.hat = predict(forest.Y)$predictions\r\rGrf:Select variables that predict Y.\rforest.Y.varimp = variable_importance(forest.Y)\r# Note: Forests may have a hard time when trained on very few variables\r# (e.g., ncol(X) = 1, 2, or 3). We recommend not being too aggressive\r# in selection.\rselected.vars = which(forest.Y.varimp / mean(forest.Y.varimp) \u0026gt; 0.2)\rThis selects five variables of 20. Indeed these are the variables that were used to simulate Y.\n\rGrf: Finally, Fit causal forest using PS and selected covariates\rif(fullrun){\rtau.forest2 = causal_forest(X[, selected.vars], Y, W,\rW.hat = W.hat, Y.hat = Y.hat,\rtune.parameters = TRUE, num.trees = 4000)\rsaveRDS(tau.forest2, \u0026quot;s8.rds\u0026quot;)\r} else {tau.forest2 \u0026lt;- readRDS(\u0026quot;s8.rds\u0026quot;)}\r\rGrf: Check fit and CI’s\rdf_sep2 \u0026lt;- CalcPredictionsGRF(df_sep4, tau.forest2)\rgrfp \u0026lt;- ggplot(df_sep2, aes(x = X3, y = qm)) +\rgeom_linerange(aes(ymin = ql, ymax = qu), col = \u0026quot;grey\u0026quot;) + geom_point() + geom_smooth() + geom_line(aes(y = TAU), col = \u0026quot;red\u0026quot;, size = 1) + ylim(-0.7,2)\rgrfp\r## `geom_smooth()` using method = \u0026#39;gam\u0026#39; and formula \u0026#39;y ~ s(x, bs = \u0026quot;cs\u0026quot;)\u0026#39;\rThis looks ok.\n\r\rFit BART on this dataset\rx.train \u0026lt;- model.matrix(~. ,data.frame(W, X))\rx.test \u0026lt;- model.matrix(~. ,data.frame(W = 1 - W, X))\ry.train \u0026lt;- Y\rif(fullrun){\rbartFit \u0026lt;- bart(x.train, y.train, x.test, ntree = 4000)\rsaveRDS(bartFit, \u0026quot;s9.rds\u0026quot;)\r} else {bartFit \u0026lt;- readRDS(\u0026quot;s9.rds\u0026quot;)}\rplot(bartFit)\r\rBART: Check fit and CI’s\rsource(\u0026quot;calcPosteriors.R\u0026quot;)\rsim \u0026lt;- CalcPosteriorsBART(df_sep4, bartFit, treatname = \u0026quot;W\u0026quot;)\rbartp \u0026lt;- ggplot(sim, aes(x = X3, qm)) + geom_linerange(aes(ymin = ql, ymax = qu), col = \u0026quot;grey\u0026quot;) +\rgeom_point() + geom_smooth() +\rgeom_line(aes(y = TAU), col = \u0026quot;red\u0026quot;, size = 1) + ylim(-0.7,2)\rbartp\r## `geom_smooth()` using method = \u0026#39;gam\u0026#39; and formula \u0026#39;y ~ s(x, bs = \u0026quot;cs\u0026quot;)\u0026#39;\r\rCompare BART and grf\rgp \u0026lt;- plot_grid(bartp, grfp)\r## `geom_smooth()` using method = \u0026#39;gam\u0026#39; and formula \u0026#39;y ~ s(x, bs = \u0026quot;cs\u0026quot;)\u0026#39;\r## `geom_smooth()` using method = \u0026#39;gam\u0026#39; and formula \u0026#39;y ~ s(x, bs = \u0026quot;cs\u0026quot;)\u0026#39;\rgp\rVery similar results. BART appears slightly more accurate, especially for low values of X3.\n\r\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5e61a0767e44f41cc719faa8702e5dae","permalink":"/post/bart_vs_grf/bart-vs-grf/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/bart_vs_grf/bart-vs-grf/","section":"post","summary":"Load packages\r# library(devtools)\r#devtools::install_github(\u0026quot;vdorie/dbarts\u0026quot;)\rlibrary(dbarts)\rlibrary(ggplot2)\rlibrary(tidyverse)\r## -- Attaching packages ------------------------------------------------------------- tidyverse 1.2.1 --\r## v tibble 2.0.1 v purrr 0.2.5\r## v tidyr 0.8.2 v dplyr 0.7.8\r## v readr 1.3.1 v stringr 1.3.1\r## v tibble 2.0.1 v forcats 0.3.0\r## -- Conflicts ---------------------------------------------------------------- tidyverse_conflicts() --\r## x dplyr::filter() masks stats::filter()\r## x dplyr::lag() masks stats::lag()\rlibrary(grf)\r#devtools::install_github(\u0026quot;vdorie/aciccomp/2017\u0026quot;)\rlibrary(aciccomp2017)\rlibrary(cowplot)\r## ## Attaching package: \u0026#39;cowplot\u0026#39;\r## The following object is masked from \u0026#39;package:ggplot2\u0026#39;:\r## ## ggsave\rsource(\u0026quot;calcPosteriors.","tags":null,"title":"BART vs Grf showdown","type":"post"},{"authors":null,"categories":null,"content":"\rsummary\rThe idea is that comparing the predictions of an RF model with the predictions of an OLS model can inform us in what ways the OLS model fails to capture all non-linearities and interactions between the predictors. Subsequently, using partial dependence plots of the RF model can guide the modelling of the non-linearities in the OLS model. After this step, the discrepancies between the RF predictions and the OLS predictions should be caused by non-modeled interactions. Using an RF to predict the discrepancy itself can then be used to discover which predictors are involved in these interactions. We test this method on the classic Boston Housing dataset to predict median house values (medv). We indeed recover interactions that, as it turns, have already been found and documented in the literature.\n\rLoad packages\rrm(list=ls())\r#library(randomForest)\r#library(party)\rlibrary(ranger)\rlibrary(data.table)\rlibrary(ggplot2)\rlibrary(MASS)\rrdunif \u0026lt;- function(n,k) sample(1:k, n, replace = T)\r\rStep 1: Run a RF on the Boston Housing set\rmy_ranger \u0026lt;- ranger(medv ~ ., data = Boston,\rimportance = \u0026quot;permutation\u0026quot;, num.trees = 500,\rmtry = 5, replace = TRUE)\rExtract the permutation importance measure.\nmyres_tmp \u0026lt;- ranger::importance(my_ranger);\rmyres \u0026lt;- cbind(names(myres_tmp), myres_tmp, i = 1)\r#my_rownames \u0026lt;- row.names(myres)\rmyres \u0026lt;- data.table(myres)\rsetnames(myres, \u0026quot;V1\u0026quot;, \u0026quot;varname\u0026quot;)\rsetnames(myres, \u0026quot;myres_tmp\u0026quot;, \u0026quot;MeanDecreaseAccuracy\u0026quot;)\rmyres \u0026lt;- myres[, varname := as.factor(varname)]\rmyres \u0026lt;- myres[, MeanDecreaseAccuracy := as.numeric(MeanDecreaseAccuracy)]\rmyres \u0026lt;- myres[, i := as.integer(i)]\rggplot(myres, aes(x = reorder(factor(varname), MeanDecreaseAccuracy), y = MeanDecreaseAccuracy)) + geom_point() + coord_flip()\r\rFit an OLS to the Boston Housing\rmy_glm \u0026lt;- glm(medv ~., data = Boston, family = \u0026quot;gaussian\u0026quot;)\r\rCompare predictions of both models\rpred_RF \u0026lt;- predict(my_ranger, data = Boston)\r#pred_RF$predictions\rpred_GLM \u0026lt;- predict(my_glm, data = Boston)\rplot(pred_RF$predictions, pred_GLM)\rabline(0, 1)\r# Run a RF on the discrepancy\nDiscrepancy is defined as the difference between the predictions of both models for each observation.\npred_diff \u0026lt;- pred_RF$predictions - pred_GLM\rmy_ranger_diff \u0026lt;- ranger(Ydiff ~ . - medv, data = data.table(Ydiff = pred_diff, Boston),\rimportance = \u0026quot;permutation\u0026quot;, num.trees = 500,\rmtry = 5, replace = TRUE)\rmy_ranger_diff\r## Ranger result\r## ## Call:\r## ranger(Ydiff ~ . - medv, data = data.table(Ydiff = pred_diff, Boston), importance = \u0026quot;permutation\u0026quot;, num.trees = 500, mtry = 5, replace = TRUE) ## ## Type: Regression ## Number of trees: 500 ## Sample size: 506 ## Number of independent variables: 13 ## Mtry: 5 ## Target node size: 5 ## Variable importance mode: permutation ## Splitrule: variance ## OOB prediction error (MSE): 5.045991 ## R squared (OOB): 0.6702479\rIt turns out the RF can “explain” 67% of these discrepancies.\nmyres_tmp \u0026lt;- ranger::importance(my_ranger_diff)\rmyres \u0026lt;- cbind(names(myres_tmp), myres_tmp, i = 1)\r#my_rownames \u0026lt;- row.names(myres)\rmyres \u0026lt;- data.table(myres)\rsetnames(myres, \u0026quot;V1\u0026quot;, \u0026quot;varname\u0026quot;)\rsetnames(myres, \u0026quot;myres_tmp\u0026quot;, \u0026quot;MeanDecreaseAccuracy\u0026quot;)\rmyres \u0026lt;- myres[, varname := as.factor(varname)]\rmyres \u0026lt;- myres[, MeanDecreaseAccuracy := as.numeric(MeanDecreaseAccuracy)]\rmyres \u0026lt;- myres[, i := as.integer(i)]\rggplot(myres, aes(x = reorder(factor(varname), MeanDecreaseAccuracy), y = MeanDecreaseAccuracy)) + geom_point() + coord_flip()\rIt turns out that rm and lstat are the variables that best predict the discrepancy.\nmy_glm_int \u0026lt;- glm(medv ~. + rm:lstat, data = Boston, family = \u0026quot;gaussian\u0026quot;)\rsummary(my_glm_int)\r## ## Call:\r## glm(formula = medv ~ . + rm:lstat, family = \u0026quot;gaussian\u0026quot;, data = Boston)\r## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -21.5738 -2.3319 -0.3584 1.8149 27.9558 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 6.073638 5.038175 1.206 0.228582 ## crim -0.157100 0.028808 -5.453 7.85e-08 ***\r## zn 0.027199 0.012020 2.263 0.024083 * ## indus 0.052272 0.053475 0.978 0.328798 ## chas 2.051584 0.750060 2.735 0.006459 ** ## nox -15.051627 3.324807 -4.527 7.51e-06 ***\r## rm 7.958907 0.488520 16.292 \u0026lt; 2e-16 ***\r## age 0.013466 0.011518 1.169 0.242918 ## dis -1.120269 0.175498 -6.383 4.02e-10 ***\r## rad 0.320355 0.057641 5.558 4.49e-08 ***\r## tax -0.011968 0.003267 -3.664 0.000276 ***\r## ptratio -0.721302 0.115093 -6.267 8.06e-10 ***\r## black 0.003985 0.002371 1.681 0.093385 . ## lstat 1.844883 0.191833 9.617 \u0026lt; 2e-16 ***\r## rm:lstat -0.418259 0.032955 -12.692 \u0026lt; 2e-16 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## (Dispersion parameter for gaussian family taken to be 16.98987)\r## ## Null deviance: 42716 on 505 degrees of freedom\r## Residual deviance: 8342 on 491 degrees of freedom\r## AIC: 2886\r## ## Number of Fisher Scoring iterations: 2\rThe interaction we have added is indeed highly significant.\nCompare approximate out-of-sample prediction accuracy using AIC:\nAIC(my_glm)\r## [1] 3027.609\rAIC(my_glm_int)\r## [1] 2886.043\rIndeed, the addition of the interaction greatly increases the prediction accuracy.\n\rRepeat this process\rpred_RF \u0026lt;- predict(my_ranger, data = Boston)\r#pred_RF$predictions\rpred_GLM \u0026lt;- predict(my_glm_int, data = Boston)\rplot(pred_RF$predictions, pred_GLM)\rabline(0, 1)\rpred_diff \u0026lt;- pred_RF$predictions - pred_GLM\rmy_ranger_diff2 \u0026lt;- ranger(Ydiff ~ . - medv, data = data.table(Ydiff = pred_diff, Boston),\rimportance = \u0026quot;permutation\u0026quot;, num.trees = 500,\rmtry = 5, replace = TRUE)\rmy_ranger_diff2\r## Ranger result\r## ## Call:\r## ranger(Ydiff ~ . - medv, data = data.table(Ydiff = pred_diff, Boston), importance = \u0026quot;permutation\u0026quot;, num.trees = 500, mtry = 5, replace = TRUE) ## ## Type: Regression ## Number of trees: 500 ## Sample size: 506 ## Number of independent variables: 13 ## Mtry: 5 ## Target node size: 5 ## Variable importance mode: permutation ## Splitrule: variance ## OOB prediction error (MSE): 5.45721 ## R squared (OOB): 0.4482457\rmyres_tmp \u0026lt;- ranger::importance(my_ranger_diff2)\rmyres \u0026lt;- cbind(names(myres_tmp), myres_tmp, i = 1)\r#my_rownames \u0026lt;- row.names(myres)\rmyres \u0026lt;- data.table(myres)\rsetnames(myres, \u0026quot;V1\u0026quot;, \u0026quot;varname\u0026quot;)\rsetnames(myres, \u0026quot;myres_tmp\u0026quot;, \u0026quot;MeanDecreaseAccuracy\u0026quot;)\rmyres \u0026lt;- myres[, varname := as.factor(varname)]\rmyres \u0026lt;- myres[, MeanDecreaseAccuracy := as.numeric(MeanDecreaseAccuracy)]\rmyres \u0026lt;- myres[, i := as.integer(i)]\rggplot(myres, aes(x = reorder(factor(varname), MeanDecreaseAccuracy), y = MeanDecreaseAccuracy)) + geom_point() + coord_flip()\rNow the variables that best predict the discrepancy are lstat and dis. Add these two variables as an interaction.\nmy_glm_int2 \u0026lt;- glm(medv ~. + rm:lstat + lstat:dis, data = Boston, family = \u0026quot;gaussian\u0026quot;)\rsummary(my_glm_int2)\r## ## Call:\r## glm(formula = medv ~ . + rm:lstat + lstat:dis, family = \u0026quot;gaussian\u0026quot;, ## data = Boston)\r## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -23.3918 -2.2997 -0.4077 1.6475 27.6766 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 1.552991 5.107295 0.304 0.761201 ## crim -0.139370 0.028788 -4.841 1.73e-06 ***\r## zn 0.042984 0.012550 3.425 0.000667 ***\r## indus 0.066690 0.052878 1.261 0.207834 ## chas 1.760779 0.743688 2.368 0.018290 * ## nox -11.544280 3.404577 -3.391 0.000753 ***\r## rm 8.640503 0.513593 16.824 \u0026lt; 2e-16 ***\r## age -0.002127 0.012067 -0.176 0.860140 ## dis -1.904982 0.268056 -7.107 4.22e-12 ***\r## rad 0.304689 0.057000 5.345 1.39e-07 ***\r## tax -0.011220 0.003228 -3.476 0.000554 ***\r## ptratio -0.641380 0.115418 -5.557 4.51e-08 ***\r## black 0.003756 0.002339 1.606 0.108924 ## lstat 1.925223 0.190368 10.113 \u0026lt; 2e-16 ***\r## rm:lstat -0.466947 0.034897 -13.381 \u0026lt; 2e-16 ***\r## dis:lstat 0.076716 0.020009 3.834 0.000143 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## (Dispersion parameter for gaussian family taken to be 16.52869)\r## ## Null deviance: 42716.3 on 505 degrees of freedom\r## Residual deviance: 8099.1 on 490 degrees of freedom\r## AIC: 2873.1\r## ## Number of Fisher Scoring iterations: 2\rAIC(my_glm_int2)\r## [1] 2873.087\rAIC(my_glm_int)\r## [1] 2886.043\rWe conclude that the second interaction also results in significant model improvement.\n\rA more ambitious goal: Try and improve Harrison \u0026amp; Rubinfeld’s model formula for Boston housing\rSo far, we assumed that all relationships are linear. Harrison and Rubinfeld have created a model without interactions, but with transformations to correct for skewness, heteroskedasticity etc. Let’s see if we can improve upon this model equation by applying our method to search for interactions. Their formula predicts log(medv).\n# Harrison and Rubinfeld (1978) model\rmy_glm_hr \u0026lt;- glm(log(medv) ~ I(rm^2) + age + log(dis) + log(rad) + tax + ptratio + black + I(black^2) + log(lstat) + crim + zn + indus + chas + I(nox^2), data = Boston, family = \u0026quot;gaussian\u0026quot;)\rsummary(my_glm_hr)\r## ## Call:\r## glm(formula = log(medv) ~ I(rm^2) + age + log(dis) + log(rad) + ## tax + ptratio + black + I(black^2) + log(lstat) + crim + ## zn + indus + chas + I(nox^2), family = \u0026quot;gaussian\u0026quot;, data = Boston)\r## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.73091 -0.09274 -0.00710 0.09800 0.78607 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 4.474e+00 1.579e-01 28.343 \u0026lt; 2e-16 ***\r## I(rm^2) 6.634e-03 1.313e-03 5.053 6.15e-07 ***\r## age 3.491e-05 5.245e-04 0.067 0.946950 ## log(dis) -1.927e-01 3.325e-02 -5.796 1.22e-08 ***\r## log(rad) 9.613e-02 1.905e-02 5.047 6.35e-07 ***\r## tax -4.295e-04 1.222e-04 -3.515 0.000481 ***\r## ptratio -2.977e-02 5.024e-03 -5.926 5.85e-09 ***\r## black 1.520e-03 5.068e-04 3.000 0.002833 ** ## I(black^2) -2.597e-06 1.114e-06 -2.331 0.020153 * ## log(lstat) -3.695e-01 2.491e-02 -14.833 \u0026lt; 2e-16 ***\r## crim -1.157e-02 1.246e-03 -9.286 \u0026lt; 2e-16 ***\r## zn 7.257e-05 5.034e-04 0.144 0.885430 ## indus -1.943e-04 2.360e-03 -0.082 0.934424 ## chas 9.180e-02 3.305e-02 2.777 0.005690 ** ## I(nox^2) -6.566e-01 1.129e-01 -5.815 1.09e-08 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## (Dispersion parameter for gaussian family taken to be 0.03299176)\r## ## Null deviance: 84.376 on 505 degrees of freedom\r## Residual deviance: 16.199 on 491 degrees of freedom\r## AIC: -273.48\r## ## Number of Fisher Scoring iterations: 2\rmy_ranger_log \u0026lt;- ranger(log(medv) ~ ., data = Boston,\rimportance = \u0026quot;permutation\u0026quot;, num.trees = 500,\rmtry = 5, replace = TRUE)\rpred_RF \u0026lt;- predict(my_ranger_log, data = Boston)\r#pred_RF$predictions\rpred_GLM \u0026lt;- predict(my_glm_hr, data = Boston)\rplot(pred_RF$predictions, pred_GLM)\rabline(0, 1)\rFor low predicted values both models differ in a systematic way. This suggests that there exists a remaining pattern that is picked up by RF but not by the OLS model.\npred_diff \u0026lt;- pred_RF$predictions - pred_GLM\rmy_ranger_log_diff \u0026lt;- ranger(Ydiff ~ . - medv, data = data.table(Ydiff = pred_diff, Boston),\rimportance = \u0026quot;permutation\u0026quot;, num.trees = 500,\rmtry = 5, replace = TRUE)\rmy_ranger_log_diff\r## Ranger result\r## ## Call:\r## ranger(Ydiff ~ . - medv, data = data.table(Ydiff = pred_diff, Boston), importance = \u0026quot;permutation\u0026quot;, num.trees = 500, mtry = 5, replace = TRUE) ## ## Type: Regression ## Number of trees: 500 ## Sample size: 506 ## Number of independent variables: 13 ## Mtry: 5 ## Target node size: 5 ## Variable importance mode: permutation ## Splitrule: variance ## OOB prediction error (MSE): 0.008946419 ## R squared (OOB): 0.5368337\rThe RF indicates that 54% of the discrepancy can be “explained” by RF.\nmyres_tmp \u0026lt;- ranger::importance(my_ranger_log_diff)\rmyres \u0026lt;- cbind(names(myres_tmp), myres_tmp, i = 1)\r#my_rownames \u0026lt;- row.names(myres)\rmyres \u0026lt;- data.table(myres)\rsetnames(myres, \u0026quot;V1\u0026quot;, \u0026quot;varname\u0026quot;)\rsetnames(myres, \u0026quot;myres_tmp\u0026quot;, \u0026quot;MeanDecreaseAccuracy\u0026quot;)\rmyres \u0026lt;- myres[, varname := as.factor(varname)]\rmyres \u0026lt;- myres[, MeanDecreaseAccuracy := as.numeric(MeanDecreaseAccuracy)]\rmyres \u0026lt;- myres[, i := as.integer(i)]\rggplot(myres, aes(x = reorder(factor(varname), MeanDecreaseAccuracy), y = MeanDecreaseAccuracy)) + geom_point() + coord_flip()\rAdd the top 2 vars as an interaction to their model equation.\nmy_glm_hr_int \u0026lt;- glm(log(medv) ~ I(rm^2) + age + log(dis) + log(rad) + tax + ptratio + black + I(black^2) + log(lstat) + crim + zn + indus + chas + I(nox^2) +\rlstat:nox, data = Boston, family = \u0026quot;gaussian\u0026quot;)\rsummary(my_glm_hr_int)\r## ## Call:\r## glm(formula = log(medv) ~ I(rm^2) + age + log(dis) + log(rad) + ## tax + ptratio + black + I(black^2) + log(lstat) + crim + ## zn + indus + chas + I(nox^2) + lstat:nox, family = \u0026quot;gaussian\u0026quot;, ## data = Boston)\r## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.70340 -0.09274 -0.00665 0.10068 0.75004 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 4.243e+00 1.613e-01 26.304 \u0026lt; 2e-16 ***\r## I(rm^2) 7.053e-03 1.286e-03 5.484 6.66e-08 ***\r## age -3.146e-04 5.174e-04 -0.608 0.54354 ## log(dis) -2.254e-01 3.317e-02 -6.795 3.15e-11 ***\r## log(rad) 9.829e-02 1.862e-02 5.278 1.96e-07 ***\r## tax -4.589e-04 1.196e-04 -3.838 0.00014 ***\r## ptratio -2.990e-02 4.910e-03 -6.089 2.30e-09 ***\r## black 1.445e-03 4.955e-04 2.917 0.00370 ** ## I(black^2) -2.470e-06 1.089e-06 -2.268 0.02376 * ## log(lstat) -2.143e-01 3.989e-02 -5.373 1.20e-07 ***\r## crim -1.046e-02 1.238e-03 -8.448 3.40e-16 ***\r## zn 7.309e-04 5.099e-04 1.434 0.15234 ## indus -8.166e-05 2.307e-03 -0.035 0.97178 ## chas 8.746e-02 3.231e-02 2.707 0.00704 ** ## I(nox^2) -3.618e-01 1.256e-01 -2.880 0.00415 ** ## lstat:nox -2.367e-02 4.819e-03 -4.911 1.24e-06 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## (Dispersion parameter for gaussian family taken to be 0.03150809)\r## ## Null deviance: 84.376 on 505 degrees of freedom\r## Residual deviance: 15.439 on 490 degrees of freedom\r## AIC: -295.79\r## ## Number of Fisher Scoring iterations: 2\rAIC(my_glm_hr)\r## [1] -273.4788\rAIC(my_glm_hr_int)\r## [1] -295.7931\rThis results in a significant improvement!\n\rRepeat this procedure\rpred_RF \u0026lt;- predict(my_ranger_log, data = Boston)\r#pred_RF$predictions\rpred_GLM \u0026lt;- predict(my_glm_hr_int, data = Boston)\rplot(pred_RF$predictions, pred_GLM)\rabline(0, 1)\rpred_diff \u0026lt;- pred_RF$predictions - pred_GLM\rmy_ranger_log_diff2 \u0026lt;- ranger(Ydiff ~ . - medv, data = data.table(Ydiff = pred_diff, Boston),\rimportance = \u0026quot;permutation\u0026quot;, num.trees = 500,\rmtry = 5, replace = TRUE)\rmy_ranger_log_diff2\r## Ranger result\r## ## Call:\r## ranger(Ydiff ~ . - medv, data = data.table(Ydiff = pred_diff, Boston), importance = \u0026quot;permutation\u0026quot;, num.trees = 500, mtry = 5, replace = TRUE) ## ## Type: Regression ## Number of trees: 500 ## Sample size: 506 ## Number of independent variables: 13 ## Mtry: 5 ## Target node size: 5 ## Variable importance mode: permutation ## Splitrule: variance ## OOB prediction error (MSE): 0.008731355 ## R squared (OOB): 0.5165952\rmyres_tmp \u0026lt;- ranger::importance(my_ranger_log_diff2)\rmyres \u0026lt;- cbind(names(myres_tmp), myres_tmp, i = 1)\r#my_rownames \u0026lt;- row.names(myres)\rmyres \u0026lt;- data.table(myres)\rsetnames(myres, \u0026quot;V1\u0026quot;, \u0026quot;varname\u0026quot;)\rsetnames(myres, \u0026quot;myres_tmp\u0026quot;, \u0026quot;MeanDecreaseAccuracy\u0026quot;)\rmyres \u0026lt;- myres[, varname := as.factor(varname)]\rmyres \u0026lt;- myres[, MeanDecreaseAccuracy := as.numeric(MeanDecreaseAccuracy)]\rmyres \u0026lt;- myres[, i := as.integer(i)]\rggplot(myres, aes(x = reorder(factor(varname), MeanDecreaseAccuracy), y = MeanDecreaseAccuracy)) + geom_point() + coord_flip()\rNow we add lstat and dis as an interaction.\nmy_glm_hr_int2 \u0026lt;- glm(log(medv) ~ I(rm^2) + age + log(dis) + log(rad) + tax + ptratio + black + I(black^2) + log(lstat) + crim + zn + indus + chas + I(nox^2) +\rlstat:nox + lstat:dis, data = Boston, family = \u0026quot;gaussian\u0026quot;)\rsummary(my_glm_hr_int2)\r## ## Call:\r## glm(formula = log(medv) ~ I(rm^2) + age + log(dis) + log(rad) + ## tax + ptratio + black + I(black^2) + log(lstat) + crim + ## zn + indus + chas + I(nox^2) + lstat:nox + lstat:dis, family = \u0026quot;gaussian\u0026quot;, ## data = Boston)\r## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.70136 -0.08746 -0.00589 0.08857 0.76349 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 4.535e+00 1.712e-01 26.481 \u0026lt; 2e-16 ***\r## I(rm^2) 7.498e-03 1.266e-03 5.924 5.94e-09 ***\r## age -1.262e-03 5.504e-04 -2.293 0.02226 * ## log(dis) -4.065e-01 5.203e-02 -7.813 3.43e-14 ***\r## log(rad) 9.668e-02 1.828e-02 5.290 1.85e-07 ***\r## tax -4.622e-04 1.173e-04 -3.940 9.35e-05 ***\r## ptratio -2.640e-02 4.881e-03 -5.409 9.93e-08 ***\r## black 1.313e-03 4.871e-04 2.696 0.00727 ** ## I(black^2) -2.172e-06 1.071e-06 -2.029 0.04303 * ## log(lstat) -3.181e-01 4.553e-02 -6.987 9.23e-12 ***\r## crim -1.049e-02 1.215e-03 -8.635 \u0026lt; 2e-16 ***\r## zn 9.078e-04 5.019e-04 1.809 0.07108 . ## indus -2.733e-04 2.264e-03 -0.121 0.90395 ## chas 7.166e-02 3.191e-02 2.246 0.02515 * ## I(nox^2) -2.569e-01 1.255e-01 -2.048 0.04113 * ## lstat:nox -2.729e-02 4.798e-03 -5.689 2.21e-08 ***\r## lstat:dis 3.906e-03 8.754e-04 4.462 1.01e-05 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## (Dispersion parameter for gaussian family taken to be 0.03033711)\r## ## Null deviance: 84.376 on 505 degrees of freedom\r## Residual deviance: 14.835 on 489 degrees of freedom\r## AIC: -313.99\r## ## Number of Fisher Scoring iterations: 2\rAIC(my_glm_hr_int2)\r## [1] -313.9904\rAIC(my_glm_hr_int)\r## [1] -295.7931\rAnd again we find an improvement in model fit.\n\rHave these interactions already been reported on in the literature?\rTom Minka reports on his website an analysis of interactions in the Boston Housing set:\n(http://alumni.media.mit.edu/~tpminka/courses/36-350.2001/lectures/day30/) \u0026gt; summary(fit3) Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) -227.5485 49.2363 -4.622 4.87e-06 *** lstat 50.8553 20.3184 2.503 0.012639 * rm 38.1245 7.0987 5.371 1.21e-07 *** dis -16.8163 2.9174 -5.764 1.45e-08 *** ptratio 14.9592 2.5847 5.788 1.27e-08 *** lstat:rm -6.8143 3.1209 -2.183 0.029475 * lstat:dis 4.8736 1.3940 3.496 0.000514 *** lstat:ptratio -3.3209 1.0345 -3.210 0.001412 ** rm:dis 2.0295 0.4435 4.576 5.99e-06 *** rm:ptratio -1.9911 0.3757 -5.299 1.76e-07 *** lstat:rm:dis -0.5216 0.2242 -2.327 0.020364 * lstat:rm:ptratio 0.3368 0.1588 2.121 0.034423 *\nRob mcCulloch, using BART (bayesian additive regression trees) also examines interactions in the Boston Housing data. There the co-occurence within trees is used to discover interactions:\nThe second, interaction detection, uncovers which pairs of variables interact in analogous fashion by keeping track of the percentage of trees in the sum in which both variables occur. This exploits the fact that a sum-of-trees model captures an interaction between xi and xj by using them both for splitting rules in the same tree.\nhttp://www.rob-mcculloch.org/some_papers_and_talks/papers/working/cgm_as.pdf\n\r\rConclusion\rWe conclude that this appears a fruitfull approach to at least discovering where a regression model can be improved.\n\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f6e1d7cb765b378e9f12194e8cd7fa31","permalink":"/post/interaction_detection/interaction-detection/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/interaction_detection/interaction-detection/","section":"post","summary":"summary\rThe idea is that comparing the predictions of an RF model with the predictions of an OLS model can inform us in what ways the OLS model fails to capture all non-linearities and interactions between the predictors. Subsequently, using partial dependence plots of the RF model can guide the modelling of the non-linearities in the OLS model. After this step, the discrepancies between the RF predictions and the OLS predictions should be caused by non-modeled interactions.","tags":null,"title":"Interaction detection using Random Forest predictions","type":"post"}]