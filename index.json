[{"authors":["admin"],"categories":null,"content":"I am a research scientist at the Dutch Healthcare Authority, working on health policy and statistical methods. I have a background in experimental physics, with a PhD in Biophysics.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://gsverhoeven.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a research scientist at the Dutch Healthcare Authority, working on health policy and statistical methods. I have a background in experimental physics, with a PhD in Biophysics.","tags":null,"title":"Gertjan Verhoeven","type":"authors"},{"authors":null,"categories":["Statistics"],"content":" Gertjan Verhoeven \u0026amp; Misja Mikkers Here we show how to use Stan with the brms R-package to calculate the posterior predictive distribution of a covariate-adjusted average treatment effect. We fit a model on simulated data that mimics a (very clean) experiment with random treatment assignment.\nIntroduction Suppose we have data from a Randomized Controlled Trial (RCT) and we want to estimate the average treatment effect (ATE). Patients get treated, or not, depending only on a coin flip. This is encoded in the Treatment variable. The outcome is a count variable Admissions, representing the number of times the patient gets admitted to the hospital. The treatment is expected to reduce the number of hospital admissions for patients.\nTo complicate matters (a bit): As is often the case with patients, not all patients are identical. Suppose that older patients have on average more Admissions. So Age is a covariate.\n Average treatment effect (ATE) Now, after we fitted a model to the data, we want to actually use our model to answer \u0026quot;What-if\u0026quot; questions (counterfactuals). Here we answer the following question:\n What would the average reduction in Admissions be if we had treated ALL the patients in the sample, compared to a situation where NO patient in the sample would have received treatment?  Well, that is easy, we just take the fitted model, change treatment from zero to one for each, and observe the (\u0026quot;marginal\u0026quot;) effect on the outcome, right?\nYes, but the uncertainty is harder. We have uncertainty in the estimated coefficients of the intercept and covariate, as well as in the coefficient of the treatment variable. And these uncertainties can be correlated (for example between the coefficients of intercept and covariate).\nHere we show how to use posterior_predict() to simulate outcomes of the model using the sampled parameters. If we do this for two counterfactuals, all patients treated, and all patients untreated, and subtract these, we can easily calculate the posterior predictive distribution of the average treatment effect.\nLet's do it!\n Load packages This tutorial uses brms, a user friendly interface to full Bayesian modelling with Stan.\nlibrary(tidyverse) library(rstan) library(brms)   Data simulation We generate fake data that matches our problem setup.\nAdmissions are determined by patient Age, whether the patient has Treatment, and some random Noise to capture unobserved effects that influence Admissions. We exponentiate them to always get a positive number, and plug it in the Poisson distribution using rpois().\nset.seed(123) id \u0026lt;- 1:200 n_obs \u0026lt;- length(id) b_tr \u0026lt;- -0.7 b_age \u0026lt;- 0.1 df_sim \u0026lt;- as.data.frame(id) %\u0026gt;% mutate(Age = rgamma(n_obs, shape = 5, scale = 2)) %\u0026gt;% # positive cont predictor mutate(Noise = rnorm(n_obs, mean = 0, sd = 0.5)) %\u0026gt;% # add noise mutate(Treatment = ifelse(runif(n_obs) \u0026lt; 0.5, 0, 1)) %\u0026gt;% # Flip a coin for treatment mutate(Lambda = exp(b_age * Age + b_tr * Treatment + Noise)) %\u0026gt;% # generate lambda for the poisson dist mutate(Admissions = rpois(n_obs, lambda = Lambda))  Summarize data Ok, so what does our dataset look like?\nsummary(df_sim) ## id Age Noise Treatment ## Min. : 1.00 Min. : 1.794 Min. :-1.32157 Min. :0.000 ## 1st Qu.: 50.75 1st Qu.: 6.724 1st Qu.:-0.28614 1st Qu.:0.000 ## Median :100.50 Median : 8.791 Median : 0.04713 Median :0.000 ## Mean :100.50 Mean : 9.474 Mean : 0.02427 Mean :0.495 ## 3rd Qu.:150.25 3rd Qu.:11.713 3rd Qu.: 0.36025 3rd Qu.:1.000 ## Max. :200.00 Max. :24.835 Max. : 1.28573 Max. :1.000 ## Lambda Admissions ## Min. : 0.2479 Min. : 0.000 ## 1st Qu.: 1.1431 1st Qu.: 1.000 ## Median : 1.8104 Median : 2.000 ## Mean : 2.6528 Mean : 2.485 ## 3rd Qu.: 3.0960 3rd Qu.: 3.000 ## Max. :37.1296 Max. :38.000 The Treatment variable should reduce admissions. Lets visualize the distribution of Admission values for both treated and untreated patients.\nggplot(data = df_sim, aes(x = Admissions)) + geom_histogram(stat=\u0026quot;count\u0026quot;) + facet_wrap(~ Treatment)  The effect of the treatment on reducing admissions is clearly visible.\nWe can also visualize the relationship between Admissions and Age, for both treated and untreated patients. We use the viridis scales to provide colour maps that are designed to be perceived by viewers with common forms of colour blindness.\nggplot(data = df_sim, aes(x = Age, y = Admissions, color = as.factor(Treatment))) + geom_point() + scale_color_viridis_d(labels = c(\u0026quot;No Treatment\u0026quot;, \u0026quot;Treatment\u0026quot;)) + labs(color = \u0026quot;Treatment\u0026quot;) Now lets fit our Bayesian Poisson regression model to it.\n Fit model We use brms default priors for convenience here. For a real application we would of course put effort into into crafting priors that reflect our current knowledge of the problem at hand.\nmodel1 \u0026lt;- brm( formula = as.integer(Admissions) ~ Age + Treatment, data = df_sim, family = poisson(), warmup = 2000, iter = 5000, cores = 2, chains = 4, seed = 123, silent = TRUE, refresh = 0, ) ## Compiling Stan program... ## Start sampling  Check model fit summary(model1) ## Family: poisson ## Links: mu = log ## Formula: as.integer(Admissions) ~ Age + Treatment ## Data: df_sim (Number of observations: 200) ## Samples: 4 chains, each with iter = 5000; warmup = 2000; thin = 1; ## total post-warmup samples = 12000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.05 0.12 -0.28 0.18 1.00 7410 7333 ## Age 0.12 0.01 0.10 0.14 1.00 8052 8226 ## Treatment -0.83 0.10 -1.02 -0.63 1.00 7794 7606 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). We see that the posterior dists for \\(\\beta_{Age}\\) and \\(\\beta_{Treatment}\\) cover the true values, so looking good. To get a fuller glimpse into the (correlated) uncertainty of the model parameters we make a pairs plot:\npairs(model1) As expected, the coefficients \\(\\beta_{Intercept}\\) (added by brms) and \\(\\beta_{Age}\\) are highly correlated.\n First attempt: Calculate Individual Treatment effects using the model fit object Conceptually, the simplest approach for prediction is to take the most likely values for all the model parameters, and use these to calculate for each patient an individual treatment effect. This is what plain OLS regression does when we call predict.lm() on a fitted model.\nest_intercept \u0026lt;- fixef(model1, pars = \u0026quot;Intercept\u0026quot;)[,1] est_age_eff \u0026lt;- fixef(model1, pars = \u0026quot;Age\u0026quot;)[,1] est_t \u0026lt;- fixef(model1, pars = \u0026quot;Treatment\u0026quot;)[,1] # brm fit parameters (intercept plus treatment) ites \u0026lt;- exp(est_intercept + (est_age_eff * df_sim$Age) + est_t) - exp(est_intercept + (est_age_eff * df_sim$Age)) ggplot(data.frame(ites), aes(x = ites)) + geom_histogram() + geom_vline(xintercept = mean(ites), col = \u0026quot;red\u0026quot;) + ggtitle(\u0026quot;Effect of treatment on Admissions for each observation\u0026quot;) + expand_limits(x = 0)  Averaging the ITEs gives us the ATE, displayed in red.\nOk, so on average, our treatment reduces the number of Admissions by -1.9.\nYou may wonder: why do we even have a distribution of treatment effects here? Should it not be the same for each patient? Here a peculiarity of the Poisson regression model comes to surface: The effect of changing Treatment from 0 to 1 on the outcome depends on the value of Age of the patient. This is because we exponentiate the linear model before we plug it into the Poisson distribution.\n Next, the uncertainty in the ATE How to get all this underlying, correlated uncertainty in the model parameters, that have varying effects depending on the covariates of patients, and properly propagate that to the ATE? What is the range of plausible values of the ATE consistent with the data \u0026amp; model?\nAt this point, using only the summary statistics of the model fit (i.e. the coefficients), we hit a wall. To make progress we have to work with the full posterior distribution of model parameters, and use this to make predictions. That is why it is often called \u0026quot;the posterior predictive distribution\u0026quot; (Check BDA3 for the full story).\n Posterior predictive distribution (PPD): two tricks Ok, you say, a Posterior Predictive Distribution, let's have it! Where can I get one?\nLuckily for us, most of the work is already done, because we have fitted our model. And thus we have a large collection of parameter draws (or samples, to confuse things a bit). All the correlated uncertainty is contained in these draws.\nThis is the first trick. Conceptually, we imagine that each separate draw of the posterior represents a particular version of our model.\nIn our example model fit, we have 12.000 samples from the posterior. In our imagination, we now have 12.000 versions of our model, where unlikely parameter combinations are present less often compared to likely parameter combinations. The full uncertainty of our model parameters is contained in this \u0026quot;collection of models\u0026quot; .\nThe second trick is that we simulate (generate) predictions for all observations, from each of these 12.000 models. Under the hood, this means computing for each model (we have 12.000), for each observation (we have 200) the predicted lambda value given the covariates, and drawing a single value from a Poisson distribution with that \\(\\Lambda\\) value (e.g. running rpois(n = 1, lambda) ).\nThis gives us a 12.000 x 200 matrix, that we can compute with.\n Computing with the PPD: brms::posterior_predict() To compute PPD's, we can use brms::posterior_predict(). We can feed it any dataset using the newdata argument, and have it generate a PPD.\nFor our application, the computation can be broken down in two steps:\n Step 1: use posterior_predict() on our dataset with Treatment set to zero, do the same for our dataset with Treatment set to one, and subtract the two matrices. This gives us a matrix of outcome differences / treatment effects. Step 2: Averaging over all cols (the N=200 simulated outcomes for each draw) should give us the distribution of the ATE. This distribution now represents the variability (uncertainty) of the estimate.  Ok, step 1:\n# create two versions of our dataset, with all Tr= 0 and all Tr=1 df_sim_t0 \u0026lt;- df_sim %\u0026gt;% mutate(Treatment = 0) df_sim_t1 \u0026lt;- df_sim %\u0026gt;% mutate(Treatment = 1) # simulate the PPDs pp_t0 \u0026lt;- posterior_predict(model1, newdata = df_sim_t0) pp_t1 \u0026lt;- posterior_predict(model1, newdata = df_sim_t1) diff \u0026lt;- pp_t1 - pp_t0 dim(diff) ## [1] 12000 200 And step 2 (averaging by row over the cols):\nATE_per_draw \u0026lt;- apply(diff, 1, mean) # equivalent expression for tidyverse fans #ATE_per_draw \u0026lt;- data.frame(diff) %\u0026gt;% rowwise() %\u0026gt;% summarise(avg = mean(c_across(cols = everything()))) length(ATE_per_draw) ## [1] 12000 Finally, a distribution of plausible ATE values. Oo, that is so nice. Lets visualize it!\nggplot(data.frame(ATE_per_draw), aes(x = ATE_per_draw)) + geom_histogram() + geom_vline(xintercept = mean(ites), col = \u0026quot;red\u0026quot;) + ggtitle(\u0026quot;Posterior distribution of the Average Treatment Effect (ATE)\u0026quot;) We can compare this distribution with the point estimate of the ATE we obtained above using the model coefficients. It sits right in the middle (red line), just as it should be!\n Demonstrating the versatility: uncertainty in the sum of treatment effects Now suppose we are a policy maker, and we want to estimate the total reduction in Admissions if all patients get the treatment. And we want to quantify the range of plausible values of this summary statistic.\nTo do so, we can easily adjust our code to summing instead of averaging all the treatment effects within each draw (i.e. by row):\nTTE_per_draw \u0026lt;- apply(diff, 1, sum) ggplot(data.frame(TTE_per_draw), aes(x = TTE_per_draw)) + geom_histogram() + geom_vline(xintercept = sum(ites), col = \u0026quot;red\u0026quot;) + ggtitle(\u0026quot;Posterior distribution of the Total Treatment Effect (TTE)\u0026quot;) So our model predicts for the aggregate reduction of patient Admissions a value in the range of -500 to -250.\nThis distribution can then be used to answer questions such as \u0026quot;what is the probability that our treatment reduces Admissions by at least 400\u0026quot;?\nTTE \u0026lt;- data.frame(TTE_per_draw) %\u0026gt;% mutate(counter = ifelse(TTE_per_draw \u0026lt; -400, 1, 0)) mean(TTE$counter) * 100 ## [1] 38.1  Take home message: PPD with brms is easy and powerful We hope to have demonstrated that when doing a full bayesian analysis with brms and Stan, it is very easy to create Posterior Predictive Distributions using posterior_predict(). And that if we have a posterior predictive distribution, incorporating uncertainty in various \u0026quot;marginal effects\u0026quot; type analyses becomes dead-easy. These analyses include what-if scenarios using the original data, or scenarios using new data with different covariate distributions (for example if we have an RCT that is enriched in young students, and we want to apply it to the general population). Ok, that it is for today, happy modelling!\n  ","date":1599177600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599177600,"objectID":"3b68b9f9a404fe9174d7e7aaeedfb038","permalink":"https://gsverhoeven.github.io/post/posterior-distribution-average-treatment-effect/","publishdate":"2020-09-04T00:00:00Z","relpermalink":"/post/posterior-distribution-average-treatment-effect/","section":"post","summary":"Here we show how to use Stan and the brms R-package to calculate the posterior predictive distribution of a covariate-adjusted average treatment effect (ATE).","tags":["brms","causal inference","bayesian statistics","Stan","rstan"],"title":"Using posterior predictive distributions to get the Average Treatment Effect (ATE) with uncertainty","type":"post"},{"authors":null,"categories":["Machine learning","Deep learning"],"content":"    Introduction I don't change computers often. The fun for me is to make the most out of sparse resources. Linux fits nicely into this philosophy, because it can be adapted to run on really tiny computers (e.g. http://www.picotux.com/), as well as huge supercomputers (https://itsfoss.com/linux-runs-top-supercomputers/). I do like to keep up with new tech developments. And with the commoditization of deep learning in the form of Keras, I felt it was about time that I finally jumped on the Deep Learning bandwagon.\nAnd the nice thing about lagging behind: The choice for deep learning is now extremely simple. I need Keras with TensorFlow as a computational backend. Which nowadays means installing TensorFlow since the Keras API has been incorporated into the TensorFlow project.\n TensorFlow and AVX Then I ran into a problem: TensorFlow is all about FAST computation. And therefore it tries to exploit all hardware features that speed up computation. One obvious way to do so is utilizing specialized hardware such as GPU's and TPU's to do the number crunching. But even for CPU's, TensorFlow likes to make use of all the computational features that modern CPU's offer. One of these is the \u0026quot;Advanced Vector Instruction Set\u0026quot; , aka AVX. As most CPU's from 2011 or later support AVX, the TensorFlow folks decided to only make binaries available that require a CPU with AVX. Bummer for me: as my CPU is from 2010, I needed to compile TensorFlow myself.\nBut come to think of it: What better rite of passage into the Deep Learning AI age is to compile TensorFlow from source on your own machine??? (Opening music of Space Odyssey 2001 in the background)\n Building TensorFlow on a really old computer I followed the tutorial from TensorFlow to build from source on a Linux system (Ubuntu 18.04 LTS). Therefore, these notes are most useful to other Linux users, and my future self of course.\nRoughly this consisted of:\n Creating a virtual environment for Python 3.6.9 Checking my GCC version (7.5.0, which is greater than 7.3 that is used for the official TF packages) Clone the TensorFlow repository from GitHub Git checkout the latest official TensorFlow release (v2.2) Installed the latest release of Bazel (Google's Make program), version 3.1. Then install exactly the right version needed for TF2.2 (2.0.0, as specified by MIN_BAZEL_VERSION in tensorflow/configure.py, use .baselversion to easily install multiple bazel versions side by side)  Then came the hard part, the final step:\n Tweak Bazel arguments endlessly to reduce resource usage to be able to complete the build process succesfully  In the end, I removed the -c opt, so no special optimization for my CPU. And asked for one CPU (I have two cores :-), one job, and max 2GB of RAM usage.\ncd tf_build_env/ source bin/activate cd ~/Github/tensorflow/ bazel build --config=opt --local_ram_resources=2048 --local_cpu_resources=HOST_CPUS-1 --jobs=1 //tensorflow/tools/pip_package:build_pip_package I ran the build process in a terminal on the Ubuntu 18.04 Desktop, without any other programs loaded. My 2010 PC has in total 4 GB of RAM. As the Ubuntu Desktop + OS consumes about 1-1.5 GB on my system, this leaves about 2.5-3.0 GB for bazel. Now as it turns out, according to htop memory consumption went up to 3.6 GB (of my 3.9GB max), but it succeeded in the end. This was after 10 hours of compiling! (I let it run overnight)\nThe final step was to turn the compiled TensorFlow into a Python Wheel package ready to install using pip.\n./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg # creates a \u0026#39;wheel\u0026#39; file called tensorflow-2.2.0-cp36-cp36m-linux_x86_64.whl To try it out, I created a new empty Python 3 virtual environment with only TensorFlow and Jupyter Notebook installed. To my delight it ran the Fashion MNIST classification with Keras example flawlessly.\nAnd even on my ancient PC performance was quite good, training the model took around 1 minute. So, after glorious succes in Python, it was time to move on to R.\n Keras in R with the classic MNIST I had to install the development version of the R package keras from GitHub to fix a bug that prevented Keras in R from working with TF v2.2.\nFrom the release notes: (https://github.com/rstudio/keras/blob/master/NEWS.md)\nFixed issue regarding the KerasMetricsCallback with TF v2.2 (#1020)\ndevtools::install_github(\u0026quot;rstudio/keras\u0026quot;) For my first deep learning in R, I followed the tutorial from https://tensorflow.rstudio.com/tutorials/beginners/\nFirst load all the required packages.\nlibrary(tensorflow) use_virtualenv(\u0026quot;~/venvs/keras_env\u0026quot;, required = TRUE) # this was the same environment that I tested TensorFlow with Python library(keras) Read in the dataset.\nmnist \u0026lt;- dataset_mnist() Rescale pixel values to be between 0 and 1.\nmnist$train$x \u0026lt;- mnist$train$x/255 mnist$test$x \u0026lt;- mnist$test$x/255 Plot the data.\nx_train \u0026lt;- mnist$train$x y_train \u0026lt;- mnist$train$y # visualize the digits par(mfcol=c(6,6)) par(mar=c(0, 0, 3, 0), xaxs=\u0026#39;i\u0026#39;, yaxs=\u0026#39;i\u0026#39;) for (idx in 1:12) { im \u0026lt;- x_train[idx,,] im \u0026lt;- t(apply(im, 2, rev)) image(1:28, 1:28, im, col=gray((0:255)/255), xaxt=\u0026#39;n\u0026#39;, main=paste(y_train[idx])) }  Keras model model \u0026lt;- keras_model_sequential() %\u0026gt;% layer_flatten(input_shape = c(28, 28)) %\u0026gt;% layer_dense(units = 128, activation = \u0026quot;relu\u0026quot;) %\u0026gt;% layer_dropout(0.2) %\u0026gt;% layer_dense(10, activation = \u0026quot;softmax\u0026quot;) summary(model) ## Model: \u0026quot;sequential\u0026quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## flatten (Flatten) (None, 784) 0 ## ________________________________________________________________________________ ## dense (Dense) (None, 128) 100480 ## ________________________________________________________________________________ ## dropout (Dropout) (None, 128) 0 ## ________________________________________________________________________________ ## dense_1 (Dense) (None, 10) 1290 ## ================================================================================ ## Total params: 101,770 ## Trainable params: 101,770 ## Non-trainable params: 0 ## ________________________________________________________________________________ It has over 100.000 parameters!!\nPython has a nice plot_model() function, in R we can use the deepviz package.\ndevtools::install_github(\u0026quot;andrie/deepviz\u0026quot;) library(deepviz) library(magrittr) model %\u0026gt;% plot_model() ## Warning: `select_()` is deprecated as of dplyr 0.7.0. ## Please use `select()` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. ## Warning: The `x` argument of `as_tibble.matrix()` must have unique column names if `.name_repair` is omitted as of tibble 2.0.0. ## Using compatibility `.name_repair`. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated.  {\"x\":{\"diagram\":\"digraph {\\n\\ngraph [layout = \\\"neato\\\",\\n outputorder = \\\"edgesfirst\\\",\\n bgcolor = \\\"white\\\"]\\n\\nnode [fontname = \\\"Helvetica\\\",\\n fontsize = \\\"10\\\",\\n shape = \\\"circle\\\",\\n fixedsize = \\\"true\\\",\\n width = \\\"0.5\\\",\\n style = \\\"filled\\\",\\n fillcolor = \\\"aliceblue\\\",\\n color = \\\"gray70\\\",\\n fontcolor = \\\"gray50\\\"]\\n\\nedge [fontname = \\\"Helvetica\\\",\\n fontsize = \\\"8\\\",\\n len = \\\"1.5\\\",\\n color = \\\"gray80\\\",\\n arrowsize = \\\"0.5\\\"]\\n\\n \\\"1\\\" [label = \\\"flatten\\nFlatten\\n\\\", shape = \\\"rectangle\\\", fixedsize = \\\"FALSE\\\", fillcolor = \\\"#F0F8FF\\\", fontcolor = \\\"#000000\\\", pos = \\\"0,4!\\\"] \\n \\\"2\\\" [label = \\\"dense\\nDense\\nrelu\\\", shape = \\\"rectangle\\\", fixedsize = \\\"FALSE\\\", fillcolor = \\\"#F0F8FF\\\", fontcolor = \\\"#000000\\\", pos = \\\"0,3!\\\"] \\n \\\"3\\\" [label = \\\"dropout\\nDropout\\n\\\", shape = \\\"rectangle\\\", fixedsize = \\\"FALSE\\\", fillcolor = \\\"#F0F8FF\\\", fontcolor = \\\"#000000\\\", pos = \\\"0,2!\\\"] \\n \\\"4\\\" [label = \\\"dense_1\\nDense\\nsoftmax\\\", shape = \\\"rectangle\\\", fixedsize = \\\"FALSE\\\", fillcolor = \\\"#F0F8FF\\\", fontcolor = \\\"#000000\\\", pos = \\\"0,1!\\\"] \\n \\\"1\\\"-\\\"2\\\" \\n \\\"2\\\"-\\\"3\\\" \\n \\\"3\\\"-\\\"4\\\" \\n}\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}  Compile the model model %\u0026gt;% compile( loss = \u0026quot;sparse_categorical_crossentropy\u0026quot;, optimizer = \u0026quot;adam\u0026quot;, metrics = \u0026quot;accuracy\u0026quot; )  Fit the model model %\u0026gt;% fit( x = mnist$train$x, y = mnist$train$y, epochs = 5, validation_split = 0.3, verbose = 1 )  Make predictions predictions \u0026lt;- predict(model, mnist$test$x) Visualize a single prediction:\nlibrary(ggplot2) id \u0026lt;- 9 ggplot(data.frame(digit = 0:9, prob = predictions[id,]), aes(x = factor(digit), y = prob)) + geom_col() + ggtitle(paste0(\u0026quot;prediction for true value of \u0026quot;, mnist$test$y[id]))  Check model performance on the test set model %\u0026gt;% evaluate(mnist$test$x, mnist$test$y, verbose = 0) ## loss accuracy ## 0.08376268 0.97469997 Our model achieved ~98% accuracy on the test set.\nAwesome.\n ","date":1589500800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589500800,"objectID":"121bd5538454651a1b9f26c355ee6b51","permalink":"https://gsverhoeven.github.io/post/deep-learning-tensorflow-keras/","publishdate":"2020-05-15T00:00:00Z","relpermalink":"/post/deep-learning-tensorflow-keras/","section":"post","summary":"With the commoditization of deep learning in the form of Keras, I felt it was about time that I jumped on the Deep Learning bandwagon.","tags":["tensorflow","keras","neural networks","deep learning"],"title":"Building TensorFlow 2.2 on an old PC","type":"post"},{"authors":null,"categories":["Data science"],"content":" This blog post is on simulating fake data. I'm interested in creating synthetic versions of real datasets. For example if the data is too sensitive to be shared, or we only have summary statistics available (for example tables from a published research paper).\nIf we want to mimic an existing dataset, it is desirable to\n Make sure that the simulated variables have the proper data type and comparable distribution of values and correlations between the variables in the real dataset are taken into account.  In addition, it would be nice if such functionality is available in a standard R package. After reviewing several R packages that can simulate data, I picked the simstudy package as most promising to explore in more detail. simstudy is created by Keith Goldfeld from New York University.\nIn this blog post, I explain how simstudy is able to generate correlated variables, having either continuous or binary values. Along the way, we learn about fancy statistical slang such as copula's and tetrachoric correlations. It turns out there is a close connection with psychometrics, which we'll briefly discuss.\nLet's start with correlated continuous variables.\n# Loading required packages library(simstudy) library(data.table) library(ggplot2) Copulas: Simulating continuous correlated variables Copulas are a fancy word for correlated (\u0026quot;coupled\u0026quot;) variables that each have a uniform distribution between 0 and 1.\nUsing copulas, we can convert correlated multivariate normal data to data from any known continuous probability distribution, while keeping exactly the same correlation matrix. The normal data is something we can easily simulate, and by choosing appropriate probability distributions, we can approximate the variables in real datasets.\nOk let's do it!\nStep 1: correlated multivariate normal data The workhorse for our simulated data is a function to simulate multivariate normal data. We'll use the MASS package function mvrnorm(). Other slightly faster (factor 3-4) implementations exist, see e.g. mvnfast.\nThe trick is to first generate multivariate normal data with the required correlation structure, with mean 0 and standard deviation 1. This gives us correlated data, where each variable is marginally (by itself) normal distributed.\nHere I simulate two variables, but the same procedure holds for N variables. The Pearson correlation is set at 0.7.\nset.seed(123) corr \u0026lt;- 0.7 cov.mat \u0026lt;- matrix(c(1, corr, corr, 1), nrow = 2) df \u0026lt;- data.frame(MASS::mvrnorm(n = 1e4, mu = c(0, 0), Sigma = cov.mat)) (The diagonal of 1 makes sure the variables have SD of 1. The off diagonal value of 0.7 gives us a Pearson correlation of 0.7)\nDid it work?\nggplot(df, aes(x = X1, y = X2)) + geom_point(alpha = 0.3) cor(df$X1, df$X2) ## [1] 0.6985089 Great!\n Step 2: transform variables to uniform distribution Using the normal cumulative distribution function pnorm(), we can transform our normally distributed variables to have a uniform distribution, while keeping the correlation structure intact!!!!\ndf$X1_U \u0026lt;- pnorm(df$X1) df$X2_U \u0026lt;- pnorm(df$X2) ggplot(df, aes(x = X1_U)) + geom_histogram(boundary = 0) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ggplot(df, aes(x = X1_U, y = X2_U)) + geom_point(alpha = 0.3) And here's our copula! Two variables, each marginally (by itself) uniform, but with pre-specified correlation intact!\ncor(df$X1_U, df$X2_U) ## [1] 0.677868  Step 3: from uniform to any standard probability distribution we like Now, if we plug in uniformly distributed data in a quantile function of any arbitrary (known) probability distribution, we can make the variables have any distribution we like.\nLet's pick for example a Gamma distribution (Continuous, positive) with shape 4 and rate 1 for X1, and Let's pick a Normal distribution (Continuous, symmetric) with mean 10 and sd 2 for X2.\ndf$X1_GAM \u0026lt;- qgamma(df$X1_U, shape = 4, rate =1) df$X2_NORM \u0026lt;- qnorm(df$X2_U, mean = 10, sd = 2) ggplot(df, aes(x = X1_GAM)) + geom_histogram(boundary = 0) + geom_vline(xintercept = 4, col = \u0026quot;red\u0026quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ggplot(df, aes(x = X2_NORM)) + geom_histogram(boundary = 0) + geom_vline(xintercept = 10, col = \u0026quot;red\u0026quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Ok, that worked nicely. But what about their correlation?\ncor(df$X1_GAM, df$X2_NORM) ## [1] 0.682233 Whoa!! They still have (almost) the same correlation we started out with before all our transformation magic.\n  Simstudy in action Now let's see how simstudy helps us generating this type of simulated data. Simstudy works with \u0026quot;definition tables\u0026quot; that allow us to specify, for each variable, which distribution and parameters to use, as well as the desired correlations between the variables.\nAfter specifing a definition table, we can call one of its workhorse functions genCorFlex() to generate the data.\nN.b. Simstudy uses different parameters for the Gamma distribution, compared to R's rgamma() function. Under water, it uses the gammaGetShapeRate() to transform the \u0026quot;mean\u0026quot; and \u0026quot;variance/ dispersion\u0026quot; to the more conventional \u0026quot;shape\u0026quot; and \u0026quot;rate\u0026quot; parameters.\nset.seed(123) corr \u0026lt;- 0.7 corr.mat \u0026lt;- matrix(c(1, corr, corr, 1), nrow = 2) # check that gamma parameters correspond to same shape and rate pars as used above #simstudy::gammaGetShapeRate(mean = 4, dispersion = 0.25) def \u0026lt;- defData(varname = \u0026quot;X1_GAM\u0026quot;, formula = 4, variance = 0.25, dist = \u0026quot;gamma\u0026quot;) def \u0026lt;- defData(def, varname = \u0026quot;X2_NORM\u0026quot;, formula = 10, variance = 2, dist = \u0026quot;normal\u0026quot;) dt \u0026lt;- genCorFlex(1e4, def, corMatrix = corr.mat) cor(dt[,-\u0026quot;id\u0026quot;]) ## X1_GAM X2_NORM ## X1_GAM 1.0000000 0.6823006 ## X2_NORM 0.6823006 1.0000000 ggplot(dt, aes(x = X1_GAM)) + geom_histogram(boundary = 0) + geom_vline(xintercept = 4, col = \u0026quot;red\u0026quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.  Generate correlated binary variables As it turns out, the copula approach does not work for binary variables. Well, it sort of works, but the correlations we get are lower than we actually specify.\nCome to think of it: two binary variables cannot have all the correlations we like. To see why, check this out.\nFeasible correlations for two binary variables Let's suppose we have a binary variable that equals 1 with probability 0.2, and zero otherwise. This variable will never be fully correlated with a binary variable that equals 1 with probability 0.8, and zero otherwise.\nTo see this, I created two binary vectors that have a fraction 0.2 and 0.8 of 1's, and let's see if we can arrange the values in both vectors in such a way that minimizes and maximizes their correlation:\n# maximal correlation x1 \u0026lt;- c(0, 0, 0, 0, 1) x2 \u0026lt;- c(0, 1, 1, 1, 1) mean(x1) ## [1] 0.2 mean(x2) ## [1] 0.8 cor(x1, x2) ## [1] 0.25 # minimal correlation x1 \u0026lt;- c(1, 0, 0, 0, 0) x2 \u0026lt;- c(0, 1, 1, 1, 1) cor(x1, x2) ## [1] -1 To get these vectors to be maximally correlated, we need to match 1's in x1 as much as possible with 1s in x2. To get these vectors to be maximally anti-correlated, we need to match 1s in x1 with as many 0s in x2.\nIn this example, we conclude that the feasible correlation range is {-1, 0.25}.\nThe simstudy package contains a function to check for feasible boundaries, that contains this piece of code:\np1 \u0026lt;- 0.2 p2 \u0026lt;- 0.8 # lowest correlation l \u0026lt;- (p1 * p2)/((1 - p1) * (1 - p2)) max(-sqrt(l), -sqrt(1/l)) ## [1] -1 # highest correlation u \u0026lt;- (p1 * (1 - p2))/(p2 * (1 - p1)) min(sqrt(u), sqrt(1/u)) ## [1] 0.25 This confirms our example above.\nNote that if we want to mimic a real dataset with binary correlated variables, the correlations are a given, and are obviously all feasible because we obtain them from actual data.\n A model for two correlated binary variables Ok let's suppose we want a two binary vectors B1 and B2 , with means p1 = 0.2 and p2 = 0.8 and (feasible) Pearson correlation 0.1.\nHow? How?\nThe idea is that to get two binary variables to have an exact particular correlation, we imagine an underlying (\u0026quot;latent\u0026quot;) bivariate (2D) normal distribution. This normal distribution has the means fixed to 0, and the standard deviations fixed to 1.\nWhy? Because a) we know it very well theoretically and b) we know how to simulate efficiently from such a distribution, using mvrnorm().\nIn this bivariate normal distribution, we draw a quadrant (i.e. two thresholds). The thresholds define transformations to binary variables. Below the threshold, the binary value is 0, above it is 1. We have to pick the thresholds such that the resulting binary variables have the desired mean (i.e. percentage of 1's).\nThis approach reduces the problem to finding the right values of three parameters: multivariate normal correlation, and the two thresholds (above, we already fixed the means and variance to zero and one respectively).\nFor now, we'll just pick some value for the correlation in the bivariate normal, say 0.5, and focus on where to put the threshholds.\nset.seed(123) corr \u0026lt;- 0.5 cov.mat \u0026lt;- matrix(c(1, corr, corr, 1), nrow = 2) df \u0026lt;- data.frame(MASS::mvrnorm(n = 10000, mu = c(0, 0), Sigma = cov.mat)) (The diagonal of 1 makes sure the variables have SD of 1. The off diagonal value of 0.7 gives us a Pearson correlation of 0.7)\nggplot(df, aes(x = X1, y = X2)) + geom_point(alpha = 0.3) Ok, where to put the thresholds? That's simple, we just need to use the quantile distribution function to partition the marginal normal variables into 0 and 1 portions.\ndf$B1 \u0026lt;- ifelse(df$X1 \u0026lt; qnorm(0.2), 1, 0) df$B2 \u0026lt;- ifelse(df$X2 \u0026lt; qnorm(0.8), 1, 0) mean(df$B1) ## [1] 0.197 mean(df$B2) ## [1] 0.7988 Let's check it out visually:\nggplot(df, aes(x = X1, y = X2)) + geom_point(alpha = 0.3) + geom_vline(xintercept = qnorm(0.2), col = \u0026quot;red\u0026quot;) + geom_hline(yintercept = qnorm(0.8), col = \u0026quot;red\u0026quot;) Nice.\nOk, so now what is the correlation for these two binary variables?\ncor(df$B1, df$B2) ## [1] 0.1877482 Ok, so if X1 and X2 have a correlation of 0.5, this results in a correlation of 0.19 between the binary variables B1 and B2.\nBut we need B1 and B2 to have a correlation of 0.1!\nAt this point, there is only one free parameter left, the correlation of the normally distributed variables X1 and X2.\nWe could of course manually try to find which correlation we must choose between X1 and X2 to get the desired correlation of 0.1 in the binary variables. But that would be very unpractical.\nFortunately, Emrich and Piedmonte (1991) published an iterative method to solve this puzzle. And this method has been implemented in simstudy.\nsimstudy:::.findRhoBin(p1 = 0.2, p2 = 0.8, d = 0.1) ## [1] 0.2218018 Let's see if it works:\nset.seed(1234) corr \u0026lt;- 0.2218018 cov.mat \u0026lt;- matrix(c(1, corr, corr, 1), nrow = 2) df \u0026lt;- data.frame(MASS::mvrnorm(n = 1e6, mu = c(0, 0), Sigma = cov.mat)) df$B1 \u0026lt;- ifelse(df$X1 \u0026lt; qnorm(0.2), 1, 0) df$B2 \u0026lt;- ifelse(df$X2 \u0026lt; qnorm(0.8), 1, 0) cor(df$B1, df$B2) ## [1] 0.09957392 Great!\n  Relation to psychometrics So what has psychometrics to do with all this simulation of correlated binary vector stuff?\nWell, psychometrics is all about theorizing about unobserved, latent, imaginary \u0026quot;constructs\u0026quot;, such as attitude, general intelligence or a personality trait. To measure these constructs, questionnaires are used. The questions are called items.\nNow imagine a situation where we are interested in a particular construct, say general intelligence, and we design two questions to measure (hope to learn more about) the construct. Furthermore, assume that one question is more difficult than the other question. The answers to both questions can either be wrong or right.\nWe can model this by assuming that the (imaginary) variable \u0026quot;intelligence\u0026quot; of each respondent is located on a two-dimensional plane, with the distribution of the respondents determined by a bivariate normal distribution. Dividing this plane into four quadrants then gives us the measurable answers (right or wrong) to both questions. Learning the answers to both questions then gives us an approximate location of a respondent on our \u0026quot;intelligence\u0026quot; plane!\nPhi, tetrachoric correlation and the psych package Officially, the Pearson correlation between two binary vectors is called the Phi coefficient. This name was actually chosen by Karl Pearson himself.\nThe psych packages contains a set of convenient functions for calculating Phi coefficients from empirical two by two tables (of two binary vectors), and finding the corresponding Pearson coefficient for the 2d (latent) normal. This coefficient is called the tetrachoric correlation. Again a fine archaic slang word for again a basic concept.\nlibrary(psych) ## ## Attaching package: \u0026#39;psych\u0026#39; ## The following objects are masked from \u0026#39;package:ggplot2\u0026#39;: ## ## %+%, alpha # convert simulated binary vectors B1 and B2 to 2x2 table twobytwo \u0026lt;- table(df$B1, df$B2)/nrow(df) phi(twobytwo, digits = 6) ## [1] 0.099574 cor(df$B1, df$B2) ## [1] 0.09957392 # both give the same result We can use phi2tetra to find the tetrachoric correlation that corresponds to the combination of a \u0026quot;Phi coefficient\u0026quot;, i.e. the correlation between the two binary vectors, as well as their marginals. This is a wrapper that builds the two by two frequency table and then calls tetrachoric() . This in turn uses optimize (Maximum Likelihood method?) to find the tetrachoric correlation.\nphi2tetra(0.1, c(0.2, 0.8)) ## [1] 0.2217801 # compare with EP method simstudy:::.findRhoBin(0.2, 0.8, 0.1) ## [1] 0.2218018 Comparing with the Emrich and Piedmonte method, we find that they give identical answers. Great, case closed!\n  Simstudy in action II Now that we feel confident in our methods and assumptions, let's see simstudy in action.\nLet's generate two binary variables, that have marginals of 20% and 80% respectively, and a Pearson correlation coefficient of 0.1.\nset.seed(123) corr \u0026lt;- 0.1 corr.mat \u0026lt;- matrix(c(1, corr, corr, 1), nrow = 2) res \u0026lt;- simstudy::genCorGen(10000, nvars = 2, params1 = c(0.2, 0.8), corMatrix = corr.mat, dist = \u0026quot;binary\u0026quot;, method = \u0026quot;ep\u0026quot;, wide = TRUE) # let\u0026#39;s check the result cor(res[, -c(\u0026quot;id\u0026quot;)]) ## V1 V2 ## V1 1.00000000 0.09682531 ## V2 0.09682531 1.00000000 Awesome, it worked!\n Conclusion Recall, my motivation for simulating fake data with particular variable types and correlation structure is to mimic real datasets.\nSo are we there yet? Well, we made some progress. We now can handle correlated continuous data, as well as correlated binary data.\nBut we need to solve two more problems:\n To simulate a particular dataset, we still need to determine for each variable its data type (binary or continuous), and if it's continuous, what is the most appropriate probability distribution (Normal, Gamma, Log-normal, etc).\n we haven't properly solved correlation between dissimilar data types, e.g. a correlation between a continuous and a binary variable.\n  Judging from the literature (Amatya \u0026amp; Demirtas 2016) and packages such as SimMultiCorrData by Allison Fialkowski, these are both solved, and I only need to learn about them! So, to be continued.\n ","date":1572048000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572048000,"objectID":"f44c3fb8bebb1b30f38acae8e2784c28","permalink":"https://gsverhoeven.github.io/post/simulating-fake-data/","publishdate":"2019-10-26T00:00:00Z","relpermalink":"/post/simulating-fake-data/","section":"post","summary":"This blog post is on simulating fake data using the R package [simstudy](https://www.rdatagen.net/page/simstudy/). Motivation comes from my interest in converting real datasets into synthetic ones.","tags":["fake data","simulation","copula","tetrachoric correlation","R"],"title":"Simulating Fake Data in R","type":"post"},{"authors":null,"categories":["Data science"],"content":" In this post, we'll explore the BupaR suite of Process Mining packages created by Gert Janssenswillen from Hasselt University.\nWe start with exploring the patients dataset contained in the eventdataR package. According to the documentation, this is an \u0026quot;Artifical eventlog about patients\u0026quot;.\nGetting started After installing all required packages, we can load the whole \u0026quot;bupaverse\u0026quot; by loading the bupaR package.\nlibrary(ggplot2) library(bupaR) ## Warning in library(package, lib.loc = lib.loc, character.only = TRUE, ## logical.return = TRUE, : there is no package called \u0026#39;xesreadR\u0026#39; ## Warning in library(package, lib.loc = lib.loc, character.only = TRUE, ## logical.return = TRUE, : there is no package called \u0026#39;processmonitR\u0026#39; ## Warning in library(package, lib.loc = lib.loc, character.only = TRUE, ## logical.return = TRUE, : there is no package called \u0026#39;petrinetR\u0026#39; library(processmapR) Now, our dataset is already in eventlog format, but typically this not the case. Here's how to turn a data.frame into an object of class eventlog:\npatients \u0026lt;- eventdataR::patients df \u0026lt;- eventlog(patients, case_id = \u0026quot;patient\u0026quot;, activity_id = \u0026quot;handling\u0026quot;, activity_instance_id = \u0026quot;handling_id\u0026quot;, lifecycle_id = \u0026quot;registration_type\u0026quot;, timestamp = \u0026quot;time\u0026quot;, resource_id = \u0026quot;employee\u0026quot;) ## Warning: The `add` argument of `group_by()` is deprecated as of dplyr 1.0.0. ## Please use the `.add` argument instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. Let's check it out.\nsummary(df) ## Number of events: 5442 ## Number of cases: 500 ## Number of traces: 7 ## Number of distinct activities: 7 ## Average trace length: 10.884 ## ## Start eventlog: 2017-01-02 11:41:53 ## End eventlog: 2018-05-05 07:16:02 ## handling patient employee handling_id ## Blood test : 474 Length:5442 r1:1000 Length:5442 ## Check-out : 984 Class :character r2:1000 Class :character ## Discuss Results : 990 Mode :character r3: 474 Mode :character ## MRI SCAN : 472 r4: 472 ## Registration :1000 r5: 522 ## Triage and Assessment:1000 r6: 990 ## X-Ray : 522 r7: 984 ## registration_type time .order ## complete:2721 Min. :2017-01-02 11:41:53 Min. : 1 ## start :2721 1st Qu.:2017-05-06 17:15:18 1st Qu.:1361 ## Median :2017-09-08 04:16:50 Median :2722 ## Mean :2017-09-02 20:52:34 Mean :2722 ## 3rd Qu.:2017-12-22 15:44:11 3rd Qu.:4082 ## Max. :2018-05-05 07:16:02 Max. :5442 ##  So we learn that there are 500 \u0026quot;cases\u0026quot;, i.e. patients. There are 7 different activities.\nLet's check out the data for a single patient:\ndf %\u0026gt;% filter(patient == 1) %\u0026gt;% arrange(handling_id) #%\u0026gt;%  ## Log of 12 events consisting of: ## 1 trace ## 1 case ## 6 instances of 6 activities ## 6 resources ## Events occurred from 2017-01-02 11:41:53 until 2017-01-09 19:45:45 ## ## Variables were mapped as follows: ## Case identifier: patient ## Activity identifier: handling ## Resource identifier: employee ## Activity instance identifier: handling_id ## Timestamp: time ## Lifecycle transition: registration_type ## ## # A tibble: 12 x 7 ## handling patient employee handling_id registration_ty… time ## \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dttm\u0026gt; ## 1 Registr… 1 r1 1 start 2017-01-02 11:41:53 ## 2 Registr… 1 r1 1 complete 2017-01-02 12:40:20 ## 3 Blood t… 1 r3 1001 start 2017-01-05 08:59:04 ## 4 Blood t… 1 r3 1001 complete 2017-01-05 14:34:27 ## 5 MRI SCAN 1 r4 1238 start 2017-01-05 21:37:12 ## 6 MRI SCAN 1 r4 1238 complete 2017-01-06 01:54:23 ## 7 Discuss… 1 r6 1735 start 2017-01-07 07:57:49 ## 8 Discuss… 1 r6 1735 complete 2017-01-07 10:18:08 ## 9 Check-o… 1 r7 2230 start 2017-01-09 17:09:43 ## 10 Check-o… 1 r7 2230 complete 2017-01-09 19:45:45 ## 11 Triage … 1 r2 501 start 2017-01-02 12:40:20 ## 12 Triage … 1 r2 501 complete 2017-01-02 22:32:25 ## # … with 1 more variable: .order \u0026lt;int\u0026gt;  # select(handling, handling_id, registration_type) # does not work We learn that each \u0026quot;handling\u0026quot; has a separate start and complete timestamp.\n Traces The summary info of the event log also counts so-called \u0026quot;traces\u0026quot;. A trace is defined a unique sequence of events in the event log. Apparently, there are only seven different traces (possible sequences). Let's visualize them.\nTo visualize all traces, we set coverage to 1.0.\ndf %\u0026gt;% processmapR::trace_explorer(type = \u0026quot;frequent\u0026quot;, coverage = 1.0) ## Warning: `rename_()` is deprecated as of dplyr 0.7.0. ## Please use `rename()` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. So there are a few traces (0.6%) that do not end with a check-out. Ignoring these rare cases, we find that there are two types of cases:\n Cases that get an X-ray Cases that get a blood test followed by an MRI scan   The dotted chart A really powerful visualization in process mining comes in the form of a \u0026quot;dotted chart\u0026quot;. The dotted chart function produces a ggplot graph, which is nice, because so we can actually tweak the graph as we can with regular ggplot objects.\nIt has two nice use cases. The first is when we plot actual time on the x-axis, and sort the cases by starting date.\ndf %\u0026gt;% dotted_chart(x = \u0026quot;absolute\u0026quot;, sort = \u0026quot;start\u0026quot;) + ggtitle(\u0026quot;All cases\u0026quot;) + theme_gray() ## Joining, by = \u0026quot;patient\u0026quot; The slope of this graphs learns us the rate of new cases, and if this changes over time. Here it appears constant, with 500 cases divided over five quarter years.\nThe second is to align all cases relative to the first event, and sort on duration of the whole sequence of events.\ndf %\u0026gt;% dotted_chart(x = \u0026quot;relative\u0026quot;, sort = \u0026quot;duration\u0026quot;) + ggtitle(\u0026quot;All cases\u0026quot;) + theme_gray() ## Joining, by = \u0026quot;patient\u0026quot; A nice pattern emerges, where all cases start with registration, then quickly proceed to triage and assessment, after that, a time varying period of 1-10 days follows where either the blood test + MRI scan, or the X-ray is performed, followed by discussing the results. Finally, check out occurs.\n Conclusion To conclude, the process mining approach to analyze time series event data appears highly promising. The dotted chart is a great addition to my data visualization repertoire, and the process mining folks appear to have at lot more goodies, such as Trace Alignment.\n ","date":1560988800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560988800,"objectID":"f36148e4bf390595d95a45aa28e76282","permalink":"https://gsverhoeven.github.io/post/exploring-process-mining/","publishdate":"2019-06-20T00:00:00Z","relpermalink":"/post/exploring-process-mining/","section":"post","summary":"In this post, we'll explore the BupaR suite of Process Mining packages created by Gert Janssenswillen of Hasselt University.","tags":["process mining","R"],"title":"Exploring Process Mining in R","type":"post"},{"authors":null,"categories":["Causal inference"],"content":" Introduction (Short intro) This is me learning causal inference (CI) by self-study together with colleagues using online resources.\n(Longer intro) A modern data scientist needs to become skilled in at least three topics (I left out visualization):\n (Bayesian) Statistical modeling Machine Learning Causal inference  For the first two topics, great introductory books exist that\n focus on learning-by-doing and are low on math and high on simulation / programming in R are fun / well written  For Bayesian statistical modeling, we have the awesome textbook \u0026quot;Statistical Rethinking\u0026quot; by Richard mcElreath.\nFor Machine Learning, we have the (free) book \u0026quot;Introduction to Statistical Learning\u0026quot; by James, Witten, Hastie \u0026amp; Tibshirani.\nHowever, for Causal Inference, such a book does not exist yet AFAIK. Therefore, I tried to piece together a Causal Inference course based on the criteria mentioned above.\n Designing an introductory causal inference course Explicit goal was to contrast/combine the causal graph (DAG) approach with what some call \u0026quot;Quasi-experimental designs\u0026quot;, i.e. the econometric causal effects toolkit (Regression Discontinuity Design, matching, instrumental variables etc).\nIn the end, I decided to combine the two causal chapters from Gelman \u0026amp; Hill (2007) (freely available on Gelman's website) with the introductory chapter on Causal Graphical Models by Felix Elwert (freely available on Elwert's website).\nThe Gelman \u0026amp; Hill chapters already come with a set of exercises. However, for DAGs, i could not find a suitable set of exercises.\nSo I created two R markdown notebooks with exercises in R, that make use of the DAGitty tool, created by Johannes Textor and freely available as R package.\nSome exercises are taken from Causal inference in statistics: A Primer by Pearl, Glymour \u0026amp; Jewell. (I probably should own this book. So I just ordered it :))\nAll materials are available in a GitHub repository\n Outline of the course The course has four parts.\nGeneral introduction The course starts with the first causal chapter of Gelman \u0026amp; Hill's book, \u0026quot;Causal inference using regression on the treatment variable\u0026quot;. This creates a first complete experience with identifying and estimating causal effects. However, there are no causal diagrams, which is unfortunate.\n Identification of Causal effects using DAGs Next we dive into causal identification using the causal diagram approach. For this we use the chapter \u0026quot;Causal Graphical Models\u0026quot; by Felix Elwert. Two R markdown Notebooks with exercises using Dagitty complete this part.\n Identification and estimation strategies We then continue with the second causal chapter of Gelman \u0026amp; Hill \u0026quot;Causal inference using more advanced models\u0026quot;. This covers matching, regression discontinuity design, and instrumental variables. This material is combined with a paper by Scheiner et al, that contains DAGs for these methods. In our study group DAGs greatly facilitated discussion of the various designs.\n Varying treatment effects using Machine Learning Finally, and this part of the course has yet to take place, is the topic of estimating heterogeneous (i.e. subgroup, or even individual) treatment effects. This covers recent developements based on (forests of) regression trees. The plan is to cover both bayesian (BART, Chipman \u0026amp; mcCullough) and non-bayesian (GRF, Athey \u0026amp; Wager) methods.\n  Looking back so far The causal diagram / DAG approach is nonparametric and its purpose is to\n Make assumptions on the data generating process explicit Formalize identification of causal effects  Thus, it is separate from, and complements statistical estimation. The distinction between identification and estimation is not so explicitly made in the Gelman \u0026amp; Hill chapters, at least this is my impression. It would really benefit from adding DAGs, as Richard mcElreath is doing in his upcoming second edition of Statistical Rethinking.\nAfter having worked through these materials, I think reading Shalizi's chapters on Causal Effects would be a smart move. This is part III of his book \u0026quot;Advanced Data Analysis from an Elementary Point of View\u0026quot;, which is awesome in its clarity, practical remarks a.k.a. normative statements by the author, and breadth.\nIf you have a question, would like to comment or share ideas feel free to contact me.\n ","date":1557532800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557532800,"objectID":"71aeaecf1bbcd7d5f52d2141de1743f4","permalink":"https://gsverhoeven.github.io/post/causal-inference-course/","publishdate":"2019-05-11T00:00:00Z","relpermalink":"/post/causal-inference-course/","section":"post","summary":"In this blog post, I describe the introductory course on Causal Inference I pieced together using various materials available online. It combines Pearl's Causal Graph approach with statistics Gelman/mcElreath style.","tags":["R","causal inference","DAG","causal graph"],"title":"Designing an introductory course on Causal Inference","type":"post"},{"authors":null,"categories":["Machine learning"],"content":" In this blog post, we explore how to implement the validation set approach in caret. This is the most basic form of the train/test machine learning concept. For example, the classic machine learning textbook \u0026quot;An introduction to Statistical Learning\u0026quot; uses the validation set approach to introduce resampling methods.\nIn practice, one likes to use k-fold Cross validation, or Leave-one-out cross validation, as they make better use of the data. This is probably the reason that the validation set approach is not one of caret's preset methods.\nBut for teaching purposes it would be very nice to have a caret implementation.\nThis would allow for an easy demonstration of the variability one gets when choosing different partionings. It also allows direct demonstration of why k-fold CV is superior to the validation set approach with respect to bias/variance.\nWe pick the BostonHousing dataset for our example code.\n# Boston Housing knitr::kable(head(Boston))   crim zn indus chas nox rm age dis rad tax ptratio black lstat medv    0.00632 18 2.31 0 0.538 6.575 65.2 4.0900 1 296 15.3 396.90 4.98 24.0  0.02731 0 7.07 0 0.469 6.421 78.9 4.9671 2 242 17.8 396.90 9.14 21.6  0.02729 0 7.07 0 0.469 7.185 61.1 4.9671 2 242 17.8 392.83 4.03 34.7  0.03237 0 2.18 0 0.458 6.998 45.8 6.0622 3 222 18.7 394.63 2.94 33.4  0.06905 0 2.18 0 0.458 7.147 54.2 6.0622 3 222 18.7 396.90 5.33 36.2  0.02985 0 2.18 0 0.458 6.430 58.7 6.0622 3 222 18.7 394.12 5.21 28.7    Our model is predicting medv (Median house value) using predictors indus and chas in a multiple linear regression. We split the data in half, 50% for fitting the model, and 50% to use as a validation set.\nStratified sampling vs random sampling To check if we understand what caret does, we first implement the validation set approach ourselves. To be able to compare, we need exactly the same data partitions for our manual approach and the caret approach. As caret requires a particular format (a named list of sets of train indices) we conform to this standard. However, all caret partitioning functions seem to perform stratified random sampling. This means that it first partitions the data in equal sized groups based on the outcome variable, and then samples at random within those groups to partitions that have similar distributions for the outcome variable.\nThis not desirable for teaching, as it adds more complexity. In addition, it would be nice to be able to compare stratified vs. random sampling.\nWe therefore write a function that generates truly random partitions of the data. We let it generate partitions in the format that trainControl likes.\n# internal function from caret package, needed to play nice with resamples() prettySeq \u0026lt;- function(x) paste(\u0026quot;Resample\u0026quot;, gsub(\u0026quot; \u0026quot;, \u0026quot;0\u0026quot;, format(seq(along = x))), sep = \u0026quot;\u0026quot;) createRandomDataPartition \u0026lt;- function(y, times, p) { vec \u0026lt;- 1:length(y) n_samples \u0026lt;- round(p * length(y)) result \u0026lt;- list() for(t in 1:times){ indices \u0026lt;- sample(vec, n_samples, replace = FALSE) result[[t]] \u0026lt;- indices #names(result)[t] \u0026lt;- paste0(\u0026quot;Resample\u0026quot;, t) } names(result) \u0026lt;- prettySeq(result) result } createRandomDataPartition(1:10, times = 2, p = 0.5) ## $Resample1 ## [1] 1 10 4 6 7 ## ## $Resample2 ## [1] 5 3 2 1 7  The validation set approach without caret Here is the validation set approach without using caret. We create a single random partition of the data in train and validation set, fit the model on the training data, predict on the validation data, and calculate the RMSE error on the test predictions.\nset.seed(1234) parts \u0026lt;- createRandomDataPartition(Boston$medv, times = 1, p = 0.5) train \u0026lt;- parts$Resample1 # fit ols on train data lm.fit \u0026lt;- lm(medv ~ indus + chas , data = Boston[train,]) # predict on held out data preds \u0026lt;- predict(lm.fit, newdata = Boston[-train,]) # calculate RMSE validation error sqrt(mean((preds - Boston[-train,]$medv)^2)) ## [1] 7.930076 If we feed caret the same data partition, we expect exactly the same test error for the held-out data. Let's find out!\n The validation set approach in caret Now we use the caret package. Regular usage requires two function calls, one to trainControl to control the resampling behavior, and one to train to do the actual model fitting and prediction generation.\nAs the validation set approach is not one of the predefined methods, we need to make use of the index argument to explicitely define the train partitions outside of caret. It automatically predicts on the records that are not contained in the train partitions.\nThe index argument plays well with the createDataPartition (Stratfied sampling) and createRandomDataPartition (our own custom function that performs truly random sampling) functions, as these functions both generate partitions in precisely the format that index wants: lists of training set indices.\nIn the code below, we generate four different 50/50 partitions of the data.\nWe set savePredictions to TRUE to be able to verify the calculated metrics such as the test RMSE.\nset.seed(1234) # create four partitions parts \u0026lt;- createRandomDataPartition(Boston$medv, times = 4, p = 0.5) ctrl \u0026lt;- trainControl(method = \u0026quot;repeatedcv\u0026quot;, ## The method doesn\u0026#39;t matter ## since we are defining the resamples index= parts, ##verboseIter = TRUE, ##repeats = 1, savePredictions = TRUE ##returnResamp = \u0026quot;final\u0026quot; )  Now we can run caret and fit the model four times:\nres \u0026lt;- train(medv ~ indus + chas, data = Boston, method = \u0026quot;lm\u0026quot;, trControl = ctrl) res ## Linear Regression ## ## 506 samples ## 2 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 1 times) ## Summary of sample sizes: 253, 253, 253, 253 ## Resampling results: ## ## RMSE Rsquared MAE ## 7.906538 0.2551047 5.764773 ## ## Tuning parameter \u0026#39;intercept\u0026#39; was held constant at a value of TRUE From the result returned by train we can verify that it has fitted a model on four different datasets, each of size 253. By default it reports the average test error over the four validation sets. We can also extract the four individual test errors:\n# strangely enough, resamples() always wants at least two train() results # see also the man page for resamples() resamples \u0026lt;- resamples(list(MOD1 = res, MOD2 = res)) resamples$values$`MOD1~RMSE` ## [1] 7.930076 8.135428 7.899054 7.661595 # check that we recover the RMSE reported by train() in the Resampling results mean(resamples$values$`MOD1~RMSE`) ## [1] 7.906538 summary(resamples) ## ## Call: ## summary.resamples(object = resamples) ## ## Models: MOD1, MOD2 ## Number of resamples: 4 ## ## MAE ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA\u0026#39;s ## MOD1 5.516407 5.730172 5.809746 5.764773 5.844347 5.923193 0 ## MOD2 5.516407 5.730172 5.809746 5.764773 5.844347 5.923193 0 ## ## RMSE ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA\u0026#39;s ## MOD1 7.661595 7.839689 7.914565 7.906538 7.981414 8.135428 0 ## MOD2 7.661595 7.839689 7.914565 7.906538 7.981414 8.135428 0 ## ## Rsquared ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA\u0026#39;s ## MOD1 0.2339796 0.2377464 0.2541613 0.2551047 0.2715197 0.2781167 0 ## MOD2 0.2339796 0.2377464 0.2541613 0.2551047 0.2715197 0.2781167 0 Note that the RMSE value for the first train/test partition is exactly equal to our own implementation of the validation set approach. Awesome.\n Validation set approach: stratified sampling versus random sampling Since we now know what we are doing, let's perform a simulation study to compare stratified random sampling with truly random sampling, using the validation set approach, and repeating this proces say a few thousand times to get a nice distribution of test errors.\n# simulation settings n_repeats \u0026lt;- 3000 train_fraction \u0026lt;- 0.8 First we fit the models on the random sampling data partitions:\nset.seed(1234) parts \u0026lt;- createRandomDataPartition(Boston$medv, times = n_repeats, p = train_fraction) ctrl \u0026lt;- trainControl(method = \u0026quot;repeatedcv\u0026quot;, ## The method doesn\u0026#39;t matter index= parts, savePredictions = TRUE ) rand_sampl_res \u0026lt;- train(medv ~ indus + chas, data = Boston, method = \u0026quot;lm\u0026quot;, trControl = ctrl) rand_sampl_res ## Linear Regression ## ## 506 samples ## 2 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 1 times) ## Summary of sample sizes: 405, 405, 405, 405, 405, 405, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 7.868972 0.2753001 5.790874 ## ## Tuning parameter \u0026#39;intercept\u0026#39; was held constant at a value of TRUE Next, we fit the models on the stratified sampling data partitions:\nset.seed(1234) parts \u0026lt;- createDataPartition(Boston$medv, times = n_repeats, p = train_fraction, list = T) ctrl \u0026lt;- trainControl(method = \u0026quot;repeatedcv\u0026quot;, ## The method doesn\u0026#39;t matter index= parts, savePredictions = TRUE ) strat_sampl_res \u0026lt;- train(medv ~ indus + chas, data = Boston, method = \u0026quot;lm\u0026quot;, trControl = ctrl) strat_sampl_res ## Linear Regression ## ## 506 samples ## 2 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 1 times) ## Summary of sample sizes: 407, 407, 407, 407, 407, 407, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 7.83269 0.277719 5.769507 ## ## Tuning parameter \u0026#39;intercept\u0026#39; was held constant at a value of TRUE Then, we merge the two results to compare the distributions:\nresamples \u0026lt;- resamples(list(RAND = rand_sampl_res, STRAT = strat_sampl_res))  Analyzing caret resampling results We now analyse our resampling results. We can use the summary method on our resamples object:\nsummary(resamples) ## ## Call: ## summary.resamples(object = resamples) ## ## Models: RAND, STRAT ## Number of resamples: 3000 ## ## MAE ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA\u0026#39;s ## RAND 4.406326 5.475846 5.775077 5.790874 6.094820 7.582886 0 ## STRAT 4.401729 5.477664 5.758201 5.769507 6.058652 7.356133 0 ## ## RMSE ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA\u0026#39;s ## RAND 5.328128 7.323887 7.847369 7.868972 8.408855 10.78024 0 ## STRAT 5.560942 7.304199 7.828765 7.832690 8.328966 10.44186 0 ## ## Rsquared ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA\u0026#39;s ## RAND 0.06982417 0.2259553 0.2733762 0.2753001 0.3249820 0.5195017 0 ## STRAT 0.05306875 0.2263577 0.2752015 0.2777190 0.3277577 0.4977015 0 We can also use the plot function provided by the caret package. It plots the mean of our performance metric (RMSE), as well as estimation uncertainty of this mean. Note that the confidence intervals here are based on a normal approximation (One sample t-test).\n# caret:::ggplot.resamples # t.test(resamples$values$`RAND~RMSE`) ggplot(resamples, metric = \u0026quot;RMSE\u0026quot;)  My personal preference is to more directly display both distributions. This is done by bwplot() (caret does not have ggplot version of this function).\nbwplot(resamples, metric = \u0026quot;RMSE\u0026quot;) It does seems that stratified sampling paints a slightly more optimistic picture of the test error when compared to truly random sampling. However, we can also see that random sampling has somewhat higher variance when compared to stratified sampling.\nBased on these results, it seems like stratified sampling is indeed a reasonable default setting for caret.\n Update: LGOCV set.seed(1234) ctrl \u0026lt;- trainControl(method = \u0026quot;LGOCV\u0026quot;, ## The method doesn\u0026#39;t matter repeats = n_repeats, number = 1, p = 0.5, savePredictions = TRUE )  ## Warning: `repeats` has no meaning for this resampling method. lgocv_res \u0026lt;- train(medv ~ indus + chas, data = Boston, method = \u0026quot;lm\u0026quot;, trControl = ctrl) lgocv_res ## Linear Regression ## ## 506 samples ## 2 predictor ## ## No pre-processing ## Resampling: Repeated Train/Test Splits Estimated (1 reps, 50%) ## Summary of sample sizes: 254 ## Resampling results: ## ## RMSE Rsquared MAE ## 8.137926 0.2389733 5.763309 ## ## Tuning parameter \u0026#39;intercept\u0026#39; was held constant at a value of TRUE  ","date":1553126400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553126400,"objectID":"faf4add31be0ffd08e307b2b0c4b5704","permalink":"https://gsverhoeven.github.io/post/validation-set-approach-in-caret/","publishdate":"2019-03-21T00:00:00Z","relpermalink":"/post/validation-set-approach-in-caret/","section":"post","summary":"In this blog post, we explore how to implement the validation set approach in caret. This is the most basic form of the train/test machine learning concept.","tags":["R","caret"],"title":"The validation set approach in caret","type":"post"},{"authors":null,"categories":["Electronics"],"content":" In this post, I show how to create a Arduino-based atmospheric sensor circuit capable of storing large amounts of data on a microSD card.\nNowadays, one can buy a commercial Thermo/Hygro datalogger for 50 Euro online (i.e. https://www.vitalitools.nl/lascar-electronics-el-usb-2-datalogger). However, I decided that it would be a nice project to learn more about Arduino, in particular how to interface it with a microSD card. So i made one myself. Working with SD cards has the advantage of having a huge storage capacity. To give you an impression: Below we analyse 10K measurements stored in a 60 Kb file, the SD card can hold 4 Gb!\nComponents After some research I ordered:\n A microSD card reader/writer with SPI interface (Catalex card) A Bosch BME-280 temperature/pressure/humidity sensor with I2C interface  As the BME-280 sensor operates at 3.3V and my Arduino Nano at 5V, I also ordered a four channel Logic Level Converter to convert the 5V I2C on the Arduino side of the LLC to 3.3V on the BME-280 side.\nTo make the circuit Mains powered, i took an old Samsung mobile phone Charger (5V 0.7A), cutoff the plug and attached it to the breadboard.\n Circuit \u0026amp; Programming The breadboard layout (created using Fritzing) is shown below:\n At first i was using the Arduino 5V pin (with Arduino connected to USB at the front of my Desktop PC, these USB ports might have lower current) to power both the SD card and the Level converter. Separately they would work fine, but together in one circuit the SD card gave erratic results. I guessed that current consumption was too high, and during testing I used the 5V charger as power supply for the SD card. During actual usage I used the 5V charger to power both the SD card AND the Arduino Nano, which worked nicely.\nCoding was simple, i just combined the example code and libraries for a SPI SD card and for a BME-280 I2C sensor. I put the code on GitHub anyway as a reference.\n Data collection and preparation I ended up testing the device by letting it collect measurements in four different places within the house. In the following order:\n The living room The basement First floor bedroom First floor bathroom  After collecting the data I put the microSD card in a microSD card reader and copied the DATALOG.TXT CSV file to my pc for analysis in R.\ndf \u0026lt;- read.csv2(\u0026quot;DATALOG.TXT\u0026quot;, header = F) colnames(df) \u0026lt;- c(\u0026quot;Time\u0026quot;, \u0026quot;Temp\u0026quot;, \u0026quot;Hum\u0026quot;, \u0026quot;Pressure\u0026quot;) # give the four traces a unique ID df$start_trace \u0026lt;- ifelse(df$Time == 0, 1, 0) df$trace_id \u0026lt;- cumsum(df$start_trace) mdf \u0026lt;- melt(df, id.vars = c(\u0026quot;Time\u0026quot;, \u0026quot;trace_id\u0026quot;)) ## Warning in melt(df, id.vars = c(\u0026quot;Time\u0026quot;, \u0026quot;trace_id\u0026quot;)): The melt generic in ## data.table has been passed a data.frame and will attempt to redirect to the ## relevant reshape2 method; please note that reshape2 is deprecated, and this ## redirection is now deprecated as well. To continue using melt methods from ## reshape2 while both libraries are attached, e.g. melt.list, you can prepend the ## namespace like reshape2::melt(df). In the next version, this warning will become ## an error. ## Warning: attributes are not identical across measure variables; they will be ## dropped # label the four traces trace_id \u0026lt;- 1:4 trace_name \u0026lt;- c(\u0026quot;Living room\u0026quot;, \u0026quot;Basement\u0026quot;, \u0026quot;Bedroom 1st floor\u0026quot;, \u0026quot;Bathroom 1st floor\u0026quot;) cod \u0026lt;- data.table(trace_id, trace_name = factor(trace_name, levels = trace_name)) mdf \u0026lt;- data.table(merge(mdf, cod, by = \u0026quot;trace_id\u0026quot;)) mdf \u0026lt;- mdf[, value := as.numeric(value)]  Analysis Pressure We start with the pressure measurements. This is supposed to be a proxy for altitude.\nggplot(mdf[mdf$variable == \u0026quot;Pressure\u0026quot; \u0026amp; Time \u0026gt; 1], aes(x = Time, y = value, color = variable, group = variable)) + geom_point(col = \u0026quot;grey\u0026quot;) + facet_grid(~ trace_name) + geom_smooth(size = 1) ## `geom_smooth()` using method = \u0026#39;loess\u0026#39; and formula \u0026#39;y ~ x\u0026#39; The basement, which is the lowest, has the highest pressure. But the difference between living room (ground floor) and the two rooms at the first floor is less pronounced. What is not so clear is what drives the changes in pressure WHILE the sensor is at a particular location, i.e. in the basement, or on the 1st floor. But no time to dwell on that, let's move on to the temperature!\n Temperature ggplot(mdf[mdf$variable == \u0026quot;Temp\u0026quot; \u0026amp; Time \u0026gt; 1], aes(x = Time, y = value, color = variable, group = variable)) + geom_point() + facet_grid(~ trace_name) Here, it appears that the sequence of the rooms can explain the slowly changing patterns of temperature. We started out in the Living room at 21C (The thermostat was set at 20C at that time). Then towards the cold basement. It appears that temperature needed some time to equilibrate, possibly because the breadboard was placed on an elevated plastic box, insulating it from below. In the bedroom it was placed on the (cold) floor, and it was already cold from the basement. Then in the bathroom, the final location, it went up, probably due to the floor being heated to keep the bathroom at 18C.\n Relative Humidity Finally, the relative humidity. This appears super strongly correlated with the temperature.\nggplot(mdf[mdf$variable == \u0026quot;Hum\u0026quot; \u0026amp; Time \u0026gt; 1], aes(x = Time, y = value, color = variable, group = variable)) + geom_point() + facet_grid(~ trace_name) Here we see that the living room is at a agreeable 45% RH. The basement has a higher RH percentage, expected because it's colder.\nAccording to Wikipedia:\nHumans can be comfortable within a wide range of humidities depending on the temperature—from 30% to 70%[14]—but ideally between 50%[15] and 60%.[16] Very low humidity can create discomfort, respiratory problems, and aggravate allergies in some individuals.\nThe bedroom is also at a nice humidity level of 55% RH. The bathroom floor was being heated, and this unsurprisingly reduces the local RH to below 40%.\n  Conclusion It all seems to work pretty well. Measurement quality appears reasonable, with temperature and humidity consistent and with little noise, whereas the pressure reading needs some averaging / smoothing to get a stable signal.\nI had great fun making this device!\n ","date":1551744000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551744000,"objectID":"8fec54b618880788adf021e0480e50fc","permalink":"https://gsverhoeven.github.io/post/arduino-atmospheric-datalogger/","publishdate":"2019-03-05T00:00:00Z","relpermalink":"/post/arduino-atmospheric-datalogger/","section":"post","summary":"In this post, I show how to create a Arduino-based atmospheric sensor prototype capable of storing large amounts of data on a microSD card.","tags":["Arduino"],"title":"Arduino Weather Station with datalogging","type":"post"},{"authors":null,"categories":["causal inference","machine learning"],"content":" Load packages # library(devtools) #devtools::install_github(\u0026quot;vdorie/dbarts\u0026quot;) library(dbarts) library(ggplot2) library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ tibble 3.0.3 ✔ dplyr 1.0.2 ## ✔ tidyr 1.1.2 ✔ stringr 1.4.0 ## ✔ readr 1.3.1 ✔ forcats 0.5.0 ## ✔ purrr 0.3.4 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ tidyr::extract() masks dbarts::extract() ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(grf) #devtools::install_github(\u0026quot;vdorie/aciccomp/2017\u0026quot;) library(aciccomp2017) library(cowplot) ## ## ******************************************************** ## Note: As of version 1.0.0, cowplot does not change the ## default ggplot2 theme anymore. To recover the previous ## behavior, execute: ## theme_set(theme_cowplot()) ## ******************************************************** source(\u0026quot;CalcPosteriors.R\u0026quot;) fullrun \u0026lt;- 0  Dataset 1: Simulated dataset from Friedman MARS paper This is not a causal problem but a prediction problem.\n## y = f(x) + epsilon , epsilon ~ N(0, sigma) ## x consists of 10 variables, only first 5 matter f \u0026lt;- function(x) { 10 * sin(pi * x[,1] * x[,2]) + 20 * (x[,3] - 0.5)^2 + 10 * x[,4] + 5 * x[,5] } set.seed(99) sigma \u0026lt;- 1.0 n \u0026lt;- 100 x \u0026lt;- matrix(runif(n * 10), n, 10) Ey \u0026lt;- f(x) y \u0026lt;- rnorm(n, Ey, sigma) df \u0026lt;- data.frame(x, y, y_true = Ey) fit BART model on simulated Friedman data if(fullrun){ ## run BART set.seed(99) bartFit \u0026lt;- bart(x, y) saveRDS(bartFit, \u0026quot;s1.rds\u0026quot;) } else { bartFit \u0026lt;- readRDS(\u0026quot;s1.rds\u0026quot;)} plot(bartFit) MCMC or sigma looks ok.\ncompare BART fit to true values df2 \u0026lt;- data.frame(df, ql = apply(bartFit$yhat.train, length(dim(bartFit$yhat.train)), quantile,probs=0.05), qm = apply(bartFit$yhat.train, length(dim(bartFit$yhat.train)), quantile,probs=.5), qu \u0026lt;- apply(bartFit$yhat.train, length(dim(bartFit$yhat.train)), quantile,probs=0.95) ) bartp \u0026lt;- ggplot(df2, aes(x= y, y = qm)) + geom_linerange(aes(ymin = ql, ymax = qu), col = \u0026quot;grey\u0026quot;) + geom_point() + geom_smooth() + geom_abline(intercept = 0, slope = 1, col = \u0026quot;red\u0026quot;, size = 1) bartp ## `geom_smooth()` using method = \u0026#39;loess\u0026#39; and formula \u0026#39;y ~ x\u0026#39; This looks nice.\n  Fit Grf regression forest on Friedman data From the manual: Trains a regression forest that can be used to estimate the conditional mean function mu(x) = E[Y | X = x]\nif(fullrun){ reg.forest = regression_forest(x, y, num.trees = 2000) saveRDS(reg.forest, \u0026quot;s00.rds\u0026quot;) } else {reg.forest \u0026lt;- readRDS(\u0026quot;s00.rds\u0026quot;)} df3 \u0026lt;- CalcPredictionsGRF(x, reg.forest) df3 \u0026lt;- data.frame(df3, y) ggplot(df3, aes(x= y, y = qm)) + geom_linerange(aes(ymin = ql, ymax = qu), col = \u0026quot;grey\u0026quot;) + geom_point() + geom_abline(intercept = 0, slope = 1, col = \u0026quot;red\u0026quot;, size = 1) This is pretty bad compared to BART. What's wrong here?\nFrom reference.md: GRF isn't working well on a small dataset\nIf you observe poor performance on a dataset with a small number of examples, it may be worth trying out two changes:\n Disabling honesty. As noted in the section on honesty above, when honesty is enabled, the training subsample is further split in half before performing splitting. This may not leave enough information for the algorithm to determine high-quality splits. Skipping the variance estimate computation, by setting ci.group.size to 1 during training, then increasing sample.fraction. Because of how variance estimation is implemented, sample.fraction cannot be greater than 0.5 when it is enabled. If variance estimates are not needed, it may help to disable this computation and use a larger subsample size for training.  Dataset is pretty small (n=100). Maybe turn of honesty? We cannot turn off variance estimate computation, because we want the CI's\nif(fullrun){ reg.forest2 = regression_forest(x, y, num.trees = 2000, honesty = FALSE) saveRDS(reg.forest2, \u0026quot;s001.rds\u0026quot;) } else {reg.forest2 \u0026lt;- readRDS(\u0026quot;s001.rds\u0026quot;)} df2 \u0026lt;- CalcPredictionsGRF(x, reg.forest2) df2 \u0026lt;- data.frame(df2, y) grfp \u0026lt;- ggplot(df2, aes(x= y, y = qm)) + geom_linerange(aes(ymin = ql, ymax = qu), col = \u0026quot;grey\u0026quot;) + geom_point() + geom_smooth() + geom_abline(intercept = 0, slope = 1, col = \u0026quot;red\u0026quot;, size = 1) grfp ## `geom_smooth()` using method = \u0026#39;loess\u0026#39; and formula \u0026#39;y ~ x\u0026#39; Ah! better now. But Grf still worse than BART. We ran with 2000 trees and turned of honesty. Perhaps dataset too small? Maybe check out the sample.fraction parameter? Sample.fraction is set by default at 0.5, so only half of data is used to grow tree. OR use tune.parameters = TRUE\n Compare methods gp \u0026lt;- plot_grid(bartp, grfp) ## `geom_smooth()` using method = \u0026#39;loess\u0026#39; and formula \u0026#39;y ~ x\u0026#39; ## `geom_smooth()` using method = \u0026#39;loess\u0026#39; and formula \u0026#39;y ~ x\u0026#39; gp    Dataset 2: Simulated data from ACIC 2017 This is a bigger dataset, N=4302.\n Treatment effect \\(\\tau\\) is a function of covariates x3, x24, x14, x15 Probability of treatment \\(\\pi\\) is a function of covariates x1, x43, x10. Outcome is a function of x43 Noise is a function of x21  head(input_2017[, c(3,24,14,15)]) ## x_3 x_24 x_14 x_15 ## 1 20 white 0 2 ## 2 0 black 0 0 ## 3 0 white 0 1 ## 4 10 white 0 0 ## 5 0 black 0 0 ## 6 1 white 0 0 Check transformed covariates used to create simulated datasets.\n# zit hidden in package head(aciccomp2017:::transformedData_2017) ## x_1 x_3 x_10 x_14 x_15 x_21 x_24 x_43 ## 2665 -1.18689448 gt_0 leq_0 leq_0 gt_0 J E -1.0897971 ## 22 -0.04543705 leq_0 leq_0 leq_0 leq_0 J B 1.1223750 ## 2416 0.13675482 leq_0 leq_0 leq_0 gt_0 J E 0.6136700 ## 1350 -0.24062700 gt_0 leq_0 leq_0 leq_0 J E -0.2995632 ## 3850 1.02054653 leq_0 leq_0 leq_0 leq_0 I B 0.6136700 ## 4167 -1.18689448 gt_0 leq_0 leq_0 leq_0 K E -1.5961206 So we find that we should not take the functions in Dorie 2018 (debrief.pdf) literately. x_3 used to calculate the treatment effect is derived from x_3 in the input data. x_24 used to calculate the treatment effect is derived from x_24 in the input data. Both have become binary variables.\nTurns out that this was a feature of the 2016 ACIC and IS mentioned in the debrief.pdf\nWe pick the iid, strong signal, low noise, low confounding first. Actually from estimated PS (W.hat) it seems that every obs has probability of treatment 50%.\nparameters_2017[21,] ## errors magnitude noise confounding ## 21 iid 1 0 0 # easiest? Grab the first replicate.\nsim \u0026lt;- dgp_2017(21, 1) Fit BART to ACIC 2017 dataset Need also counterfactual predictions. Most efficient seems to create x.test with Z reversed. This will give use a y.test as well as y.train in the output. We expect draws for both. Plotting a histogram of the difference gives us the treatment effect with uncertainty.\nFrom the MCMC draws for sigma we infer that we need to drop more \u0026quot;burn in\u0026quot; samples.\nPrepare data for BART, including x.test with treatment reversed:\n# combine x and y y \u0026lt;- sim$y x \u0026lt;- model.matrix(~. ,cbind(z = sim$z, input_2017)) # flip z for counterfactual predictions (needed for BART) x.test \u0026lt;- model.matrix(~. ,cbind(z = 1 - sim$z, input_2017)) ## run BART #fullrun \u0026lt;- 0 if(fullrun){ set.seed(99) bartFit \u0026lt;- bart(x, y, x.test, nskip = 350, ntree = 1000) saveRDS(bartFit, \u0026quot;s2.rds\u0026quot;) } else { bartFit \u0026lt;- readRDS(\u0026quot;s2.rds\u0026quot;)} plot(bartFit) Extract individual treatment effect (ITE / CATE) plus uncertainty from bartfit This means switching z from 0 to 1 and looking at difference in y + uncertainty in y.\n#source(\u0026quot;calcPosteriors.R\u0026quot;) sim \u0026lt;- CalcPosteriorsBART(sim, bartFit, \u0026quot;z\u0026quot;) sim \u0026lt;- sim %\u0026gt;% arrange(alpha) bartp \u0026lt;- ggplot(sim, aes(x = 1:nrow(sim), qm)) + geom_linerange(aes(ymin = ql, ymax = qu), col = \u0026quot;grey\u0026quot;) + geom_point() + geom_smooth() + geom_point(aes(y = alpha), col = \u0026quot;red\u0026quot;) + ylim(-2.5, 4.5) bartp ## `geom_smooth()` using method = \u0026#39;gam\u0026#39; and formula \u0026#39;y ~ s(x, bs = \u0026quot;cs\u0026quot;)\u0026#39; ## Warning: Removed 1 rows containing missing values (geom_smooth). This looks sort of ok, but still weird. Some points it gets REALLY wrong.\n Calculate coverage sim \u0026lt;- sim %\u0026gt;% mutate(in_ci = ql \u0026lt; alpha \u0026amp; qu \u0026gt; alpha) mean(sim$in_ci) ## [1] 0.4363087 Pretty bad coverage. Look into whats going on here. Here it should be 0.9\nThe iid plot for method 2 gives coverage 0.7 (where it should be 0.95)\n Calculate RMSE of CATE sqrt(mean((sim$alpha - sim$ite)^2)) ## [1] 0.1587338 For All i.i.d. (averaged over 250 replicates averaged over 8 scenarios) method 2 (BART should have RMSE of CATE of 0.35-0.4)\n  Fit grf to ACIC 2017 dataset need large num.trees for CI.\n# prep data for Grf # combine x and y sim \u0026lt;- dgp_2017(21, 1) Y \u0026lt;- sim$y X \u0026lt;- model.matrix(~. ,input_2017) W = sim$z # Train a causal forest. #fullrun \u0026lt;- 0 if(fullrun){ grf.fit_alt \u0026lt;- causal_forest(X, Y, W, num.trees = 500) saveRDS(grf.fit_alt, \u0026quot;s3.rds\u0026quot;) } else{grf.fit_alt \u0026lt;- readRDS(\u0026quot;s3.rds\u0026quot;)} It appears that using 4000 trees consumes too much memory (bad_alloc)\n Compare predictions vs true value df_sep2 \u0026lt;- CalcPredictionsGRF(X, grf.fit_alt) df_sep2 \u0026lt;- data.frame(df_sep2, Y, W, TAU = sim$alpha) df_sep2 \u0026lt;- df_sep2 %\u0026gt;% arrange(TAU) grfp \u0026lt;- ggplot(df_sep2, aes(x = 1:nrow(df_sep2), y = qm)) + geom_linerange(aes(ymin = ql, ymax = qu), col = \u0026quot;grey\u0026quot;) + geom_point() + geom_smooth() + geom_point(aes(y = TAU), col = \u0026quot;red\u0026quot;) + ylim(-2.5, 4.5) grfp ## `geom_smooth()` using method = \u0026#39;gam\u0026#39; and formula \u0026#39;y ~ s(x, bs = \u0026quot;cs\u0026quot;)\u0026#39; This works ok now.\n Compare both methods gp \u0026lt;- plot_grid(bartp, grfp) ## `geom_smooth()` using method = \u0026#39;gam\u0026#39; and formula \u0026#39;y ~ s(x, bs = \u0026quot;cs\u0026quot;)\u0026#39; ## Warning: Removed 1 rows containing missing values (geom_smooth). ## `geom_smooth()` using method = \u0026#39;gam\u0026#39; and formula \u0026#39;y ~ s(x, bs = \u0026quot;cs\u0026quot;)\u0026#39; gp   Dataset 3: simulated data used by grf example THis dataset is used in the Grf manual page. Size N = 2000. Probability of treatment function of X1. Treatment effect function of X1.\n# Generate data. set.seed(123) n = 2000; p = 10 X = matrix(rnorm(n*p), n, p) # treatment W = rbinom(n, 1, 0.4 + 0.2 * (X[,1] \u0026gt; 0)) # outcome (parallel max) Y = pmax(X[,1], 0) * W + X[,2] + pmin(X[,3], 0) + rnorm(n) # TAU is true treatment effect df \u0026lt;- data.frame(X, W, Y, TAU = pmax(X[,1], 0)) Fit GRF Default settings are honesty = TRUE.\n# Train a causal forest. if(fullrun){ tau.forest = causal_forest(X, Y, W, num.trees = 2000) saveRDS(tau.forest, \u0026quot;s4.rds\u0026quot;) } else {tau.forest \u0026lt;- readRDS(\u0026quot;s4.rds\u0026quot;)}  OOB predictions From the GRF manual:\nGiven a test example, the GRF algorithm computes a prediction as follows:\nFor each tree, the test example is \u0026#39;pushed down\u0026#39; to determine what leaf it falls in. Given this information, we create a list of neighboring training examples, weighted by how many times the example fell in the same leaf as the test example. A prediction is made using this weighted list of neighbors, using the relevant approach for the type of forest. In causal prediction, we calculate the treatment effect using the outcomes and treatment status of the neighbor examples. Those familiar with classic random forests might note that this approach differs from the way forest prediction is usually described. The traditional view is that to predict for a test example, each tree makes a prediction on that example. To make a final prediction, the tree predictions are combined in some way, for example through averaging or through 'majority voting'. It's worth noting that for regression forests, the GRF algorithm described above is identical this 'ensemble' approach, where each tree predicts by averaging the outcomes in each leaf, and predictions are combined through a weighted average.\n# Estimate treatment effects for the training data using out-of-bag prediction. tau.hat.oob = predict(tau.forest) res \u0026lt;- data.frame(df, pred = tau.hat.oob$predictions) ggplot(res, aes(x = X1, y = pred)) + geom_point() + geom_smooth() + geom_abline(intercept = 0, slope = 1) + geom_line(aes(y = TAU), col = \u0026quot;red\u0026quot;, size = 1) ## `geom_smooth()` using method = \u0026#39;gam\u0026#39; and formula \u0026#39;y ~ s(x, bs = \u0026quot;cs\u0026quot;)\u0026#39;  ATE \u0026amp; ATT # Estimate the conditional average treatment effect on the full sample (CATE). average_treatment_effect(tau.forest, target.sample = \u0026quot;all\u0026quot;) ## estimate std.err ## 0.37316437 0.04795009 mean(res$TAU) ## [1] 0.4138061 # Estimate the conditional average treatment effect on the treated sample (CATT). # Here, we don\u0026#39;t expect much difference between the CATE and the CATT, since # treatment assignment was randomized. average_treatment_effect(tau.forest, target.sample = \u0026quot;treated\u0026quot;) ## estimate std.err ## 0.47051526 0.04850751 mean(res[res$W == 1,]$TAU) ## [1] 0.5010723  Fit more trees for CI's # Add confidence intervals for heterogeneous treatment effects; growing more # trees is now recommended. if(fullrun){ tau.forest_big = causal_forest(X, Y, W, num.trees = 4000) saveRDS(tau.forest_big, \u0026quot;s5.rds\u0026quot;) } else {tau.forest_big \u0026lt;- readRDS(\u0026quot;s5.rds\u0026quot;)}  Plot CI's ## PM #source(\u0026quot;CalcPosteriors.R\u0026quot;) df_res \u0026lt;- CalcPredictionsGRF(df, tau.forest_big) grfp \u0026lt;- ggplot(df_res, aes(x = X1, y = qm)) + geom_linerange(aes(ymin = ql, ymax = qu), col = \u0026quot;grey\u0026quot;) + geom_point() + geom_smooth() + geom_line(aes(y = TAU), col = \u0026quot;red\u0026quot;, size = 1) + ylim(-1,3.5) grfp ## `geom_smooth()` using method = \u0026#39;gam\u0026#39; and formula \u0026#39;y ~ s(x, bs = \u0026quot;cs\u0026quot;)\u0026#39;  Fit BART on this dataset x.train \u0026lt;- model.matrix(~. ,data.frame(W, X)) x.test \u0026lt;- model.matrix(~. ,data.frame(W = 1 - W, X)) y.train \u0026lt;- Y if(fullrun){ bartFit \u0026lt;- bart(x.train, y.train, x.test, ntree = 2000, ndpost = 1000, nskip = 100) saveRDS(bartFit, \u0026quot;s10.rds\u0026quot;) } else {bartFit \u0026lt;- readRDS(\u0026quot;s10.rds\u0026quot;)} plot(bartFit)  BART: Check fit and CI's #source(\u0026quot;calcPosteriors.R\u0026quot;) sim \u0026lt;- CalcPosteriorsBART(df, bartFit, treatname = \u0026quot;W\u0026quot;) bartp \u0026lt;- ggplot(sim, aes(x = X1, qm)) + geom_linerange(aes(ymin = ql, ymax = qu), col = \u0026quot;grey\u0026quot;) + geom_point() + geom_smooth() + geom_line(aes(y = TAU), col = \u0026quot;red\u0026quot;, size = 1) + ylim(-1,3.5) bartp ## `geom_smooth()` using method = \u0026#39;gam\u0026#39; and formula \u0026#39;y ~ s(x, bs = \u0026quot;cs\u0026quot;)\u0026#39;  Compare gp \u0026lt;- plot_grid(bartp, grfp) ## `geom_smooth()` using method = \u0026#39;gam\u0026#39; and formula \u0026#39;y ~ s(x, bs = \u0026quot;cs\u0026quot;)\u0026#39; ## `geom_smooth()` using method = \u0026#39;gam\u0026#39; and formula \u0026#39;y ~ s(x, bs = \u0026quot;cs\u0026quot;)\u0026#39; gp Here Grf appears more accurate. Mental note: Both W and TAU function of X1.\n  Dataset 4: Fit separate grf forests for Y and W This dataset has a complex propensity of treatment function (Exponential of X1 and X2), as well as hetergeneous treatment effect that is exponential function of X3. It has size N=4000.\n# In some examples, pre-fitting models for Y and W separately may # be helpful (e.g., if different models use different covariates). # In some applications, one may even want to get Y.hat and W.hat # using a completely different method (e.g., boosting). set.seed(123) # Generate new data. n = 4000; p = 20 X = matrix(rnorm(n * p), n, p) TAU = 1 / (1 + exp(-X[, 3])) W = rbinom(n ,1, 1 / (1 + exp(-X[, 1] - X[, 2]))) Y = pmax(X[, 2] + X[, 3], 0) + rowMeans(X[, 4:6]) / 2 + W * TAU + rnorm(n) df_sep4 \u0026lt;- data.frame(X, TAU, W, Y) Grf two-step: First fit model for W (PS) Regression forest to predict W from X. This is a propensity score.\nif(fullrun){ forest.W \u0026lt;- regression_forest(X, W, tune.parameters = c(\u0026quot;min.node.size\u0026quot;, \u0026quot;honesty.prune.leaves\u0026quot;), num.trees = 2000) saveRDS(forest.W, \u0026quot;s6.rds\u0026quot;) } else {forest.W \u0026lt;- readRDS(\u0026quot;s6.rds\u0026quot;)} W.hat = predict(forest.W)$predictions Grf:Then Fit model for Y, selecting covariates This predict Y from X, ignoring treatment.\nif(fullrun){ forest.Y = regression_forest(X, Y, tune.parameters = c(\u0026quot;min.node.size\u0026quot;, \u0026quot;honesty.prune.leaves\u0026quot;), num.trees = 2000) saveRDS(forest.Y, \u0026quot;s7.rds\u0026quot;) } else {forest.Y \u0026lt;- readRDS(\u0026quot;s7.rds\u0026quot;)} Y.hat = predict(forest.Y)$predictions  Grf:Select variables that predict Y. forest.Y.varimp = variable_importance(forest.Y) # Note: Forests may have a hard time when trained on very few variables # (e.g., ncol(X) = 1, 2, or 3). We recommend not being too aggressive # in selection. selected.vars = which(forest.Y.varimp / mean(forest.Y.varimp) \u0026gt; 0.2) This selects five variables of 20. Indeed these are the variables that were used to simulate Y.\n Grf: Finally, Fit causal forest using PS and selected covariates if(fullrun){ tau.forest2 = causal_forest(X[, selected.vars], Y, W, W.hat = W.hat, Y.hat = Y.hat, tune.parameters = c(\u0026quot;min.node.size\u0026quot;, \u0026quot;honesty.prune.leaves\u0026quot;), num.trees = 4000) saveRDS(tau.forest2, \u0026quot;s8.rds\u0026quot;) } else {tau.forest2 \u0026lt;- readRDS(\u0026quot;s8.rds\u0026quot;)}  Grf: Check fit and CI's df_sep2 \u0026lt;- CalcPredictionsGRF(df_sep4[,selected.vars], tau.forest2) grfp \u0026lt;- ggplot(df_sep2, aes(x = X3, y = qm)) + geom_linerange(aes(ymin = ql, ymax = qu), col = \u0026quot;grey\u0026quot;) + geom_point() + geom_smooth() + geom_line(aes(y = TAU), col = \u0026quot;red\u0026quot;, size = 1) + ylim(-0.7,2) grfp ## `geom_smooth()` using method = \u0026#39;gam\u0026#39; and formula \u0026#39;y ~ s(x, bs = \u0026quot;cs\u0026quot;)\u0026#39; This looks ok.\n  Fit BART on this dataset x.train \u0026lt;- model.matrix(~. ,data.frame(W, X)) x.test \u0026lt;- model.matrix(~. ,data.frame(W = 1 - W, X)) y.train \u0026lt;- Y if(fullrun){ bartFit \u0026lt;- bart(x.train, y.train, x.test, ntree = 4000) saveRDS(bartFit, \u0026quot;s9.rds\u0026quot;) } else {bartFit \u0026lt;- readRDS(\u0026quot;s9.rds\u0026quot;)} plot(bartFit)  BART: Check fit and CI's #source(\u0026quot;calcPosteriors.R\u0026quot;) sim \u0026lt;- CalcPosteriorsBART(df_sep4, bartFit, treatname = \u0026quot;W\u0026quot;) bartp \u0026lt;- ggplot(sim, aes(x = X3, qm)) + geom_linerange(aes(ymin = ql, ymax = qu), col = \u0026quot;grey\u0026quot;) + geom_point() + geom_smooth() + geom_line(aes(y = TAU), col = \u0026quot;red\u0026quot;, size = 1) + ylim(-0.7,2) bartp ## `geom_smooth()` using method = \u0026#39;gam\u0026#39; and formula \u0026#39;y ~ s(x, bs = \u0026quot;cs\u0026quot;)\u0026#39;  Compare BART and grf gp \u0026lt;- plot_grid(bartp, grfp) ## `geom_smooth()` using method = \u0026#39;gam\u0026#39; and formula \u0026#39;y ~ s(x, bs = \u0026quot;cs\u0026quot;)\u0026#39; ## `geom_smooth()` using method = \u0026#39;gam\u0026#39; and formula \u0026#39;y ~ s(x, bs = \u0026quot;cs\u0026quot;)\u0026#39; gp Very similar results. BART appears slightly more accurate, especially for low values of X3.\n  ","date":1544832000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544832000,"objectID":"5e61a0767e44f41cc719faa8702e5dae","permalink":"https://gsverhoeven.github.io/post/bart_vs_grf/bart-vs-grf-showdown/","publishdate":"2018-12-15T00:00:00Z","relpermalink":"/post/bart_vs_grf/bart-vs-grf-showdown/","section":"post","summary":"In this post, we test both `Bayesian Additive Regression Trees (BART)` and `Causal forests (grf)` on four simulated datasets of increasing complexity. May the best method win!","tags":["causal inference","BART","grf"],"title":"BART vs Causal forests showdown","type":"post"},{"authors":null,"categories":["machine learning"],"content":" The idea is that comparing the predictions of an RF model with the predictions of an OLS model can inform us in what ways the OLS model fails to capture all non-linearities and interactions between the predictors. Subsequently, using partial dependence plots of the RF model can guide the modelling of the non-linearities in the OLS model. After this step, the discrepancies between the RF predictions and the OLS predictions should be caused by non-modeled interactions. Using an RF to predict the discrepancy itself can then be used to discover which predictors are involved in these interactions. We test this method on the classic Boston Housing dataset to predict median house values (medv). We indeed recover interactions that, as it turns, have already been found and documented in the literature.\nLoad packages rm(list=ls()) #library(randomForest) #library(party) library(ranger) library(data.table) library(ggplot2) library(MASS) rdunif \u0026lt;- function(n,k) sample(1:k, n, replace = T)  Step 1: Run a RF on the Boston Housing set my_ranger \u0026lt;- ranger(medv ~ ., data = Boston, importance = \u0026quot;permutation\u0026quot;, num.trees = 500, mtry = 5, replace = TRUE) Extract the permutation importance measure.\nmyres_tmp \u0026lt;- ranger::importance(my_ranger); myres \u0026lt;- cbind(names(myres_tmp), myres_tmp, i = 1) #my_rownames \u0026lt;- row.names(myres) myres \u0026lt;- data.table(myres) setnames(myres, \u0026quot;V1\u0026quot;, \u0026quot;varname\u0026quot;) setnames(myres, \u0026quot;myres_tmp\u0026quot;, \u0026quot;MeanDecreaseAccuracy\u0026quot;) myres \u0026lt;- myres[, varname := as.factor(varname)] myres \u0026lt;- myres[, MeanDecreaseAccuracy := as.numeric(MeanDecreaseAccuracy)] myres \u0026lt;- myres[, i := as.integer(i)] ggplot(myres, aes(x = reorder(factor(varname), MeanDecreaseAccuracy), y = MeanDecreaseAccuracy)) + geom_point() + coord_flip()  Fit an OLS to the Boston Housing my_glm \u0026lt;- glm(medv ~., data = Boston, family = \u0026quot;gaussian\u0026quot;)  Compare predictions of both models pred_RF \u0026lt;- predict(my_ranger, data = Boston) #pred_RF$predictions pred_GLM \u0026lt;- predict(my_glm, data = Boston) plot(pred_RF$predictions, pred_GLM) abline(0, 1) # Run a RF on the discrepancy\nDiscrepancy is defined as the difference between the predictions of both models for each observation.\npred_diff \u0026lt;- pred_RF$predictions - pred_GLM my_ranger_diff \u0026lt;- ranger(Ydiff ~ . - medv, data = data.table(Ydiff = pred_diff, Boston), importance = \u0026quot;permutation\u0026quot;, num.trees = 500, mtry = 5, replace = TRUE) my_ranger_diff ## Ranger result ## ## Call: ## ranger(Ydiff ~ . - medv, data = data.table(Ydiff = pred_diff, Boston), importance = \u0026quot;permutation\u0026quot;, num.trees = 500, mtry = 5, replace = TRUE) ## ## Type: Regression ## Number of trees: 500 ## Sample size: 506 ## Number of independent variables: 13 ## Mtry: 5 ## Target node size: 5 ## Variable importance mode: permutation ## Splitrule: variance ## OOB prediction error (MSE): 5.036276 ## R squared (OOB): 0.6698162 It turns out the RF can \u0026quot;explain\u0026quot; 67% of these discrepancies.\nmyres_tmp \u0026lt;- ranger::importance(my_ranger_diff) myres \u0026lt;- cbind(names(myres_tmp), myres_tmp, i = 1) #my_rownames \u0026lt;- row.names(myres) myres \u0026lt;- data.table(myres) setnames(myres, \u0026quot;V1\u0026quot;, \u0026quot;varname\u0026quot;) setnames(myres, \u0026quot;myres_tmp\u0026quot;, \u0026quot;MeanDecreaseAccuracy\u0026quot;) myres \u0026lt;- myres[, varname := as.factor(varname)] myres \u0026lt;- myres[, MeanDecreaseAccuracy := as.numeric(MeanDecreaseAccuracy)] myres \u0026lt;- myres[, i := as.integer(i)] ggplot(myres, aes(x = reorder(factor(varname), MeanDecreaseAccuracy), y = MeanDecreaseAccuracy)) + geom_point() + coord_flip() It turns out that rm and lstat are the variables that best predict the discrepancy.\nmy_glm_int \u0026lt;- glm(medv ~. + rm:lstat, data = Boston, family = \u0026quot;gaussian\u0026quot;) summary(my_glm_int) ## ## Call: ## glm(formula = medv ~ . + rm:lstat, family = \u0026quot;gaussian\u0026quot;, data = Boston) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -21.5738 -2.3319 -0.3584 1.8149 27.9558 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 6.073638 5.038175 1.206 0.228582 ## crim -0.157100 0.028808 -5.453 7.85e-08 *** ## zn 0.027199 0.012020 2.263 0.024083 * ## indus 0.052272 0.053475 0.978 0.328798 ## chas 2.051584 0.750060 2.735 0.006459 ** ## nox -15.051627 3.324807 -4.527 7.51e-06 *** ## rm 7.958907 0.488520 16.292 \u0026lt; 2e-16 *** ## age 0.013466 0.011518 1.169 0.242918 ## dis -1.120269 0.175498 -6.383 4.02e-10 *** ## rad 0.320355 0.057641 5.558 4.49e-08 *** ## tax -0.011968 0.003267 -3.664 0.000276 *** ## ptratio -0.721302 0.115093 -6.267 8.06e-10 *** ## black 0.003985 0.002371 1.681 0.093385 . ## lstat 1.844883 0.191833 9.617 \u0026lt; 2e-16 *** ## rm:lstat -0.418259 0.032955 -12.692 \u0026lt; 2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 16.98987) ## ## Null deviance: 42716 on 505 degrees of freedom ## Residual deviance: 8342 on 491 degrees of freedom ## AIC: 2886 ## ## Number of Fisher Scoring iterations: 2 The interaction we have added is indeed highly significant.\nCompare approximate out-of-sample prediction accuracy using AIC:\nAIC(my_glm) ## [1] 3027.609 AIC(my_glm_int) ## [1] 2886.043 Indeed, the addition of the interaction greatly increases the prediction accuracy.\n Repeat this process pred_RF \u0026lt;- predict(my_ranger, data = Boston) #pred_RF$predictions pred_GLM \u0026lt;- predict(my_glm_int, data = Boston) plot(pred_RF$predictions, pred_GLM) abline(0, 1) pred_diff \u0026lt;- pred_RF$predictions - pred_GLM my_ranger_diff2 \u0026lt;- ranger(Ydiff ~ . - medv, data = data.table(Ydiff = pred_diff, Boston), importance = \u0026quot;permutation\u0026quot;, num.trees = 500, mtry = 5, replace = TRUE) my_ranger_diff2 ## Ranger result ## ## Call: ## ranger(Ydiff ~ . - medv, data = data.table(Ydiff = pred_diff, Boston), importance = \u0026quot;permutation\u0026quot;, num.trees = 500, mtry = 5, replace = TRUE) ## ## Type: Regression ## Number of trees: 500 ## Sample size: 506 ## Number of independent variables: 13 ## Mtry: 5 ## Target node size: 5 ## Variable importance mode: permutation ## Splitrule: variance ## OOB prediction error (MSE): 5.569622 ## R squared (OOB): 0.4348793 myres_tmp \u0026lt;- ranger::importance(my_ranger_diff2) myres \u0026lt;- cbind(names(myres_tmp), myres_tmp, i = 1) #my_rownames \u0026lt;- row.names(myres) myres \u0026lt;- data.table(myres) setnames(myres, \u0026quot;V1\u0026quot;, \u0026quot;varname\u0026quot;) setnames(myres, \u0026quot;myres_tmp\u0026quot;, \u0026quot;MeanDecreaseAccuracy\u0026quot;) myres \u0026lt;- myres[, varname := as.factor(varname)] myres \u0026lt;- myres[, MeanDecreaseAccuracy := as.numeric(MeanDecreaseAccuracy)] myres \u0026lt;- myres[, i := as.integer(i)] ggplot(myres, aes(x = reorder(factor(varname), MeanDecreaseAccuracy), y = MeanDecreaseAccuracy)) + geom_point() + coord_flip() Now the variables that best predict the discrepancy are lstat and dis. Add these two variables as an interaction.\nmy_glm_int2 \u0026lt;- glm(medv ~. + rm:lstat + lstat:dis, data = Boston, family = \u0026quot;gaussian\u0026quot;) summary(my_glm_int2) ## ## Call: ## glm(formula = medv ~ . + rm:lstat + lstat:dis, family = \u0026quot;gaussian\u0026quot;, ## data = Boston) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -23.3918 -2.2997 -0.4077 1.6475 27.6766 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 1.552991 5.107295 0.304 0.761201 ## crim -0.139370 0.028788 -4.841 1.73e-06 *** ## zn 0.042984 0.012550 3.425 0.000667 *** ## indus 0.066690 0.052878 1.261 0.207834 ## chas 1.760779 0.743688 2.368 0.018290 * ## nox -11.544280 3.404577 -3.391 0.000753 *** ## rm 8.640503 0.513593 16.824 \u0026lt; 2e-16 *** ## age -0.002127 0.012067 -0.176 0.860140 ## dis -1.904982 0.268056 -7.107 4.22e-12 *** ## rad 0.304689 0.057000 5.345 1.39e-07 *** ## tax -0.011220 0.003228 -3.476 0.000554 *** ## ptratio -0.641380 0.115418 -5.557 4.51e-08 *** ## black 0.003756 0.002339 1.606 0.108924 ## lstat 1.925223 0.190368 10.113 \u0026lt; 2e-16 *** ## rm:lstat -0.466947 0.034897 -13.381 \u0026lt; 2e-16 *** ## dis:lstat 0.076716 0.020009 3.834 0.000143 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 16.52869) ## ## Null deviance: 42716.3 on 505 degrees of freedom ## Residual deviance: 8099.1 on 490 degrees of freedom ## AIC: 2873.1 ## ## Number of Fisher Scoring iterations: 2 AIC(my_glm_int2) ## [1] 2873.087 AIC(my_glm_int) ## [1] 2886.043 We conclude that the second interaction also results in significant model improvement.\n A more ambitious goal: Try and improve Harrison \u0026amp; Rubinfeld's model formula for Boston housing So far, we assumed that all relationships are linear. Harrison and Rubinfeld have created a model without interactions, but with transformations to correct for skewness, heteroskedasticity etc. Let's see if we can improve upon this model equation by applying our method to search for interactions. Their formula predicts log(medv).\n# Harrison and Rubinfeld (1978) model my_glm_hr \u0026lt;- glm(log(medv) ~ I(rm^2) + age + log(dis) + log(rad) + tax + ptratio + black + I(black^2) + log(lstat) + crim + zn + indus + chas + I(nox^2), data = Boston, family = \u0026quot;gaussian\u0026quot;) summary(my_glm_hr) ## ## Call: ## glm(formula = log(medv) ~ I(rm^2) + age + log(dis) + log(rad) + ## tax + ptratio + black + I(black^2) + log(lstat) + crim + ## zn + indus + chas + I(nox^2), family = \u0026quot;gaussian\u0026quot;, data = Boston) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.73091 -0.09274 -0.00710 0.09800 0.78607 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 4.474e+00 1.579e-01 28.343 \u0026lt; 2e-16 *** ## I(rm^2) 6.634e-03 1.313e-03 5.053 6.15e-07 *** ## age 3.491e-05 5.245e-04 0.067 0.946950 ## log(dis) -1.927e-01 3.325e-02 -5.796 1.22e-08 *** ## log(rad) 9.613e-02 1.905e-02 5.047 6.35e-07 *** ## tax -4.295e-04 1.222e-04 -3.515 0.000481 *** ## ptratio -2.977e-02 5.024e-03 -5.926 5.85e-09 *** ## black 1.520e-03 5.068e-04 3.000 0.002833 ** ## I(black^2) -2.597e-06 1.114e-06 -2.331 0.020153 * ## log(lstat) -3.695e-01 2.491e-02 -14.833 \u0026lt; 2e-16 *** ## crim -1.157e-02 1.246e-03 -9.286 \u0026lt; 2e-16 *** ## zn 7.257e-05 5.034e-04 0.144 0.885430 ## indus -1.943e-04 2.360e-03 -0.082 0.934424 ## chas 9.180e-02 3.305e-02 2.777 0.005690 ** ## I(nox^2) -6.566e-01 1.129e-01 -5.815 1.09e-08 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 0.03299176) ## ## Null deviance: 84.376 on 505 degrees of freedom ## Residual deviance: 16.199 on 491 degrees of freedom ## AIC: -273.48 ## ## Number of Fisher Scoring iterations: 2 my_ranger_log \u0026lt;- ranger(log(medv) ~ ., data = Boston, importance = \u0026quot;permutation\u0026quot;, num.trees = 500, mtry = 5, replace = TRUE) pred_RF \u0026lt;- predict(my_ranger_log, data = Boston) #pred_RF$predictions pred_GLM \u0026lt;- predict(my_glm_hr, data = Boston) plot(pred_RF$predictions, pred_GLM) abline(0, 1) For low predicted values both models differ in a systematic way. This suggests that there exists a remaining pattern that is picked up by RF but not by the OLS model.\npred_diff \u0026lt;- pred_RF$predictions - pred_GLM my_ranger_log_diff \u0026lt;- ranger(Ydiff ~ . - medv, data = data.table(Ydiff = pred_diff, Boston), importance = \u0026quot;permutation\u0026quot;, num.trees = 500, mtry = 5, replace = TRUE) my_ranger_log_diff ## Ranger result ## ## Call: ## ranger(Ydiff ~ . - medv, data = data.table(Ydiff = pred_diff, Boston), importance = \u0026quot;permutation\u0026quot;, num.trees = 500, mtry = 5, replace = TRUE) ## ## Type: Regression ## Number of trees: 500 ## Sample size: 506 ## Number of independent variables: 13 ## Mtry: 5 ## Target node size: 5 ## Variable importance mode: permutation ## Splitrule: variance ## OOB prediction error (MSE): 0.008999277 ## R squared (OOB): 0.5351876 The RF indicates that 54% of the discrepancy can be \u0026quot;explained\u0026quot; by RF.\nmyres_tmp \u0026lt;- ranger::importance(my_ranger_log_diff) myres \u0026lt;- cbind(names(myres_tmp), myres_tmp, i = 1) #my_rownames \u0026lt;- row.names(myres) myres \u0026lt;- data.table(myres) setnames(myres, \u0026quot;V1\u0026quot;, \u0026quot;varname\u0026quot;) setnames(myres, \u0026quot;myres_tmp\u0026quot;, \u0026quot;MeanDecreaseAccuracy\u0026quot;) myres \u0026lt;- myres[, varname := as.factor(varname)] myres \u0026lt;- myres[, MeanDecreaseAccuracy := as.numeric(MeanDecreaseAccuracy)] myres \u0026lt;- myres[, i := as.integer(i)] ggplot(myres, aes(x = reorder(factor(varname), MeanDecreaseAccuracy), y = MeanDecreaseAccuracy)) + geom_point() + coord_flip() Add the top 2 vars as an interaction to their model equation.\nmy_glm_hr_int \u0026lt;- glm(log(medv) ~ I(rm^2) + age + log(dis) + log(rad) + tax + ptratio + black + I(black^2) + log(lstat) + crim + zn + indus + chas + I(nox^2) + lstat:nox, data = Boston, family = \u0026quot;gaussian\u0026quot;) summary(my_glm_hr_int) ## ## Call: ## glm(formula = log(medv) ~ I(rm^2) + age + log(dis) + log(rad) + ## tax + ptratio + black + I(black^2) + log(lstat) + crim + ## zn + indus + chas + I(nox^2) + lstat:nox, family = \u0026quot;gaussian\u0026quot;, ## data = Boston) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.70340 -0.09274 -0.00665 0.10068 0.75004 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 4.243e+00 1.613e-01 26.304 \u0026lt; 2e-16 *** ## I(rm^2) 7.053e-03 1.286e-03 5.484 6.66e-08 *** ## age -3.146e-04 5.174e-04 -0.608 0.54354 ## log(dis) -2.254e-01 3.317e-02 -6.795 3.15e-11 *** ## log(rad) 9.829e-02 1.862e-02 5.278 1.96e-07 *** ## tax -4.589e-04 1.196e-04 -3.838 0.00014 *** ## ptratio -2.990e-02 4.910e-03 -6.089 2.30e-09 *** ## black 1.445e-03 4.955e-04 2.917 0.00370 ** ## I(black^2) -2.470e-06 1.089e-06 -2.268 0.02376 * ## log(lstat) -2.143e-01 3.989e-02 -5.373 1.20e-07 *** ## crim -1.046e-02 1.238e-03 -8.448 3.40e-16 *** ## zn 7.309e-04 5.099e-04 1.434 0.15234 ## indus -8.166e-05 2.307e-03 -0.035 0.97178 ## chas 8.746e-02 3.231e-02 2.707 0.00704 ** ## I(nox^2) -3.618e-01 1.256e-01 -2.880 0.00415 ** ## lstat:nox -2.367e-02 4.819e-03 -4.911 1.24e-06 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 0.03150809) ## ## Null deviance: 84.376 on 505 degrees of freedom ## Residual deviance: 15.439 on 490 degrees of freedom ## AIC: -295.79 ## ## Number of Fisher Scoring iterations: 2 AIC(my_glm_hr) ## [1] -273.4788 AIC(my_glm_hr_int) ## [1] -295.7931 This results in a significant improvement!\n Repeat this procedure pred_RF \u0026lt;- predict(my_ranger_log, data = Boston) #pred_RF$predictions pred_GLM \u0026lt;- predict(my_glm_hr_int, data = Boston) plot(pred_RF$predictions, pred_GLM) abline(0, 1) pred_diff \u0026lt;- pred_RF$predictions - pred_GLM my_ranger_log_diff2 \u0026lt;- ranger(Ydiff ~ . - medv, data = data.table(Ydiff = pred_diff, Boston), importance = \u0026quot;permutation\u0026quot;, num.trees = 500, mtry = 5, replace = TRUE) my_ranger_log_diff2 ## Ranger result ## ## Call: ## ranger(Ydiff ~ . - medv, data = data.table(Ydiff = pred_diff, Boston), importance = \u0026quot;permutation\u0026quot;, num.trees = 500, mtry = 5, replace = TRUE) ## ## Type: Regression ## Number of trees: 500 ## Sample size: 506 ## Number of independent variables: 13 ## Mtry: 5 ## Target node size: 5 ## Variable importance mode: permutation ## Splitrule: variance ## OOB prediction error (MSE): 0.008787765 ## R squared (OOB): 0.5146271 myres_tmp \u0026lt;- ranger::importance(my_ranger_log_diff2) myres \u0026lt;- cbind(names(myres_tmp), myres_tmp, i = 1) #my_rownames \u0026lt;- row.names(myres) myres \u0026lt;- data.table(myres) setnames(myres, \u0026quot;V1\u0026quot;, \u0026quot;varname\u0026quot;) setnames(myres, \u0026quot;myres_tmp\u0026quot;, \u0026quot;MeanDecreaseAccuracy\u0026quot;) myres \u0026lt;- myres[, varname := as.factor(varname)] myres \u0026lt;- myres[, MeanDecreaseAccuracy := as.numeric(MeanDecreaseAccuracy)] myres \u0026lt;- myres[, i := as.integer(i)] ggplot(myres, aes(x = reorder(factor(varname), MeanDecreaseAccuracy), y = MeanDecreaseAccuracy)) + geom_point() + coord_flip() Now we add lstat and dis as an interaction.\nmy_glm_hr_int2 \u0026lt;- glm(log(medv) ~ I(rm^2) + age + log(dis) + log(rad) + tax + ptratio + black + I(black^2) + log(lstat) + crim + zn + indus + chas + I(nox^2) + lstat:nox + lstat:dis, data = Boston, family = \u0026quot;gaussian\u0026quot;) summary(my_glm_hr_int2) ## ## Call: ## glm(formula = log(medv) ~ I(rm^2) + age + log(dis) + log(rad) + ## tax + ptratio + black + I(black^2) + log(lstat) + crim + ## zn + indus + chas + I(nox^2) + lstat:nox + lstat:dis, family = \u0026quot;gaussian\u0026quot;, ## data = Boston) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.70136 -0.08746 -0.00589 0.08857 0.76349 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 4.535e+00 1.712e-01 26.481 \u0026lt; 2e-16 *** ## I(rm^2) 7.498e-03 1.266e-03 5.924 5.94e-09 *** ## age -1.262e-03 5.504e-04 -2.293 0.02226 * ## log(dis) -4.065e-01 5.203e-02 -7.813 3.43e-14 *** ## log(rad) 9.668e-02 1.828e-02 5.290 1.85e-07 *** ## tax -4.622e-04 1.173e-04 -3.940 9.35e-05 *** ## ptratio -2.640e-02 4.881e-03 -5.409 9.93e-08 *** ## black 1.313e-03 4.871e-04 2.696 0.00727 ** ## I(black^2) -2.172e-06 1.071e-06 -2.029 0.04303 * ## log(lstat) -3.181e-01 4.553e-02 -6.987 9.23e-12 *** ## crim -1.049e-02 1.215e-03 -8.635 \u0026lt; 2e-16 *** ## zn 9.078e-04 5.019e-04 1.809 0.07108 . ## indus -2.733e-04 2.264e-03 -0.121 0.90395 ## chas 7.166e-02 3.191e-02 2.246 0.02515 * ## I(nox^2) -2.569e-01 1.255e-01 -2.048 0.04113 * ## lstat:nox -2.729e-02 4.798e-03 -5.689 2.21e-08 *** ## lstat:dis 3.906e-03 8.754e-04 4.462 1.01e-05 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 0.03033711) ## ## Null deviance: 84.376 on 505 degrees of freedom ## Residual deviance: 14.835 on 489 degrees of freedom ## AIC: -313.99 ## ## Number of Fisher Scoring iterations: 2 AIC(my_glm_hr_int2) ## [1] -313.9904 AIC(my_glm_hr_int) ## [1] -295.7931 And again we find an improvement in model fit.\n Have these interactions already been reported on in the literature? Tom Minka reports on his website an analysis of interactions in the Boston Housing set:\n(http://alumni.media.mit.edu/~tpminka/courses/36-350.2001/lectures/day30/) \u0026gt; summary(fit3) Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) -227.5485 49.2363 -4.622 4.87e-06 *** lstat 50.8553 20.3184 2.503 0.012639 * rm 38.1245 7.0987 5.371 1.21e-07 *** dis -16.8163 2.9174 -5.764 1.45e-08 *** ptratio 14.9592 2.5847 5.788 1.27e-08 *** lstat:rm -6.8143 3.1209 -2.183 0.029475 * lstat:dis 4.8736 1.3940 3.496 0.000514 *** lstat:ptratio -3.3209 1.0345 -3.210 0.001412 ** rm:dis 2.0295 0.4435 4.576 5.99e-06 *** rm:ptratio -1.9911 0.3757 -5.299 1.76e-07 *** lstat:rm:dis -0.5216 0.2242 -2.327 0.020364 * lstat:rm:ptratio 0.3368 0.1588 2.121 0.034423 *\nRob mcCulloch, using BART (bayesian additive regression trees) also examines interactions in the Boston Housing data. There the co-occurence within trees is used to discover interactions:\nThe second, interaction detection, uncovers which pairs of variables interact in analogous fashion by keeping track of the percentage of trees in the sum in which both variables occur. This exploits the fact that a sum-of-trees model captures an interaction between xi and xj by using them both for splitting rules in the same tree.\nhttp://www.rob-mcculloch.org/some_papers_and_talks/papers/working/cgm_as.pdf\n  Conclusion We conclude that this appears a fruitfull approach to at least discovering where a regression model can be improved.\n ","date":1504224000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504224000,"objectID":"f6e1d7cb765b378e9f12194e8cd7fa31","permalink":"https://gsverhoeven.github.io/post/interaction_detection/interaction-detection/","publishdate":"2017-09-01T00:00:00Z","relpermalink":"/post/interaction_detection/interaction-detection/","section":"post","summary":"In this post, I explore how we can improve a parametric regression model by comparing its predictions to those of a Random Forest model. This might informs us in what ways the OLS model fails to capture all non-linearities and interactions between the predictors.","tags":["interaction detection","random forest","ranger"],"title":"Improving a parametric regression model using machine learning","type":"post"},{"authors":null,"categories":null,"content":"Summary We review the book by Bogetoft and Olesen on Cooperatives and payment schemes. We investigate the potential for using its material to explore the ability of reinforcement learning to solve games. The idea is to start with games that can be easily solved using standard game theoretic analysis, and then add complexity that makes them hard to solve using the standard techniques.\nGame theory vs Reinforcement learning If we take a simple game such as the Prisoners dilemma, we see that we have two players, that have access to the same action sets (confess or not confess), and perform their action simultaneously.\nEach players outcome (the Reward) depends on the action of the other player.\nIf they both remain silent, i.e. do not confess, they both get -1. However, if one of them cooperates and confesses, and the other remains silent, the prisoner that confesses gets 0, and the one remaining silent gets -10. If both of them confess, they both get a penalty of -5.\nThe games is played only once: so it does not make sense to \u0026ldquo;act nice\u0026rdquo; for future reward. Now we look for a strategy that is a Nash equilibrium. This means that no player wants to change its action, given the other players action. If both remain silent, then a player is better of to switch to confess. If both confess, then no player is better of remain silent. So both confessing is the Nash equilibrium for this game. We can also call this the rational or optimal solution of the game.\nIf we translate this to a reinforcement learning setting, we end up in what is called a Multi Agent Reinforcement learning (MARL) setting. We immediately spot a difference with GT: in RL, players play the game repeatedly. So could it be possible to reach the optimal solution of both remaining silent? And: does this difference mean that we cannot use MARL to solve one shot static games? And: if so, is this a problem? The real life systems we wish to model are typically dynamic (i.e. repeat) games as well.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"98720739d6189e32023b8d74399b44c2","permalink":"https://gsverhoeven.github.io/post/bogetoft_rl/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/bogetoft_rl/","section":"post","summary":"Summary We review the book by Bogetoft and Olesen on Cooperatives and payment schemes. We investigate the potential for using its material to explore the ability of reinforcement learning to solve games. The idea is to start with games that can be easily solved using standard game theoretic analysis, and then add complexity that makes them hard to solve using the standard techniques.\nGame theory vs Reinforcement learning If we take a simple game such as the Prisoners dilemma, we see that we have two players, that have access to the same action sets (confess or not confess), and perform their action simultaneously.","tags":null,"title":"Cooperatives and payment schemes","type":"post"}]