<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Gertjan Verhoeven</title>
    <link>/</link>
      <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <description>Gertjan Verhoeven</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2019, 2020</copyright><lastBuildDate>Sun, 06 Jun 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Gertjan Verhoeven</title>
      <link>/</link>
    </image>
    
    <item>
      <title>Using R to analyse the Roche Antigen Rapid Test: How accurate is it?</title>
      <link>/post/covid_antigen_test_reliability/</link>
      <pubDate>Sun, 06 Jun 2021 00:00:00 +0000</pubDate>
      <guid>/post/covid_antigen_test_reliability/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;em&gt;(Image is from a different antigen test)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Many people are suspicious about the reliability of rapid self-tests, so I decided to check it out.
For starters, I LOVE measurement. It is where learning from data starts, with technology and statistics involved.
With this post, I’d like to join the swelling ranks of amateur epidemiologists :) I have spent a few years in a molecular biology lab, that should count for something right?&lt;/p&gt;
&lt;p&gt;At home, we now have a box of the &lt;strong&gt;SARS-CoV-2 Rapid Antigen Test Nasal&lt;/strong&gt; kit.
The kit is distributed by Roche, and manufactured in South Korea by a company called SD Biosensor.&lt;/p&gt;
&lt;p&gt;So how reliable is it? A practical approach is to compare it to the golden standard, the PCR test, that public health test centers use to detect COVID-19. Well, the leaflet of the kit describes three experiments that do exactly that!
So I tracked down the data mentioned in the kit’s leaflet, and decided to check them out.&lt;/p&gt;
&lt;p&gt;But before we analyze the data, you want to know how they were generated, right? RIGHT?
For this we use cause-effect diagrams (a.k.a. DAGs), which we can quickly draw using &lt;a href=&#34;http://dagitty.net&#34;&gt;DAGitty&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;a-causal-model-of-the-measurement-process&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A causal model of the measurement process&lt;/h1&gt;
&lt;p&gt;The cool thing about DAGitty is that we can use a point-n-click interface to draw the diagram, and then export code that contains an exact description of the graph to include in R. (You can also view the &lt;a href=&#34;http://dagitty.net/dags.html?id=whqGBx&#34;&gt;graph for this blog post at DAGitty.net&lt;/a&gt;):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021_05_14_covid_rapid_test_reliability_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The graph is based on the following description of the various cause-effect pairs:&lt;/p&gt;
&lt;p&gt;It all starts with whether someone is infected. After infection, virus particles start to build up. These particles can be in the lungs, in the throat, nose etc.
These particles either do or do not cause symptoms. Whether there are symptoms will likely influence the decision to test, but there will also be people without symptoms that will be tested (i.e. if a family member was tested positive).&lt;/p&gt;
&lt;p&gt;In the experiments we analyze, two samples were taken, one for the PCR test and one for the antigen test. The way the samples were taken differed as well: “shallow” nose swabs for the rapid antigen test, and a combination of “deep” nose and throat swabs for the PCR test.&lt;/p&gt;
&lt;p&gt;Now that we now a bit about the measurement process, lets look at how the accuracy of the antigen test is quantified.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;quantifying-the-accuracy-of-an-covid-19-test&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Quantifying the accuracy of an COVID-19 test&lt;/h1&gt;
&lt;p&gt;The PCR test result serves as the ground truth, the standard to which the antigen test is compared.
Both tests are binary tests, either it detects the infection or it does not (to a first approximation).&lt;/p&gt;
&lt;p&gt;For this type of outcome, two concepts are key: the &lt;strong&gt;sensitivity&lt;/strong&gt; (does the antigen test detect COVID when the PCR test has detected it) and &lt;strong&gt;specificity&lt;/strong&gt; of the test (does the antigen test ONLY detect COVID, or also other flu types or even unrelated materials, for which the PCR test produces a negative result).&lt;/p&gt;
&lt;p&gt;The leaflet contains information on both.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sensitivity 83.3% (95%CI: 74.7% - 90.0%)&lt;/li&gt;
&lt;li&gt;Specificity 99.1% (95%CI: 97.7% - 99.7%)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;But what does this really tell us? And where do these numbers come from?&lt;/p&gt;
&lt;p&gt;Before we go to the data, we first need to know a bit more detail on what we are actually trying to measure, the viral load, and what factors influence this variable.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;viral-load-as-target-for-measurement&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Viral load as target for measurement&lt;/h1&gt;
&lt;p&gt;So, both tests work by detecting viral particles in a particular sample. The amount of virus particles present in the sample depends on, among others:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Time since infection&lt;/li&gt;
&lt;li&gt;How and where the sample is taken (throat, nose, lungs, using a swab etc)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I’ll discuss both.&lt;/p&gt;
&lt;div id=&#34;time-since-infection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Time since infection&lt;/h2&gt;
&lt;p&gt;When you have just been infected, your body will contain only a small amount of virus.
The &lt;strong&gt;viral load&lt;/strong&gt; is a function of time since infection, because it takes time for the virus to multiply itself. Even PCR cannot detect an infection on the first day, and even after 8 days, there are still some 20% of cases that go undetected by PCR (presumably because the amount of viral particle is too low) (Ref: Kucirka et al 2020).&lt;/p&gt;
&lt;p&gt;If you want to know more about the ability of PCR to detect COVID infections go check out &lt;a href=&#34;https://github.com/HopkinsIDD/covidRTPCR&#34;&gt;the covidRTPCR Github repository&lt;/a&gt;. It is completely awesome, with open data, open code, and Bayesian statistics using &lt;a href=&#34;https://mc-stan.org/&#34;&gt;Stan&lt;/a&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-and-where-the-sample-is-taken&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How and where the sample is taken&lt;/h2&gt;
&lt;p&gt;There are many ways to obtain a sample from a person.&lt;/p&gt;
&lt;p&gt;Here the golden standard is a so-called &lt;strong&gt;Nasopharyngeal swab&lt;/strong&gt;. This goes through your nose all the way (~ 5 cm) into the back of the throat, and is highly uncomfortable. Typically, only professional health workers perform &lt;strong&gt;nasopharyngeal&lt;/strong&gt; swabs.
In these experiments, this deep nose swab was combined with a swab from the throat (&lt;strong&gt;oroharyngeal&lt;/strong&gt;). This is also how test centers in the Netherlands operated during the last year.&lt;/p&gt;
&lt;p&gt;There are various alternatives: We have spit, saliva, we can cough up “sputum” (slime from the lungs) or we can take swab from the front part of the nose (“nasal”).&lt;/p&gt;
&lt;p&gt;The Roche antigen test is a &lt;strong&gt;nasal&lt;/strong&gt; test that only goes up to 2 cm in the nose and can be used by patients themselves (“self-collected”).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-dataset-results-from-the-three-berlin-studies&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The dataset: results from the three Berlin studies&lt;/h1&gt;
&lt;p&gt;Now that we have some background info, we are ready to check the data!&lt;/p&gt;
&lt;p&gt;As mentioned above, this data came from three experiments on samples from in total 547 persons.&lt;/p&gt;
&lt;p&gt;After googling a bit, I found out that the experiments were performed by independent researchers in a famous University hospital in Berlin, &lt;a href=&#34;https://de.wikipedia.org/wiki/Charit%C3%A9&#34;&gt;Charité&lt;/a&gt;. After googling a bit more and mailing with one of the researchers involved, Dr. Andreas Lindner, I received a list of papers that describe the research mentioned in the leaflet (References at the end of this post).&lt;/p&gt;
&lt;p&gt;The dataset for the blog post compares &lt;strong&gt;nasal&lt;/strong&gt; samples tested with the Roche Antigen test kit, to PCR-tested &lt;strong&gt;nasopharyngeal&lt;/strong&gt; plus &lt;strong&gt;oropharyngeal&lt;/strong&gt; samples taken by professionals.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;This blog post is possible because the three papers by Lindner and co-workers all contain the raw data as a table in the paper. Cool!&lt;/strong&gt;
Unfortunately, this means the data is not &lt;strong&gt;machine readable&lt;/strong&gt;. However, with a combination of manual tweaking / find-replace and some coding, I tidied the data of the three studies into a single &lt;code&gt;tibble&lt;/code&gt; data frame. You can grab the code and data from my &lt;a href=&#34;https://github.com/gsverhoeven/hugo_source/tree/master/content/post/sars_test&#34;&gt;Github&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# creates df_pcr_pos
source(&amp;quot;sars_test/dataprep_roche_test.R&amp;quot;)

# creates df_leaflet
source(&amp;quot;sars_test/dataprep_roche_test_leaflet.R&amp;quot;)

# see below
source(&amp;quot;sars_test/bootstrap_conf_intervals.R&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The dataset &lt;code&gt;df_pcr_pos&lt;/code&gt; contains, for each &lt;strong&gt;PCR positive&lt;/strong&gt; patient:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ct_value&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;viral_load&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;days_of_symptoms&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mm_value&lt;/code&gt; (Result of a &lt;strong&gt;nasal&lt;/strong&gt; antigen test measurement, 1 is positive, 0 is negative)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To understand the PCR data, we need to know a bit more about the PCR method.&lt;/p&gt;
&lt;div id=&#34;the-pcr-method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The PCR method&lt;/h2&gt;
&lt;p&gt;The PCR method not only measures &lt;strong&gt;if&lt;/strong&gt; someone is infected, it also provides an estimate of the viral load in the sample.
How does this work? PCR can amplify, in so-called cycles, really low quantities of viral material in a biological sample. The amount of cycles of the PCR device needed to reach a threshold of signal is called the cycle threshold or &lt;strong&gt;Ct value&lt;/strong&gt;. The less material we have in our sample, the more cycles we need to amplify the signal to reach a certain threshold.&lt;/p&gt;
&lt;p&gt;Because the amplification is an exponential process, if we take the log of the number of virus particles, we get a linear inverse (negative) relationship between &lt;strong&gt;ct_value&lt;/strong&gt; and &lt;strong&gt;viral_load&lt;/strong&gt;. For example, &lt;span class=&#34;math inline&#34;&gt;\(10^6\)&lt;/span&gt; particles is a viral load of 6 on the log10 scale.&lt;/p&gt;
&lt;p&gt;So let’s plot the &lt;code&gt;ct_value&lt;/code&gt; of the PCR test vs the &lt;code&gt;viral_load&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)
ggplot(df_pcr_pos, aes(x = ct_value, y = viral_load, color = factor(pcr_assay_type))) + 
  geom_point() + ggtitle(&amp;quot;Calibration curves for viral load (log10 scale)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021_05_14_covid_rapid_test_reliability_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;
This plot shows that &lt;code&gt;viral_load&lt;/code&gt; is directly derived from the &lt;code&gt;ct_value&lt;/code&gt; through a calibration factor.
PCR Ct values of &amp;gt; 35 are considered as the threshold value for detecting a COVID infection using the PCR test, so the values in this plot make sense for COVID positive samples.&lt;/p&gt;
&lt;p&gt;Take some time to appreciate the huge range difference in the samples on display here.
From only 10.000 viral particles (&lt;span class=&#34;math inline&#34;&gt;\(log_{10}{(10^4)} = 4\)&lt;/span&gt; ) to almost 1 billion (&lt;span class=&#34;math inline&#34;&gt;\(log_{10}{(10^9)} = 9\)&lt;/span&gt; ) particles.&lt;/p&gt;
&lt;p&gt;We can also see that apparently, there were two separate PCR assays (test types), each with a separate conversion formula used to obtain the estimated viral load.&lt;/p&gt;
&lt;p&gt;(&lt;strong&gt;N.b.&lt;/strong&gt; The missings for &lt;code&gt;pcr_assay_type&lt;/code&gt; are because for two of three datasets, it was difficult to extract this information from the PDF file. From the plot, we can conclude that for these datasets, the same two assays were used since the values map onto the same two calibration lines)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sensitivity-of-the-antigen-test&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sensitivity of the Antigen test&lt;/h2&gt;
&lt;p&gt;The dataset contains all samples for which the PCR test was positive.
Let’s start by checking the raw percentage of antigen test measurements that are positive as well.
This is called the &lt;strong&gt;sensitivity&lt;/strong&gt; of a test.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res &amp;lt;- df_pcr_pos %&amp;gt;%
  summarize(sensitivity = mean(mm_value), 
            N = n())

res&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 2
##   sensitivity     N
##         &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;
## 1       0.792   120&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So for all PCR positive samples, 79.2 % is positive as well.
This means that, on average, if we would use the antigen test kit, we have a one in five (20%) probability of not detecting COVID-19, compared to when we would have used the method used by test centers operated by the public health agencies.&lt;/p&gt;
&lt;p&gt;This value is slightly lower, but close to what is mentioned in the Roche kit’s leaflet.&lt;/p&gt;
&lt;p&gt;Let’s postpone evaluation of this fact for a moment and look a bit closer at the data.
For example, we can example the relationship between &lt;code&gt;viral_load&lt;/code&gt; and a positive antigen test result (&lt;code&gt;mm_value&lt;/code&gt; = 1):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(df_pcr_pos$mm_value, df_pcr_pos_np$mm_value)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    
##      0  1
##   0 20  5
##   1  5 90&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)
ggplot(df_pcr_pos, aes(x = viral_load, y = mm_value)) + 
  geom_jitter(height = 0.1) +
  geom_smooth() + 
  geom_vline(xintercept = c(5.7, 7), col = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021_05_14_covid_rapid_test_reliability_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From this plot, we can see that the probability of obtaining a false negative result (&lt;code&gt;mm_value&lt;/code&gt; of 0) on the antigen test decreases as the viral load &lt;strong&gt;of the PCR sample&lt;/strong&gt; increases.&lt;/p&gt;
&lt;p&gt;From the data we also see that before the antigen test to work about half of the time (blue line at 0.5), the PCR sample needs to contain around &lt;span class=&#34;math inline&#34;&gt;\(5 \cdot 10^5\)&lt;/span&gt; viral particles (log10 scale 5.7), and for it to work reliably, we need around &lt;span class=&#34;math inline&#34;&gt;\(10^7\)&lt;/span&gt; particles (“high” viral load) in the PCR sample (which is a combination of &lt;strong&gt;oropharyngeal and nasopharyngeal swab&lt;/strong&gt;). This last bit is important: the researchers did not measure the viral load in the nasal swabs used for the antigen test, these are likely different.&lt;/p&gt;
&lt;p&gt;For really high viral loads, above &lt;span class=&#34;math inline&#34;&gt;\(10^7\)&lt;/span&gt; particles in the &lt;strong&gt;NP/OP sample&lt;/strong&gt;, the probability of a false negative result is only a few percent:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_pcr_pos %&amp;gt;% filter(viral_load &amp;gt;= 7) %&amp;gt;%
  summarize(sensitivity = mean(mm_value), 
            N = n())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 2
##   sensitivity     N
##         &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;
## 1       0.972    71&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;viral-loads-varies-with-days-of-symptoms&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Viral loads varies with days of symptoms&lt;/h1&gt;
&lt;p&gt;Above, we already discussed that the viral load varies with the time since infection.&lt;/p&gt;
&lt;p&gt;If we want to use the antigen test &lt;strong&gt;instead&lt;/strong&gt; of taking a PCR test, we don’t have information on the viral load. What we often do have is the days since symptoms, and we know that in the first few days of symptoms viral load is highest.&lt;/p&gt;
&lt;p&gt;We can check this by plotting the &lt;code&gt;days_of_symptoms&lt;/code&gt; versus &lt;code&gt;viral_load&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(df_pcr_pos, aes(x = days_of_symptoms, y = viral_load)) + 
  geom_smooth() + expand_limits(x = -4) + geom_vline(xintercept = 1, linetype = &amp;quot;dashed&amp;quot;) +
  geom_vline(xintercept = c(3, 7), col = &amp;quot;red&amp;quot;) + geom_hline(yintercept = 7, col = &amp;quot;grey&amp;quot;, linetype = &amp;quot;dashed&amp;quot;) +
  geom_jitter(height = 0, width = 0.2) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021_05_14_covid_rapid_test_reliability_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;
From this plot, we learn that the viral load is highest on the onset of symptoms day (typically 5 days after infection) and decreases afterwards.&lt;/p&gt;
&lt;p&gt;Above, we saw that the sensitivity in the whole sample was not equal to the sensitivity mentioned in the leaflet.
When evaluating rapid antigen tests, sometimes thresholds for days of symptoms are used, for example &amp;lt;= 3 days or &amp;lt;= 7 days (plotted in red).&lt;/p&gt;
&lt;p&gt;For the sensitivity in the leaflet, a threshold of &amp;lt;= 7 days was used on the days of symptoms.&lt;/p&gt;
&lt;p&gt;Let us see how sensitive the antigen test is for these subgroups:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res &amp;lt;- df_pcr_pos %&amp;gt;%
  filter(days_of_symptoms &amp;lt;= 3) %&amp;gt;%
  summarize(label = &amp;quot;&amp;lt; 3 days&amp;quot;,
            sensitivity = mean(mm_value), 
            N = n())

res2 &amp;lt;- df_pcr_pos %&amp;gt;%
  filter(days_of_symptoms &amp;lt;= 7) %&amp;gt;%
  summarize(label = &amp;quot;&amp;lt; 7 days&amp;quot;,
            sensitivity = mean(mm_value), 
            N = n())

bind_rows(res, res2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 3
##   label    sensitivity     N
##   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;
## 1 &amp;lt; 3 days       0.857    49
## 2 &amp;lt; 7 days       0.85    100&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The sensitivity in both subgroups is increased to 85.7 % and 85 %.
Now only 1 in 7 cases is missed by the antigen test.
This sensitivity is now very close to that in the leaflet. The dataset in the leaflet has N = 102, whereas here we have N = 100.
Given that the difference is very small, I decided to not look into this any further.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;is-there-a-swab-effect&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Is there a swab effect?&lt;/h1&gt;
&lt;p&gt;Ok, so the rapid antigen test is less sensitive than PCR.
What about the effect of using self-administered nasal swabs, versus professional health workers taking a nasopharyngeal swab (and often a swab in the back of the throat as well)?&lt;/p&gt;
&lt;p&gt;Interestingly, the three Berlin studies all contain a head-to-head comparison of &lt;strong&gt;nasal&lt;/strong&gt; versus &lt;strong&gt;nasopharygeal (NP)&lt;/strong&gt; swabs. Lets have a look, shall we?&lt;/p&gt;
&lt;p&gt;The dataset &lt;code&gt;df_pcr_pos_np&lt;/code&gt; is identical to &lt;code&gt;df_pcr_pos_np&lt;/code&gt;, but contains the measurement results for the &lt;strong&gt;nasopharygeal&lt;/strong&gt; swabs.&lt;/p&gt;
&lt;p&gt;To compare both measurement methods, we can plot the relationship between the probability of obtaining a positive result versus viral load. If one method gathers systematically more viral load from the patient, we expect that method to detect infection at lower patient viral loads, and the curves (nasal vs NP) would be shifted relative to each other.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)

ggplot(df_pcr_pos_np , aes(x = viral_load, y = mm_value)) + 
  geom_jitter(data = df_pcr_pos, height = 0.05, col = &amp;quot;blue&amp;quot;) +
  geom_jitter(height = 0.05, col = &amp;quot;orange&amp;quot;) +
  geom_smooth(data = df_pcr_pos , col = &amp;quot;blue&amp;quot;) + 
  geom_smooth(col = &amp;quot;orange&amp;quot;) +
  ggtitle(&amp;quot;nasal (blue) versus nasopharyngeal (orange) swabs&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021_05_14_covid_rapid_test_reliability_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;By fitting a smoother through the binary data, we obtain an estimate of the relationship between the probability of obtaining a positive result, and the viral load of the patient as measured by PCR on a combined NP/OP swab.&lt;/p&gt;
&lt;p&gt;From this plot, I conclude that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The sensitivity of a test is strongly dependent on the distribution of viral loads in the population the measurement was conducted in&lt;/li&gt;
&lt;li&gt;There is no evidence for any differences in sensitivity between nasal and nasopharyngeal swabs&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This last conclusion came as a surprise for me, as nasopharygeal swabs are long considered to be the golden standard for obtaining samples for PCR detection of respiratory viruses, such as influenza and COVID-19 (Seaman &lt;em&gt;et al.&lt;/em&gt; (2019), (Lee &lt;span class=&#34;math inline&#34;&gt;\(et al.\)&lt;/span&gt; 2021) ). So let’s look a bit deeper still.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;double-check-rotterdam-vs-berlin&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Double-check: Rotterdam vs Berlin&lt;/h1&gt;
&lt;p&gt;We can compare the results from the three Berlin studies with a recent Dutch study that also used the Roche antigen test (ref: Igloi et al 2021). The study was conducted in Rotterdam, and used nasopharygeal swabs to obtain the sample for the antigen test.&lt;/p&gt;
&lt;p&gt;Cool! Lets try and create two comparable groups in both studies so we can compare the sensitivity.&lt;/p&gt;
&lt;p&gt;The Igloi et al. paper reports results for a particular subset that we can also create in the Berlin dataset.
They report that for the subset of samples with high viral load (viral load &lt;span class=&#34;math inline&#34;&gt;\(2.17 \cdot 10^5\)&lt;/span&gt; particles / ml = 5.35 on the log10 scale, ct_value &amp;lt;= 30) &lt;strong&gt;AND&lt;/strong&gt; who presented within 7 days of symptom onset, they found a sensitivity of 95.8% (CI95% 90.5-98.2). The percentage is based on N = 640 persons.&lt;/p&gt;
&lt;p&gt;We can check what the sensitivity is for this subgroup in the Berlin dataset:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_pcr_pos %&amp;gt;% filter(viral_load &amp;gt;= 5.35 &amp;amp; days_of_symptoms &amp;lt;= 7) %&amp;gt;%
  summarize(sensitivity = mean(mm_value), 
            N = n())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 2
##   sensitivity     N
##         &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;
## 1       0.898    88&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the same subgroup of high viral load, sensitivity of the nasal swab test is 6% lower than the nasopharyngeal swab test, across the two studies. But how do we have to weigh this evidence? N = 88 is not so much data, and the studies are not identical in design.&lt;/p&gt;
&lt;p&gt;Importantly, since the threshold to be included in this comparison (ct value &amp;lt;= 30, viral_load &amp;gt; 5.35) contains a large part of the region where the probability of a positive result is between 0 and 1, we need to compare the distributions of viral loads for both studies to make an apples to apples comparison.&lt;/p&gt;
&lt;p&gt;The Igloi study reports their distribution of viral loads for PCR-positive samples (N=186) in five bins:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat &amp;lt;- c(&amp;quot;ct &amp;lt;= 20&amp;quot;, &amp;quot;ct 20-25&amp;quot;, &amp;quot;ct 25-30&amp;quot;, &amp;quot;ct 30-35&amp;quot;, &amp;quot;ct 35+&amp;quot;)
counts &amp;lt;- c(31, 82, 46, 27, 1)

ggplot(data.frame(cat, counts), aes(x = cat, y = counts)) +
  geom_col()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021_05_14_covid_rapid_test_reliability_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;
Lets create those same bins in the Berlin dataset as well:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_pcr_pos$ct_bin &amp;lt;- cut(df_pcr_pos$ct_value, breaks = c(-Inf,20,25,30,35,Inf))

ggplot(df_pcr_pos, aes(x = ct_bin)) +
  geom_histogram(stat = &amp;quot;count&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021_05_14_covid_rapid_test_reliability_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;
For the subset where we can compare the sensitivities (ct_value &amp;lt;= 30), the Berlin clinical population has a higher viral load than the Rotterdam clinical population! So that does not explain why the Rotterdam study reports a higher sensitivity.&lt;/p&gt;
&lt;p&gt;I use simulation to create distributions of plausible values for the sensitivity, assuming the observed values in both studies (89.7% for the Berlin studies, and 95.8% for the Rotterdam study) to be the true data generating values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)

# Berlin
sample_size = 88
prior_probability = 0.898

est_p &amp;lt;- rbinom(10000, sample_size, p=prior_probability)/sample_size

# Rotterdam
sample_size2 = 159 # derived from Table 1 (Ct value distribution of PCR+ samples, &amp;lt;= 30)
prior_probability2 = 0.958

est_p2 &amp;lt;- rbinom(10000, sample_size2, p=prior_probability2)/sample_size2

ggplot(data.frame(est_p), aes(x = est_p)) +
  geom_histogram(binwidth = 0.005) +
    geom_histogram(data = data.frame(est_p = est_p2), fill = &amp;quot;gray60&amp;quot;, alpha = 0.5, binwidth = 0.005) +
  geom_vline(xintercept = prior_probability, linetype = &amp;quot;dashed&amp;quot;, col= &amp;quot;red&amp;quot;) +
geom_vline(xintercept = prior_probability2, linetype = &amp;quot;dashed&amp;quot;, col= &amp;quot;blue&amp;quot;) +
  ggtitle(&amp;quot;Berlin (black bars) vs Rotterdam (grey bars) sensitivity for higher viral loads&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021_05_14_covid_rapid_test_reliability_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;
There is a region of overlap between the two distributions, so the difference between the studies could be (in part) attributed to statistical sampling variation for the same underlying process.&lt;/p&gt;
&lt;p&gt;I conclude that the Berlin study, who does a head to head comparison of NP versus nasal swabs, finds them to be highly comparable, and reports sensitivities that are close to those reported by the Rotterdam study.&lt;/p&gt;
&lt;p&gt;Surprisingly, nasal swabs appear to give results that are comparable to those of nasopharyngeal swabs, while having not having the disadvantages of them (unpleasant, can only be performed by professional health worker).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;that-other-metric-the-specificity&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;That other metric: the specificity&lt;/h1&gt;
&lt;p&gt;So far, the discussion centered around the &lt;strong&gt;sensitivity&lt;/strong&gt; of the test.
Equally important is the &lt;strong&gt;specificity&lt;/strong&gt; of the test. This quantifies if the test result of the antigen test is specific for COVID-19. It would be bad if the test would also show a result for other viruses, or even unrelated molecules.&lt;/p&gt;
&lt;p&gt;To examine this, we use the aggregated data supplied on the leaflet from the kit, &lt;code&gt;df_leaflet&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;N.b.&lt;/strong&gt; The aggregated data is a subset of all the data from the three studies, because the data was subsetted for cases with &lt;code&gt;&amp;lt;= 7 days_of_symptoms&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This dataset contains for each sample one of four possibilities:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Both tests are negative,&lt;/li&gt;
&lt;li&gt;both tests are positive,&lt;/li&gt;
&lt;li&gt;the PCR test is positive but the antigen test negative,&lt;/li&gt;
&lt;li&gt;the PCR test is negative but the antigen positive.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We use the &lt;code&gt;yardstick&lt;/code&gt; package of R’s &lt;code&gt;tidymodels&lt;/code&gt; family to create the 2x2 table and analyze the specificity.&lt;/p&gt;
&lt;p&gt;(&lt;strong&gt;Overthinking&lt;/strong&gt;: Note that the &lt;code&gt;yardstick&lt;/code&gt; package is used to quantify the performance of statistical prediction models by comparing the model predictions to the true values contained in the training data. This provides us with an analogy where the antigen test can be viewed as a model that is trying the predict the outcome of the PCR test.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;options(yardstick.event_first = FALSE)

conf_matrix &amp;lt;- yardstick::conf_mat(df_leaflet, pcr_result, ag_result)

autoplot(conf_matrix, 
         type = &amp;quot;heatmap&amp;quot;, 
         title = &amp;quot;Truth = PCR test, Prediction = Antigen test&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021_05_14_covid_rapid_test_reliability_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;288&#34; /&gt;
From the heatmap (confusingly called a confusion matrix among ML practioners), we see that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For most samples (N = 431), both tests are COVID-19 negative.&lt;/li&gt;
&lt;li&gt;85 + 17 = 102 samples tested COVID-19 positive using the PCR-test&lt;/li&gt;
&lt;li&gt;85 out of 102 samples that are PCR positive, are antigen test positive as well&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For the specificity, we have to look at the samples where the PCR test is negative, but the antigen test is positive, and compare these to all the samples that are PCR-test negative. These are the number of tests where the antigen test picked up a non-specific signal. One minus this percentage gives the specificity (1 - 4/435 = 431/435):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;yardstick::spec(df_leaflet, pcr_result, ag_result)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
## 1 spec    binary         0.991&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thus, we find that the antigen test is highly specific, with around 1% of false positives.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;uncertainty-in-the-estimated-specificity-and-sensitivity&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Uncertainty in the estimated specificity and sensitivity&lt;/h1&gt;
&lt;p&gt;So far, we did not discuss the sampling variability in the estimated specificity and sensitivity.&lt;/p&gt;
&lt;p&gt;The kit leaflet mentions the following confidence intervals:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sensitivity 83.3% (95%CI: 74.7% - 90.0%)&lt;/li&gt;
&lt;li&gt;Specificity 99.1% (95%CI: 97.7% - 99.7%)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The R-package &lt;code&gt;yardstick&lt;/code&gt; does not yet include confidence intervals, so I generated these using bootstrapping. I calculate both metrics for 10.000 samples sampled from the raw data. For brevity I omitted the code here, go check out my &lt;a href=&#34;https://github.com/gsverhoeven/hugo_source/tree/master/content/post/sars_test&#34;&gt;Github&lt;/a&gt; for the R script.&lt;/p&gt;
&lt;p&gt;The bootstrapping approach yields the following range of plausible values given the data (95% interval):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quantile(spec_vec, probs = c(0.025, 0.975))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      2.5%     97.5% 
## 0.9811765 0.9977477&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quantile(sens_vec, probs = c(0.025, 0.975))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      2.5%     97.5% 
## 0.7570030 0.9029126&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The amount of data (N = 537) prevents us from getting an exact match to the leaflet’s confidence intervals, that are based on theoretic formulas. But we do get pretty close.&lt;/p&gt;
&lt;p&gt;Especially for the sensitivity, there is quite some uncertainty, we see that plausible values range from 76% up to 90% &lt;em&gt;for this particular cohort of cases with this particular mix of viral loads that showed up during the last four months of 2020 in the University hospital in Berlin&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusions&lt;/h1&gt;
&lt;p&gt;To summarise: we found that the numbers of the kit’s leaflet are reliable, reproducible, and published in full detail in the scientific literature.
Hurray!&lt;/p&gt;
&lt;p&gt;We also found that even the gold standard PCR is not able to detect all infected persons, it all depends on how much virus is present, and how the sample is obtained.&lt;/p&gt;
&lt;p&gt;But all in all, the PCR test is clearly more accurate. Why would we want to use an antigen test then?
To do the PCR test you need a lab with skilled people, equipment such as PCR devices and pipets, and time, as the process takes at least a few hours to complete. The advantage of an antigen test is to have a low-tech, faster alternative that can be self-administered. But that comes at a cost, because the antigen tests are less sensitive.&lt;/p&gt;
&lt;p&gt;From the analysis, it is clear that the rapid Antigen tests need more virus present to reliably detect infection. It is ALSO clear that the test is highly specific, with less than 1% false positives. Note that a false positive rate of 1% still means that in a healthy population of 1000, 10 are falsely detected as having COVID-19.&lt;/p&gt;
&lt;p&gt;Surprisingly, nasal swabs appear to give results that are comparable to those of nasopharyngeal swabs, while having not having the disadvantages of them (unpleasant, can only be performed by professional health worker).&lt;/p&gt;
&lt;p&gt;So the antigen tests are less sensitive than PCR tests. But now comes the key insight: the persons that produce the largest amounts of virus get detected, irrespective of whether they have symptoms or not. To me, this seems like a “Unique Selling Point” of the rapid tests: the ability to rapidly detect the most contagious persons in a group, after which these persons can go into quarantine and help reduce spread.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Thanks to dr. Andreas Lindner for providing helpful feedback and pointing out flaws in my original blog post. This should not be seen as an endorsement of the conclusions of this post, and any remaining mistakes are all my own!&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Head-to-head comparison of SARS-CoV-2 antigen-detecting rapid test with self-collected nasal swab versus professional-collected nasopharyngeal swab&lt;/em&gt;:
Andreas K. Lindner, Olga Nikolai, Franka Kausch, Mia Wintel, Franziska Hommes, Maximilian Gertler, Lisa J. Krüger, Mary Gaeddert, Frank Tobian, Federica Lainati, Lisa Köppel, Joachim Seybold, Victor M. Corman, Christian Drosten, Jörg Hofmann, Jilian A. Sacks, Frank P. Mockenhaupt, Claudia M. Denkinger, &lt;a href=&#34;https://erj.ersjournals.com/content/57/4/2003961&#34;&gt;European Respiratory Journal 2021 57: 2003961&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Head-to-head comparison of SARS-CoV-2 antigen-detecting rapid test with professional-collected nasal versus nasopharyngeal swab&lt;/em&gt;:
Andreas K. Lindner, Olga Nikolai, Chiara Rohardt, Susen Burock, Claudia Hülso, Alisa Bölke, Maximilian Gertler, Lisa J. Krüger, Mary Gaeddert, Frank Tobian, Federica Lainati, Joachim Seybold, Terry C. Jones, Jörg Hofmann, Jilian A. Sacks, Frank P. Mockenhaupt, Claudia M. Denkinger &lt;a href=&#34;https://erj.ersjournals.com/content/57/5/2004430&#34;&gt;European Respiratory Journal 2021 57: 2004430&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;SARS-CoV-2 patient self-testing with an antigen-detecting rapid test: a head-to-head comparison with professional testing&lt;/em&gt;:
Andreas K. Lindner, Olga Nikolai, Chiara Rohardt, Franka Kausch, Mia Wintel, Maximilian Gertler, Susen Burock, Merle Hörig, Julian Bernhard, Frank Tobian, Mary Gaeddert, Federica Lainati, Victor M. Corman, Terry C. Jones, Jilian A. Sacks, Joachim Seybold, Claudia M. Denkinger, Frank P. Mockenhaupt, under review, &lt;a href=&#34;https://www.medrxiv.org/content/10.1101/2021.01.06.20249009v1&#34;&gt;preprint on medrxiv.org&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Variation in False-Negative Rate of Reverse Transcriptase Polymerase Chain Reaction–Based SARS-CoV-2 Tests by Time Since Exposure&lt;/em&gt;: Lauren M. Kucirka, Stephen A. Lauer, Oliver Laeyendecker, Denali Boon, Justin Lessler &lt;a href=&#34;https://www.acpjournals.org/doi/full/10.7326/M20-1495&#34;&gt;Link to paper&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Clinical evaluation of the Roche/SD Biosensor rapid antigen test with symptomatic, non-hospitalized patients in a municipal health service drive-through testing site&lt;/em&gt;:
Zsὁfia Iglὁi, Jans Velzing, Janko van Beek, David van de Vijver, Georgina Aron, Roel Ensing, KimberleyBenschop, Wanda Han, Timo Boelsums, Marion Koopmans, Corine Geurtsvankessel, Richard Molenkamp &lt;a href=&#34;https://www.medrxiv.org/content/10.1101/2020.11.18.20234104v1&#34;&gt;Link on medrxiv.org&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Performance of Saliva, Oropharyngeal Swabs, and Nasal Swabs for SARS-CoV-2 Molecular Detection: a Systematic Review and Meta-analysis&lt;/em&gt;:
Rose A. Lee, Joshua C. Herigona, Andrea Benedetti, Nira R. Pollock, Claudia M. Denkinger &lt;a href=&#34;https://journals.asm.org/doi/10.1128/JCM.02881-20&#34;&gt;Link to paper&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Self-collected compared with professional-collected swabbing in the diagnosis of influenza in symptomatic individuals: A meta-analysis and assessment of validity&lt;/em&gt;:
Seaman et al 2019 &lt;a href=&#34;https://pubmed.ncbi.nlm.nih.gov/31400670/&#34;&gt;Link to paper&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Writing scientific papers using Rstudio and Zotero</title>
      <link>/post/zotero-rmarkdown-csl/</link>
      <pubDate>Sun, 02 May 2021 00:00:00 +0000</pubDate>
      <guid>/post/zotero-rmarkdown-csl/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;em&gt;(This blog post is mainly for my future self, and for people that ask me about my workflow for scientific papers. Please contact me if you spot ways to improve this!)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This blog post describes a sequence of 9 steps to set up a reproducible workflow for scientific writing with a focus on getting the journal citation hell right. It boils down to writing the manuscript in Rmarkdown, and using a set of auxiliary tools to manage citations and output to Word to share with collaborators and to prepare the final document for submission to the journal.&lt;/p&gt;
&lt;div id=&#34;step-1-choose-the-target-journal&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 1: Choose the target journal&lt;/h2&gt;
&lt;p&gt;First step is to check what the constraints of the journal are where we want to submit.
Do they have a particular word count? Do they have a particular format for the abstract? etc.
Go to the target journal, and download the author instructions.
Because author instructions are typically &lt;strong&gt;REALLY&lt;/strong&gt; boring to read, a quick visual way to find out what is needed is to download a few &lt;strong&gt;recent&lt;/strong&gt; (because change is the only constant), &lt;strong&gt;open access&lt;/strong&gt; (no fuss with paywalls etc), and &lt;strong&gt;highly cited&lt;/strong&gt; (this must mean they did something right, right?) example papers.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-2-install-reference-manager-zotero&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 2: Install reference manager Zotero&lt;/h2&gt;
&lt;p&gt;Zotero is THE open source reference manager.
Source code on Github, check! Cross-platform, check! 4K stars on Github, check!
Another cool feature is that your references library is stored online, and your local Zotero install synchronizes with it, so you no longer need to move around library files between work and home, laptop and desktop.&lt;/p&gt;
&lt;p&gt;Go to &lt;a href=&#34;https://zotero.org&#34;&gt;the Zotero website&lt;/a&gt; to download, install and configure Zotero.
(This includes creating a Zotero account for online storage of your references)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-3-save-citations-to-zotero-from-your-browser&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 3: Save citations to Zotero from your browser&lt;/h2&gt;
&lt;p&gt;The easiest way to fill your Zotero library is to use a browser plugin.
I use Firefox on Linux, so I installed the &lt;a href=&#34;https://www.zotero.org/download/&#34;&gt;Zotero Firefox connector&lt;/a&gt;.
Once this is installed, I use Google scholar to look up papers I want to cite.
To add a paper to the Zotero library, make sure to have Zotero open, then click on a paper in Google Scholar to go to that paper’s web site (Typically the Publishers website for the journal).
Finally, click the icon in the browsers top right top corner (“Save to zotero”).
Repeat ad nauseam.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-4-create-stable-citation-keys&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 4: Create stable citation keys&lt;/h2&gt;
&lt;p&gt;While writing our papers, we want short but recognizable identifier keys for our citations.
For this we use the “Better BibTex for Zotero” Add-on.
Go to &lt;a href=&#34;https://retorque.re/zotero-better-bibtex/installation/&#34;&gt;the Better Bibtex website&lt;/a&gt; and follow the installation instructions.&lt;/p&gt;
&lt;p&gt;In the Zotero preferences (Better bibtex tab), I changed the “Citation Key Format” to create keys like &lt;code&gt;verhoeven_etal20&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[auth.etal:lower:replace=.,_][&amp;gt;0][shortyear]|[veryshorttitle][shortyear]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(This snippet comes from &lt;a href=&#34;https://fishandwhistle.net/post/2020/getting-started-zotero-better-bibtex-rmarkdown/&#34;&gt;Dewey Dunnington’s blog&lt;/a&gt; that was a big help in getting my workflow up and running)&lt;/p&gt;
&lt;p&gt;This should automatically create / update all the citations keys in Zotero.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-5-rstudio-rmarkdown-and-all-that&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 5: Rstudio, Rmarkdown and all that&lt;/h2&gt;
&lt;p&gt;Rstudio is where we actually write the paper.
We use the &lt;code&gt;.Rmd&lt;/code&gt; Rmarkdown format.
This format consists of a YAML header, followed by a body that consists of Markdown formatted text with optional code chunks, figures and tables interspersed.
Version control is through &lt;a href=&#34;https://git-scm.com/book/en/v2/Getting-Started-About-Version-Control&#34;&gt;Git&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Important thing to check: make sure that your &lt;code&gt;.Rmd&lt;/code&gt; file uses UTF-8 encoding.
In Rstudio, &lt;code&gt;File --&amp;gt; Save with Encoding --&amp;gt; UTF-8&lt;/code&gt; can set this straight if somehow you ended up with a file in the wrong encoding.&lt;/p&gt;
&lt;p&gt;Check the &lt;a href=&#34;https://rmarkdown.rstudio.com/authoring_quick_tour.html&#34;&gt;Rstudio website&lt;/a&gt; for more info on Rmarkdown.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; The latest version of Rstudio (1.4) contains a new editing mode for Rmarkdown, “the visual markdown editor”, that contains support for inserting citations from Zotero. I am not sure yet whether I like this, and noted that on my system, it was still buggy, and the editor, when invoked, makes &lt;strong&gt;CHANGES&lt;/strong&gt; to the markdown code. Brrr. Therefore, this blog post does not make use of this new feature.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-6-connecting-rstudio-to-zotero&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 6: Connecting Rstudio to Zotero&lt;/h2&gt;
&lt;p&gt;Hold on, almost there. We’re in Rstudio, and writing our paper.
Now comes the moment where we want to cite something!
Now we need a connection to Zotero. There are two Rstudio Addins that compete for this functionality, &lt;code&gt;citr&lt;/code&gt; and &lt;code&gt;rbbt&lt;/code&gt;.
Both packages are not on CRAN and therefore need to be installed from Github.
I tried them both out, and went for &lt;code&gt;rbbt&lt;/code&gt; as &lt;code&gt;citr&lt;/code&gt; does not support CSL-JSON and &lt;code&gt;rbbt&lt;/code&gt; appears slightly leaner.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;remotes::install_github(&amp;quot;paleolimbot/rbbt&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After installing and restarting Rstdio, the &lt;code&gt;rbbt&lt;/code&gt; addin can be found under &lt;code&gt;Addins&lt;/code&gt;.
Now since citing stuff is a common activity while writing a paper, we want a keyboard shortcut for this.
I put it under &lt;code&gt;CTRL + K&lt;/code&gt; where K stand for errr, Knowledge ?&lt;/p&gt;
&lt;p&gt;To bind &lt;code&gt;rbbt&lt;/code&gt; to a particular keyboard shortcut, do the following:
First, in RStudio, choose &lt;code&gt;Tools --&amp;gt; Modify Keyboard Shortcuts&lt;/code&gt;.
Type &lt;code&gt;zotero&lt;/code&gt; to filter out the Zotero plugin.
Click on the ‘Shortcut’ field for the ‘Insert Zotero citation’ addin row, and type the desired shortcut keys.&lt;/p&gt;
&lt;p&gt;In Rstudio, we can now press CTRL+K, type the name of the first author, select the citation, press enter, and have the citation key added to our &lt;code&gt;.Rmd&lt;/code&gt; document.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-7-creating-the-bibliography&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 7: Creating the bibliography&lt;/h2&gt;
&lt;p&gt;Now that we have an Rmarkdown document filled with citation keys that references citations in Zotero, we still need one more thing, and that is to create the actual &lt;code&gt;.bib&lt;/code&gt; or &lt;code&gt;.json&lt;/code&gt; file containing the cited references.&lt;/p&gt;
&lt;p&gt;Here I describe the simplest approach.
We go to Zotero and export all the references using “Export Collection” , and choosing &lt;code&gt;CSL JSON&lt;/code&gt;, save the file as &lt;code&gt;references.json&lt;/code&gt; in the same folder as your Rmd paper.
CSL-JSON is an emerging standard that is recommended by Yihui Xie, author of Rmarkdown.&lt;/p&gt;
&lt;p&gt;In Rstudio, we add to following line to our YAML header:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;bibliography: references.json&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;rbbt&lt;/code&gt; has the functionality to automatically create a bib/json file that ONLY contains the references that are cited in the Rmd document. I haven’t tested this yet, but this would be the icing on the cake. Instructions for this are on the &lt;a href=&#34;https://github.com/paleolimbot/rbbt&#34;&gt;rbbt Github page&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-8-getting-the-references-in-the-proper-format&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 8: Getting the references in the proper format&lt;/h2&gt;
&lt;p&gt;At this point, we can &lt;code&gt;knit&lt;/code&gt; our Rmarkdown document, and it will contain the cited references appended at the end of the HTML/PDF/Word generated output document.&lt;/p&gt;
&lt;p&gt;However, the references are (most likely) not yet properly formatted for the journal we want to send it to.
For example, the journal “Health Services Research” wants the references in the main text to be numbered, and the reference list sorted in the order of appearance, and formatted according the APA format (whatever that is).&lt;/p&gt;
&lt;p&gt;Luckily for us, enter the &lt;a href=&#34;https://citationstyles.org/&#34;&gt;Citation Style Language project&lt;/a&gt;.
They created a common standard, CSL, and a crowdsourced repository, that contains more than 10.000 free citation styles.
All we need to do is grab the CSL file for our target journal!&lt;/p&gt;
&lt;p&gt;Go to the &lt;a href=&#34;https://www.zotero.org/styles&#34;&gt;Zotero Style Repository&lt;/a&gt; , search for the target journal name (in my case Health Services Research) and click on the result. This downloads a CSL file that we add to our Git repo containing the manuscript.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-9-make-rstudio-output-to-word-for-our-collaborators&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 9: Make Rstudio output to Word for our collaborators&lt;/h2&gt;
&lt;p&gt;Still here? Great. Now we are ready for the final step.
This one is for our collaborators (who we feel sorry for, because they use Word and miss out on all the Rmarkdown fun), and for those journals, that force us to submit our manuscript as a Word file.&lt;/p&gt;
&lt;p&gt;In Rstudio, we add the following code to the YAML header:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;output:
  word_document:
    reference_docx: style_template.docx&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This tells Rmarkdown to use the Word formatting styles contained in the &lt;code&gt;style_template.docx&lt;/code&gt; file.
For me, this contains at the moment three things: A4 page size, double line spacing, and numbered lines.&lt;/p&gt;
&lt;p&gt;Follow the instructions by &lt;a href=&#34;https://rmarkdown.rstudio.com/articles_docx.html&#34;&gt;Rstudio&lt;/a&gt; to make this template.
In short, you let Rstudio’s pandoc generate a Word document from a &lt;code&gt;.Rmd&lt;/code&gt; file, and tweak the formatting styles of that document. Name the document &lt;code&gt;style_template.docx&lt;/code&gt; and keep it with your &lt;code&gt;.Rmd&lt;/code&gt; manuscript.
I can confirm that this also works when you edit this document using LibreOffice / OpenOffice.&lt;/p&gt;
&lt;p&gt;The great thing for me: now I have this blog post, I can forget about all this stuff, and finally get to the scientific writing part!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>OpenAI Gym&#39;s FrozenLake: Converging on the true Q-values</title>
      <link>/post/frozenlake-qlearning-convergence/</link>
      <pubDate>Sun, 07 Mar 2021 00:00:00 +0000</pubDate>
      <guid>/post/frozenlake-qlearning-convergence/</guid>
      <description>


&lt;p&gt;(Photo by Ryan Fishel on Unsplash)&lt;/p&gt;
&lt;p&gt;This blog post concerns a famous “toy” problem in Reinforcement Learning, the &lt;a href=&#34;https://gym.openai.com/envs/FrozenLake-v0/&#34;&gt;FrozenLake environment&lt;/a&gt;. We compare solving an environment with RL by reaching &lt;strong&gt;maximum performance&lt;/strong&gt; versus obtaining the &lt;strong&gt;true state-action values&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(Q_{s,a}\)&lt;/span&gt;. In doing so I learned a lot about RL as well as about Python (such as the existence of a &lt;code&gt;ggplot&lt;/code&gt; clone for Python, &lt;code&gt;plotnine&lt;/code&gt;, see this blog post for some cool examples).&lt;/p&gt;
&lt;p&gt;FrozenLake was created by OpenAI in 2016 as part of their Gym python package for Reinforcement Learning. Nowadays, the interwebs is full of tutorials how to “solve” FrozenLake. Most of them focus on performance in terms of episodic reward. As soon as this maxes out the algorithm is often said to have converged. For example, in this &lt;a href=&#34;https://stats.stackexchange.com/questions/206944/how-do-i-know-when-a-q-learning-algorithm-converges&#34;&gt;question on Cross-Validated&lt;/a&gt; about Convergence and Q-learning:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;In practice, a reinforcement learning algorithm is considered to converge when the learning curve gets flat and no longer increases.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Now, for &lt;strong&gt;Q-learning&lt;/strong&gt; it has been proven that, &lt;em&gt;under certain conditions&lt;/em&gt;, the Q-values convergence to their true values. But does this happens when the performance maxes out? In this blog we’ll see that this is not generally the case.&lt;/p&gt;
&lt;p&gt;We start with obtaining the true, exact state-action values. For this we use Dynamic Programming (DP). Having implemented Dynamic Programming (DP) for the FrozenLake environment as an exercise notebook already (created by Udacity, go check them out), this was a convenient starting point.&lt;/p&gt;
&lt;div id=&#34;loading-the-packages-and-modules&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Loading the packages and modules&lt;/h2&gt;
&lt;p&gt;We need various Python packages and modules.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# data science packages
import numpy as np
import pandas as pd
import plotnine as p9
import matplotlib.pyplot as plt
%matplotlib inline

# RL algorithms
from qlearning import *
from dp import *

# utils
from helpers import *
from plot_utils import plot_values
import copy
import dill

from frozenlake import FrozenLakeEnv

env = FrozenLakeEnv(is_slippery = True)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;frozen-lake-environment-description&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Frozen Lake Environment description&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake (G).
The water is mostly frozen (F), but there are a few holes (H) where the ice has melted.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-03-07-qlearning_frozenlake_convergence_files/Frozen-Lake.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;If you step into one of those holes, you’ll fall into the freezing water. At this time, there’s an international frisbee shortage, so it’s absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won’t always move in the direction you intend.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The episode ends when you reach the goal or fall in a hole. You receive a reward of 1 if you reach the goal, and zero otherwise.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The agent moves through a &lt;span class=&#34;math inline&#34;&gt;\(4 \times 4\)&lt;/span&gt; gridworld, with states numbered as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[[ 0  1  2  3]
 [ 4  5  6  7]
 [ 8  9 10 11]
 [12 13 14 15]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and the agent has 4 potential actions:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;LEFT = 0
DOWN = 1
RIGHT = 2
UP = 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The FrozenLake Gym environment has been amended to make the one-step dynamics accessible to the agent. For example, if we are in State &lt;code&gt;s = 1&lt;/code&gt; and we perform action &lt;code&gt;a = 0&lt;/code&gt;, the probabilities of ending up in new states, including the associated rewards are contained in the &lt;span class=&#34;math inline&#34;&gt;\(P_{s,a}\)&lt;/span&gt; array:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;env.P[1][0]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[(0.3333333333333333, 1, 0.0, False),
 (0.3333333333333333, 0, 0.0, False),
 (0.3333333333333333, 5, 0.0, True)]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The FrozenLake environment is highly stochastic, with a very sparse reward: only when the agent reaches the goal, a reward of &lt;code&gt;+1&lt;/code&gt; is obtained. This means that if we do not set a discount rate, the agent can keep on wandering around without receiving a learning “signal” that can be propagated back through the preceding state-actions (since falling into the holes does not result in a negative reward) and thus learning very slowly. We will focus on the discounting case (&lt;code&gt;gamma = 0.95&lt;/code&gt;) for this reason (less computation needed for convergence), but compare also with the undiscounted case.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;solving-frozen-lake-using-dp&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Solving Frozen Lake using DP&lt;/h2&gt;
&lt;p&gt;Let us solve FrozenLake first for the no discounting case (&lt;code&gt;gamma = 1&lt;/code&gt;).
The Q-value for the first state will then tell us the average episodic reward, which for FrozenLake translates into the percentage of episodes in which the Agent succesfully reaches its goal.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;policy_pi, V_pi = policy_iteration(env, gamma = 1, theta=1e-9, \
                                   verbose = False)


plot_values(V_pi)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_9_1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The Q-value for state &lt;code&gt;s = 0&lt;/code&gt; is &lt;code&gt;0.824&lt;/code&gt;. This means that for &lt;code&gt;gamma = 1&lt;/code&gt; and following an optimal policy &lt;span class=&#34;math inline&#34;&gt;\(\pi^*\)&lt;/span&gt;, 82.4% of all episodes ends in succes.&lt;/p&gt;
&lt;p&gt;As already mentioned above, for computational reasons we will apply Q-learning to the environment with &lt;code&gt;gamma = 0.95&lt;/code&gt;. So… lets solve FrozenLake for &lt;code&gt;gamma = 0.95&lt;/code&gt; as well:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# obtain the optimal policy and optimal state-value function
policy_pi_dc, V_pi_dc = policy_iteration(env, gamma = 0.95, theta=1e-9, \
                                   verbose = False)

Q_perfect = create_q_dict_from_v(env, V_pi_dc, gamma = 0.95)

df_true = convert_vals_to_pd(V_pi_dc)

plot_values(V_pi_dc)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_11_1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now, comparing the optimal policies for &lt;code&gt;gamma = 0.95&lt;/code&gt; and for &lt;code&gt;gamma = 1&lt;/code&gt; (not shown here) we find that they are not the same. Therefore, 82.4% succes rate is likely an upper bound for &lt;code&gt;gamma = 0.95&lt;/code&gt;, since introducing discounting in this stochastic environment can (intuitively) either have no effect on the optimal policy, or favor more risky (lower succes rate) but faster (less discounting) policies, leading to a lower overall succes rate.&lt;/p&gt;
&lt;p&gt;For example, for the undiscounted case, the Agent is indifferent in choosing direction in the first state. If it ends up going &lt;strong&gt;right&lt;/strong&gt; , it can choose UP and wander around at the top row at no cost until it reaches the starting state again. As soon as this wandering around gets a cost by discounting we see (not shown) that the Agent is no longer indifferent, and does NOT want to end up wandering in the top row, but chooses to play LEFT in state &lt;code&gt;s = 0&lt;/code&gt; instead.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;checking-the-performance-of-an-optimal-greedy-policy-based-on-perfect-q-values&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Checking the performance of an optimal greedy policy based on perfect Q-values&lt;/h1&gt;
&lt;p&gt;Now that we have the &lt;span class=&#34;math inline&#34;&gt;\(Q_{s,a}\)&lt;/span&gt; values corresponding to the optimal policy given that &lt;code&gt;gamma = 0.95&lt;/code&gt;, we can check its performance. To do so, we use brute force and simulate the average reward under the optimal policy for a large number of episodes.&lt;/p&gt;
&lt;p&gt;To do so, I wrote a function &lt;code&gt;test_performance()&lt;/code&gt; by taking the &lt;code&gt;q_learning()&lt;/code&gt; function, removing the learning (Q-updating) part and setting epsilon to zero when selecting an action (i.e. a pure greedy policy based on a given Q-table).&lt;/p&gt;
&lt;p&gt;Playing around with the binomial density in &lt;code&gt;R&lt;/code&gt; (&lt;code&gt;summary(rbinom(1e5, 1e5, prob = 0.8)/1e5&lt;/code&gt;) tells me that choosing 100.000 episodes gives a precision of around three decimals in estimating the probability of succes. This is good enough for me.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Obtain Q for Gamma = 0.95 and convert to defaultdict 
Q_perfect = create_q_dict_from_v(env, V_pi_dc, gamma = 0.95)

fullrun = 0

if fullrun == 1:
    d = []
    runs = 100
    for i in range(runs):
        avg_scores = test_performance(Q_perfect, env, num_episodes = 1000, \
                                   plot_every = 1000, verbose = False)
        d.append({&amp;#39;run&amp;#39;: i,
                 &amp;#39;avg_score&amp;#39;: np.mean(avg_scores)})
        print(&amp;quot;\ri {}/{}&amp;quot;.format(i, runs), end=&amp;quot;&amp;quot;)
        sys.stdout.flush() 

    d_perfect = pd.DataFrame(d)
    d_perfect.to_pickle(&amp;#39;cached/scores_perfect_0.95.pkl&amp;#39;)
else:
    d_perfect = pd.read_pickle(&amp;#39;cached/scores_perfect_0.95.pkl&amp;#39;)

&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;round(np.mean(d_perfect.avg_score), 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.781&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thus, we find that with the true optimal policy for &lt;code&gt;gamma = 0.95&lt;/code&gt;, in the long run 78% of all episodes is succesful. This is therefore the expected plateau value for the learning curve in Q-learning, provided that the exploration rate has become sufficiently small.&lt;/p&gt;
&lt;p&gt;Let’s move on to Q-learning and convergence.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;solving-frozenlake-using-q-learning&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;“Solving” FrozenLake using Q-learning&lt;/h1&gt;
&lt;p&gt;The typical RL tutorial approach to solve a simple MDP as FrozenLake is to choose a constant learning rate, not too high, not too low, say &lt;span class=&#34;math inline&#34;&gt;\(\alpha = 0.1\)&lt;/span&gt;. Then, the exploration parameter &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; starts at 1 and is gradually reduced to a floor value of say &lt;span class=&#34;math inline&#34;&gt;\(\epsilon = 0.0001\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Lets solve FrozenLake this way, monitoring the learning curve (average reward per episode) as it learns, and compare the learned Q-values with the true Q-values found using DP.&lt;/p&gt;
&lt;p&gt;I wrote Python functions that generate a &lt;strong&gt;decay schedule&lt;/strong&gt;, a 1D numpy array of &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; values, with length equal to the total number of episode the Q-learning algorithm is to run. This array is passed on to the Q-learning algorithm, and used during learning.&lt;/p&gt;
&lt;p&gt;It is helpful to visualize the decay schedule of &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; to check that it is reasonable before we start to use them with our Q-learning algorithm. I played around with the decay rate until the “elbow” of the curve is around 20% of the number of episodes, and reaches the desired minimal end value (&lt;span class=&#34;math inline&#34;&gt;\(\epsilon = 0.0001\)&lt;/span&gt;).&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def create_epsilon_schedule(num_episodes,  \
                       epsilon_start=1.0, epsilon_decay=.9999, epsilon_min=1e-4):
    x = np.arange(num_episodes)+1
    y = np.full(num_episodes, epsilon_start)
    y = np.maximum((epsilon_decay**x)*epsilon_start, epsilon_min)
    return y

epsilon_schedule = create_epsilon_schedule(100_000)

plot_schedule(epsilon_schedule, ylab = &amp;#39;Epsilon&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_20_0.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;My version of the Q-learning algorithm has slowly evolved to include more and more lists of things to monitor during the execution of the algorithm. Every 100 episodes, a copy of &lt;span class=&#34;math inline&#34;&gt;\(Q^{*}_{s}\)&lt;/span&gt; and of &lt;span class=&#34;math inline&#34;&gt;\(Q_{s, a}\)&lt;/span&gt; is stored in a list, as well as the average reward over this episode, as well as the final &lt;span class=&#34;math inline&#34;&gt;\(Q_{s,a}\)&lt;/span&gt; table, and &lt;span class=&#34;math inline&#34;&gt;\(N_{s,a}\)&lt;/span&gt; that kept track of how often each state-action was chosen. The &lt;code&gt;dill&lt;/code&gt; package is used to store these datastructures on disk to avoid the need to rerun the algorithm every time the notebook is run.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# K / N + K decay learning rate schedule
# fully random policy

plot_every = 100

n_episodes = len(epsilon_schedule)

fullrun = 0

random.seed(123)
np.random.seed(123)

if fullrun == 1:
    Q_sarsamax, N_sarsamax, avg_scores, Qlist, Qtable_list = q_learning(env, num_episodes = n_episodes, \
                                                eps_schedule = epsilon_schedule,\
                                                alpha = 0.1, \
                                                gamma =  0.95, \
                                                plot_every = plot_every, \
                                                verbose = True, log_full = True)
    
    with open(&amp;#39;cached/es1_Q_sarsamax.pkl&amp;#39;, &amp;#39;wb&amp;#39;) as f:
        dill.dump(Q_sarsamax, f)
    with open(&amp;#39;cached/es1_avg_scores.pkl&amp;#39;, &amp;#39;wb&amp;#39;) as f:
        dill.dump(avg_scores, f)
    with open(&amp;#39;cached/es1_Qlist.pkl&amp;#39;, &amp;#39;wb&amp;#39;) as f:
        dill.dump(Qlist, f)   
    with open(&amp;#39;cached/es1_Qtable_list.pkl&amp;#39;, &amp;#39;wb&amp;#39;) as f:
        dill.dump(Qtable_list, f)           
else:
    with open(&amp;#39;cached/es1_Q_sarsamax.pkl&amp;#39;, &amp;#39;rb&amp;#39;) as f:
        Q_sarsamax = dill.load(f)
    with open(&amp;#39;cached/es1_avg_scores.pkl&amp;#39;, &amp;#39;rb&amp;#39;) as f:
        avg_scores = dill.load(f)
    with open(&amp;#39;cached/es1_Qlist.pkl&amp;#39;, &amp;#39;rb&amp;#39;) as f:
        Qlist = dill.load(f)   
    with open(&amp;#39;cached/es1_Qtable_list.pkl&amp;#39;, &amp;#39;rb&amp;#39;) as f:
        Qtable_list = dill.load(f)            

Q_es1_lasttable = [np.max(Q_sarsamax[key]) if key in Q_sarsamax \
     else 0 for key in np.arange(env.nS)]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lets plot the learning curve. Here &lt;code&gt;plotnine&lt;/code&gt;, a &lt;code&gt;ggplot&lt;/code&gt; clone in Python comes in handy.&lt;/p&gt;
&lt;p&gt;As we can see below, the “recipe” for solving FrozenLake has worked really well. The displayed red line gives the theoretical optimal performance for &lt;code&gt;gamma = 0.95&lt;/code&gt;, I used a &lt;code&gt;loess&lt;/code&gt; smoother so we can more easily compare the Q-learning results with the theoretical optimum.&lt;/p&gt;
&lt;p&gt;We can see that the Q-learning algorithm has indeed found a policy that performs optimal. This appears to happen at around 60.000 episodes. We return to this later.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# plot performance
df_scores = pd.DataFrame(
    {&amp;#39;episode_nr&amp;#39;: np.linspace(0,n_episodes,len(avg_scores),endpoint=False),
     &amp;#39;avg_score&amp;#39;: np.asarray(avg_scores)})

(p9.ggplot(data = df_scores.loc[df_scores.episode_nr &amp;gt; -1], mapping = p9.aes(x = &amp;#39;episode_nr&amp;#39;, y = &amp;#39;avg_score&amp;#39;))
    + p9.geom_point(colour = &amp;#39;gray&amp;#39;)
    + p9.geom_smooth(method = &amp;#39;loess&amp;#39;)
    + p9.geom_hline(yintercept = 0.781, colour = &amp;quot;red&amp;quot;)
    + p9.geom_vline(xintercept = 60_000, color = &amp;quot;blue&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_24_0.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;But now comes the big question: did the &lt;span class=&#34;math inline&#34;&gt;\(Q^{*}_{a}\)&lt;/span&gt; estimates converge onto the true values as well?&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;q_showdown = pd.DataFrame(
    {&amp;#39;Q_es1_lasttable&amp;#39;: Q_es1_lasttable,
     &amp;#39;q_true&amp;#39;: V_pi_dc})
q_showdown[&amp;#39;state&amp;#39;] = range(16)

q_showdown = pd.melt(q_showdown, [&amp;#39;state&amp;#39;])

(p9.ggplot(data = q_showdown, mapping = p9.aes(x = &amp;#39;state&amp;#39;, y = &amp;#39;value&amp;#39;, color = &amp;#39;factor(variable)&amp;#39;))
+ p9.geom_point(size = 5, shape = &amp;#39;x&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_26_0.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Nope, they did not. Ok, most learned Q-values are close to their true values, but they clearly did not converge exactly to their true value.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;plotting-the-learning-curves-for-all-the-state-action-values&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Plotting the learning curves for all the state-action values&lt;/h1&gt;
&lt;p&gt;To really understand what is going on, I found it helpful to plot the learning curves for all the 16 x 4 = 64 state-action values at the same time. Here &lt;code&gt;plotnine&lt;/code&gt; really shines. I leave out the actual estimates, and only plot a &lt;code&gt;loess&lt;/code&gt; smoothed curve for each of the state-action values over time.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# convert to pandas df
dfm = list_of_tables_to_df(Qtable_list)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;#10 s
(p9.ggplot(data = dfm.loc[(dfm.episode &amp;gt; -1)], mapping = p9.aes(x = &amp;#39;episode&amp;#39;, y = &amp;#39;value&amp;#39;, group = &amp;#39;action&amp;#39;, color = &amp;#39;factor(action)&amp;#39;))
    #+ p9.geom_point(shape = &amp;#39;x&amp;#39;) 
    + p9.geom_smooth(method = &amp;#39;loess&amp;#39;)
    + p9.geom_hline(data = df_true, mapping = p9.aes(yintercept = &amp;#39;value&amp;#39;), color = &amp;#39;red&amp;#39;)
    + p9.geom_vline(xintercept = 600, color = &amp;#39;blue&amp;#39;)
    + p9.facet_wrap(&amp;quot;variable&amp;quot;, ncol = 4)
    + p9.theme(subplots_adjust={&amp;#39;wspace&amp;#39;:0.4}) # fix a displaying issue
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_30_0.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;First the main question: what about convergence of &lt;span class=&#34;math inline&#34;&gt;\(Q^{*}_{a}\)&lt;/span&gt;? For this we need only to look, for each state, at the action with the highest value, and compare that to the true value (red horizontal lines). Most of the values appear to have converged to a value close to the true value, but Q3 is clearly still way off. Note that we using smoothing here to average out the fluctuations around the true value.&lt;/p&gt;
&lt;p&gt;We noted earlier that at around episode 60.000, the optimal policy emerges and the learning curve becomes flat. Now, the most obvious reason why performance increases is because the value of &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; is decaying, so the deleterious effects of exploration should go down, and performance should go up.&lt;/p&gt;
&lt;p&gt;Another reason that performance goes up could be that the greedy policy is improving. It is interesting to examine whether at this point, meaningfull changes in the greedy policy still occur. Meaningfull changes in policy are caused by changes in the estimated state-action values. For example, we might expect two or more state-action value lines crossing, with the “right” action becoming dominant over the “wrong” action. Is this indeed the case?&lt;/p&gt;
&lt;p&gt;Indeed, from the plot above, with the change point at around episode 600 (x 100),a change occurs in Q6, where the actions 0 and 2 cross. However, from the true Q-value table (not shown) we see that both actions are equally rewarding, so the crossing has no effect on performance. Note that only one of the two converges to the true value, because the exploration rate becomes so low that learning for the other action almost completely stops in the end.&lt;/p&gt;
&lt;p&gt;lets zoom in then at the states that have low expected reward, Q0, Q1, Q2, and Q3. These are difficult to examine in the plot above, and have actions with expected rewards that are similar and therefore more difficult to resolve (BUT: since the difference in expected reward is small, the effect of resolving them on the overall performance is small as well). For these we plot the actual state-actions values:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;(p9.ggplot(data = dfm[dfm.variable.isin([&amp;#39;Q0&amp;#39;, &amp;#39;Q1&amp;#39;,&amp;#39;Q2&amp;#39;, &amp;#39;Q3&amp;#39;])], mapping = p9.aes(x = &amp;#39;episode&amp;#39;, y = &amp;#39;value&amp;#39;, group = &amp;#39;action&amp;#39;, color = &amp;#39;factor(action)&amp;#39;))
    + p9.geom_point(shape = &amp;#39;x&amp;#39;) 
    #+ p9.geom_smooth(method = &amp;#39;loess&amp;#39;)
    + p9.geom_hline(data = df_true[df_true.variable.isin([&amp;#39;Q0&amp;#39;, &amp;#39;Q1&amp;#39;,&amp;#39;Q2&amp;#39;, &amp;#39;Q3&amp;#39;])], mapping = p9.aes(yintercept = &amp;#39;value&amp;#39;), color = &amp;#39;red&amp;#39;)
    + p9.geom_vline(xintercept = 600, color = &amp;#39;blue&amp;#39;)
    + p9.facet_wrap(&amp;quot;variable&amp;quot;, scales = &amp;#39;free_y&amp;#39;)
    + p9.theme(subplots_adjust={&amp;#39;wspace&amp;#39;:0.15})
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_33_0.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From Q1, Q2 and Q3, we can see that exploration really goes down at around episode 500 (x 100) (&lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; at this point is &lt;code&gt;0.007&lt;/code&gt;), and with the optimal action standing out already long before reaching this point.&lt;/p&gt;
&lt;p&gt;Only with Q2 there is a portion of the learning curve where action 1 has the highest value, and is chosen for quite some time before switching back to action 0 again at around episode 60.000. Let’s compare with the true values for Q2:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;Q_perfect[2]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([0.15347714, 0.14684971, 0.14644451, 0.13958106])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, the difference in expected reward between the actions in state 2 is really small, and because of the increasingly greedy action selection, only action 0 converges to its true values, with the other values “frozen” in place because of the low exploration rate.&lt;/p&gt;
&lt;p&gt;Now, after analyzing what happens if we apply the “cookbook” approach to solving problems using RL, let’s change our attention to getting convergence for preferably ALL the state-action values.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;q-learning-theoretical-sufficient-conditions-for-convergence&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Q-learning: Theoretical sufficient conditions for convergence&lt;/h1&gt;
&lt;p&gt;According to our RL bible (Sutton &amp;amp; Barto), to obtain &lt;strong&gt;exact&lt;/strong&gt; convergence, we need two conditions to hold.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The first is that all states continue to be visited, and&lt;/li&gt;
&lt;li&gt;the second is that the learning rate goes to zero.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;continuous-exploration-visiting-all-the-states&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Continuous exploration: visiting all the states&lt;/h2&gt;
&lt;p&gt;The nice thing of Q-learning is that it is an &lt;strong&gt;off-policy&lt;/strong&gt; learning algorithm. This means that no matter what the actual policy is used to explore the states, the Q-values we learn correspond to the expected reward when following the &lt;strong&gt;optimal policy&lt;/strong&gt;. Which is quite miraculous if you ask me.&lt;/p&gt;
&lt;p&gt;Now, our (admittedly, a bit academic) goal is getting &lt;strong&gt;all&lt;/strong&gt; of the learned state values &lt;strong&gt;as close as possible&lt;/strong&gt; to the true values. An epsilon-greedy policy with a low &lt;span class=&#34;math inline&#34;&gt;\(epsilon\)&lt;/span&gt; would spent a lot of time by choosing state-actions that are on the optimal path between start and goal state, and would only rarely visit low value states, or choose low value state-actions.&lt;/p&gt;
&lt;p&gt;Because we can choose any policy we like, I chose a completely &lt;strong&gt;random&lt;/strong&gt; policy. This way, the Agent is more likely to end up in low value states and estimate the Q-values of those state-actions accurately as well.&lt;/p&gt;
&lt;p&gt;Note that since theFrozen Lake enviroment has a lot of inherent randomness already because of the ice being slippery, a policy with a low &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; (most of the time exploiting and only rarely exploring) will still bring the agent in low values states, but this would require much more episodes.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;convergence-and-learning-rate-schedules&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Convergence and learning rate schedules&lt;/h2&gt;
&lt;p&gt;For a particular constant learning rate, provided that it is not too high, the estimated Q-values will converge to a situation where they start to fluctuate around their true values.
If we subsequently lower the learning rate, the scale of the fluctuations (their &lt;strong&gt;variance&lt;/strong&gt;) will decrease.
If the learning rate is gradually decayed to zero, the estimates will converge.
According to Sutton &amp;amp; Barto (2018), the &lt;strong&gt;decay schedule&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; must obey two constraints to assure convergence: (p 33).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;sum of increments must go to infinity&lt;/li&gt;
&lt;li&gt;sum of the square of increments must go to zero&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;On the interwebs, I found two formulas that are often used to decay hyperparameters of Reinforcement Learning algorithms:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\alpha_n = K / (K + n)\)&lt;/span&gt; (Eq. 1)&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\alpha_n = \Sigma^{\infty}_{n = 1} \delta^n \alpha_0\)&lt;/span&gt; (Eq. 2)&lt;/p&gt;
&lt;p&gt;Again, just as with the decay schedule for &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;, it is helpful to visualize the decay schedules to check that they are reasonable before we start to use them with our Q-learning algorithm.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Eq 1
def create_alpha_schedule(num_episodes, \
                     alpha_K = 100, alpha_min = 1e-3):
    x = np.arange(num_episodes)+1
    y = np.maximum(alpha_K/(x + alpha_K), alpha_min)
    return y

# Eq 2
def create_alpha_schedule2(num_episodes,  \
                       alpha_start=1.0, alpha_decay=.999, alpha_min=1e-3):
    x = np.arange(num_episodes)+1
    y = np.full(num_episodes, alpha_start)
    y = np.maximum((alpha_decay**x)*alpha_start, alpha_min)
    return y

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I played a bit with the shape parameter to get a curve with the “elbow” around 20% of the episodes.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;alpha_schedule = create_alpha_schedule(num_episodes = 500_000, \
                                       alpha_K = 5_000)

plot_schedule(alpha_schedule, ylab = &amp;#39;Alpha&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_44_0.png&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;round(min(alpha_schedule), 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.01&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This curve decays &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; in 500.000 episodes to &lt;code&gt;0.01&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The second formula (Equation 2) decays &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; even further, to &lt;code&gt;0.001&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;alpha_schedule2 = create_alpha_schedule2(num_episodes = 500_000, \
                                        alpha_decay = 0.99997)

plot_schedule(alpha_schedule2, ylab = &amp;#39;Alpha&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_47_0.png&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;min(alpha_schedule2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.001&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;running-the-q-learning-algorithm-for-different-learning-rate-schedules&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Running the Q-learning algorithm for different learning rate schedules&lt;/h1&gt;
&lt;p&gt;We start with the decay function that follows Equation 1. To get a full random policy, we set &lt;span class=&#34;math inline&#34;&gt;\(\epsilon = 1\)&lt;/span&gt;. Note that this gives awful performance where the learning curve suggests it is hardly learning anything at all. However, wait until we try out a fully exploiting policy on the Q-value table learned during this run!&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# K / N + K decay learning rate schedule
# fully random policy

plot_every = 100

alpha_schedule = create_alpha_schedule(num_episodes = 500_000, \
                                       alpha_K = 5_000)

n_episodes = len(alpha_schedule)

fullrun = 0

if fullrun == 1:
    Q_sarsamax, N_sarsamax, avg_scores, Qlist = q_learning(env, num_episodes = n_episodes, \
                                                eps = 1,\
                                                alpha_schedule = alpha_schedule, \
                                                gamma =  0.95, \
                                                plot_every = plot_every, \
                                                verbose = True)
    
    with open(&amp;#39;cached/as1_Q_sarsamax.pkl&amp;#39;, &amp;#39;wb&amp;#39;) as f:
        dill.dump(Q_sarsamax, f)
    with open(&amp;#39;cached/as1_avg_scores.pkl&amp;#39;, &amp;#39;wb&amp;#39;) as f:
        dill.dump(avg_scores, f)
    with open(&amp;#39;cached/as1_Qlist.pkl&amp;#39;, &amp;#39;wb&amp;#39;) as f:
        dill.dump(Qlist, f)        
else:
    with open(&amp;#39;cached/as1_Q_sarsamax.pkl&amp;#39;, &amp;#39;rb&amp;#39;) as f:
        Q_sarsamax = dill.load(f)
    with open(&amp;#39;cached/as1_avg_scores.pkl&amp;#39;, &amp;#39;rb&amp;#39;) as f:
        avg_scores = dill.load(f)
    with open(&amp;#39;cached/as1_Qlist.pkl&amp;#39;, &amp;#39;rb&amp;#39;) as f:
        Qlist = dill.load(f)      

Q_as1_lasttable = [np.max(Q_sarsamax[key]) if key in Q_sarsamax \
     else 0 for key in np.arange(env.nS)]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is the learning curve for this run of Q-learning:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# plot performance
plt.plot(np.linspace(0,n_episodes,len(avg_scores),endpoint=False), np.asarray(avg_scores))
plt.xlabel(&amp;#39;Episode Number&amp;#39;)
plt.ylabel(&amp;#39;Average Reward (Over Next %d Episodes)&amp;#39; % plot_every)
plt.show()

print((&amp;#39;Best Average Reward over %d Episodes: &amp;#39; % plot_every), np.max(avg_scores))    
 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_53_0.png&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Best Average Reward over 100 Episodes:  0.08&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Pretty awfull huh? Now let us check out the performance of the learned Q-table:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fullrun = 0
runs = 100

if fullrun == 1:
    d_q = []
    for i in range(runs):
        avg_scores = test_performance(Q_sarsamax, env, num_episodes = 1000, \
                                   plot_every = 1000, verbose = False)
        d_q.append({&amp;#39;run&amp;#39;: i,
                 &amp;#39;avg_score&amp;#39;: np.mean(avg_scores)})  
        print(&amp;quot;\ri = {}/{}&amp;quot;.format(i+1, runs), end=&amp;quot;&amp;quot;)
        sys.stdout.flush() 
        
    d_qlearn = pd.DataFrame(d_q)
    d_qlearn.to_pickle(&amp;#39;cached/scores_qlearn_0.95.pkl&amp;#39;)
else:
    d_qlearn = pd.read_pickle(&amp;#39;cached/scores_qlearn_0.95.pkl&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;round(np.mean(d_qlearn.avg_score), 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.778&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Bam! Equal to the performance of the optimal policy found using Dynamic programming (sampling error (2x SD) is +/- 0.008). The random policy has actual learned Q-values that for a greedy policy result in optimal performance!&lt;/p&gt;
&lt;div id=&#34;but-what-about-convergence&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;But what about convergence?&lt;/h2&gt;
&lt;p&gt;Ok, so Q-learning found an optimal policy. But did it converge?
Our &lt;code&gt;q_learning()&lt;/code&gt; function made a list of Q-tables while learning, adding a new table every 100 episodes. This gives us 5.000 datapoints for each Q-value, which we can plot to visually check for convergence.&lt;/p&gt;
&lt;p&gt;As with the list of state-action tables above, It takes some datawrangling to get the list of Q-tables in a nice long pandas DataFrame suitable for plotting. This is hidden away in the &lt;code&gt;list_to_df()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# 13s
dfm = list_to_df(Qlist)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;#10 s
(p9.ggplot(data = dfm.loc[(dfm.episode &amp;gt; -1)], mapping = p9.aes(x = &amp;#39;episode&amp;#39;, y = &amp;#39;value&amp;#39;))
    + p9.geom_point() 
    + p9.geom_smooth(method = &amp;quot;loess&amp;quot;, color = &amp;quot;yellow&amp;quot;)
    + p9.geom_hline(data = df_true, mapping = p9.aes(yintercept = &amp;#39;value&amp;#39;), color = &amp;#39;red&amp;#39;)
    + p9.facet_wrap(&amp;quot;variable&amp;quot;, scales = &amp;#39;free_y&amp;#39;)
    + p9.theme(subplots_adjust={&amp;#39;wspace&amp;#39;:0.4}) # fix plotting issue
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_60_0.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This looks good, lets zoom in at one of the more noisier Q-values, &lt;code&gt;Q14&lt;/code&gt;.
In the learning schedule used, the lowest value for &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; is &lt;code&gt;0.01&lt;/code&gt;.
At this value of the learning rate, there is still considerable variation around the true value.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;(p9.ggplot(data = dfm.loc[(dfm.variable == &amp;#39;Q10&amp;#39;) &amp;amp; (dfm.episode &amp;gt; -1)], mapping = p9.aes(x = &amp;#39;episode&amp;#39;, y = &amp;#39;value&amp;#39;))
    + p9.geom_point() 
    + p9.geom_smooth(method = &amp;quot;loess&amp;quot;, color = &amp;quot;yellow&amp;quot;)
    + p9.geom_hline(data = df_true[df_true.variable == &amp;#39;Q10&amp;#39;], mapping = p9.aes(yintercept = &amp;#39;value&amp;#39;), color = &amp;#39;red&amp;#39;)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_62_0.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Suppose we did not know the true value of Q(S = 14), and wanted to estimate it using Q-learning. From the plot above, an obvious strategy is to average all the values in the tail of the learning rate schedule, say after episode 400.000.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# drop burn-in, then average Q-vals by variable
Q_est = (dfm.loc[dfm.episode &amp;gt; 4000]
         .groupby([&amp;#39;variable&amp;#39;])
         .mean()
        )

# convert to a 1D array sorted by Q-value
Q_est[&amp;#39;sort_order&amp;#39;] = [int(str.replace(x, &amp;#39;Q&amp;#39;, &amp;#39;&amp;#39;)) \
                       for x in Q_est.index.values]

Q_est = Q_est.sort_values(by=[&amp;#39;sort_order&amp;#39;])

Q_est_as1 = Q_est[&amp;#39;value&amp;#39;].values
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lets compare these with the final estimated values, and with true values:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;Q_as1_lasttable = [np.max(Q_sarsamax[key]) if key in Q_sarsamax \
             else 0 for key in np.arange(env.nS)]

q_showdown = pd.DataFrame(
    {&amp;#39;q_est_as1&amp;#39;: Q_est_as1,
     &amp;#39;q_est_as1_lasttable&amp;#39;: Q_as1_lasttable,
     &amp;#39;q_true&amp;#39;: V_pi_dc})
q_showdown[&amp;#39;state&amp;#39;] = range(16)

q_showdown = pd.melt(q_showdown, [&amp;#39;state&amp;#39;])

(p9.ggplot(data = q_showdown, mapping = p9.aes(x = &amp;#39;state&amp;#39;, y = &amp;#39;value&amp;#39;, color = &amp;#39;factor(variable)&amp;#39;))
+ p9.geom_point(size = 5, shape = &amp;#39;x&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_66_0.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Pretty close! So here we see Q-learning finally delivering on its convergence promise.&lt;/p&gt;
&lt;p&gt;And what about final values vs averaging? Averaging seems to have improved the estimates a bit. Note that we had to choose an averaging window based on eyeballing the learning curves for the separate Q-values.&lt;/p&gt;
&lt;p&gt;Can we do even better? A learning rate schedule where alpha is lowered further would diminish the fluctuations around the true values, but at the risk of lowering it too fast and effectively freezing (or very slowly evolving) the Q-values at non-equilibrium values.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;decay-learning-rate-schedule-variant-ii&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;decay learning rate schedule variant II&lt;/h1&gt;
&lt;p&gt;The second formula decays &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; to &lt;code&gt;0.001&lt;/code&gt;, ten times lower than the previous decay schedule:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# fully random policy

plot_every = 100

alpha_schedule2 = create_alpha_schedule2(num_episodes = 500_000, \
                                        alpha_decay = 0.99997)
n_episodes = len(alpha_schedule2)

fullrun = 0

if fullrun == 1:
    Q_sarsamax, N_sarsamax, avg_scores, Qlist = q_learning(env, num_episodes = n_episodes, \
                                                eps = 1,\
                                                alpha_schedule = alpha_schedule2, \
                                                gamma =  0.95, \
                                                plot_every = plot_every, \
                                                verbose = True)
    
    with open(&amp;#39;cached/as2_Q_sarsamax.pkl&amp;#39;, &amp;#39;wb&amp;#39;) as f:
        dill.dump(Q_sarsamax, f)
    with open(&amp;#39;cached/as2_avg_scores.pkl&amp;#39;, &amp;#39;wb&amp;#39;) as f:
        dill.dump(avg_scores, f)
    with open(&amp;#39;cached/as2_Qlist.pkl&amp;#39;, &amp;#39;wb&amp;#39;) as f:
        dill.dump(Qlist, f)        
else:
    with open(&amp;#39;cached/as2_Q_sarsamax.pkl&amp;#39;, &amp;#39;rb&amp;#39;) as f:
        Q_sarsamax = dill.load(f)
    with open(&amp;#39;cached/as2_avg_scores.pkl&amp;#39;, &amp;#39;rb&amp;#39;) as f:
        avg_scores = dill.load(f)
    with open(&amp;#39;cached/as2_Qlist.pkl&amp;#39;, &amp;#39;rb&amp;#39;) as f:
        Qlist = dill.load(f)     &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;dfm = list_to_df(Qlist)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;(p9.ggplot(data = dfm.loc[(dfm.episode &amp;gt; -1)], mapping = p9.aes(x = &amp;#39;episode&amp;#39;, y = &amp;#39;value&amp;#39;))
    + p9.geom_point() 
    + p9.geom_smooth(method = &amp;quot;loess&amp;quot;, color = &amp;quot;yellow&amp;quot;)
    + p9.geom_hline(data = df_true, mapping = p9.aes(yintercept = &amp;#39;value&amp;#39;), color = &amp;#39;red&amp;#39;)
    + p9.facet_wrap(&amp;quot;variable&amp;quot;, scales = &amp;#39;free_y&amp;#39;)
    + p9.theme(subplots_adjust={&amp;#39;wspace&amp;#39;:0.4})
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_70_0.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Cleary, the fluctuations are reduced compared to the previous schedule. AND all Q-values still fluctuate around their true values, so it seems that this schedule is better with respect to finding the true values.&lt;/p&gt;
&lt;p&gt;Let’s see if the accuracy of the estimated Q-values is indeed higher:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# drop burn-in, then average Q-vals by variable
Q_est = (dfm.loc[dfm.episode &amp;gt; 2000]
         .groupby([&amp;#39;variable&amp;#39;])
         .mean()
        )
# convert to 1d array sorted by state nr
Q_est[&amp;#39;sort_order&amp;#39;] = [int(str.replace(x, &amp;#39;Q&amp;#39;, &amp;#39;&amp;#39;)) \
                       for x in Q_est.index.values]

Q_est = Q_est.sort_values(by=[&amp;#39;sort_order&amp;#39;])

Q_est_as2 = Q_est[&amp;#39;value&amp;#39;].values

Q_as2_lasttable = [np.max(Q_sarsamax[key]) if key in Q_sarsamax \
             else 0 for key in np.arange(env.nS)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;q_showdown = pd.DataFrame(
    {&amp;#39;Q_as2_lasttable&amp;#39;: Q_as2_lasttable,
     &amp;#39;q_est_as2&amp;#39;: Q_est_as2,
     &amp;#39;q_true&amp;#39;: V_pi_dc})
q_showdown[&amp;#39;state&amp;#39;] = range(16)

q_showdown = pd.melt(q_showdown, [&amp;#39;state&amp;#39;])

(p9.ggplot(data = q_showdown, mapping = p9.aes(x = &amp;#39;state&amp;#39;, y = &amp;#39;value&amp;#39;, color = &amp;#39;factor(variable)&amp;#39;))
+ p9.geom_point(size = 5, shape = &amp;#39;x&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-03-07-qlearning_frozenlake_convergence_files/Q_learning_FrozenLake_73_0.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Boom! Now our &lt;span class=&#34;math inline&#34;&gt;\(Q^{*}_{a}\)&lt;/span&gt; estimates are really getting close to the true values. Clearly, the second learning rate schedule is able to learn the true Q-values compared to the first rate schedule, given the fixed amount of computation, in this case 500.000 episodes each.&lt;/p&gt;
&lt;p&gt;Averaging out does not do much anymore, except for states 10 and 14, where it improves the estimates a tiny bit.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;wrapping-up&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Wrapping up&lt;/h1&gt;
&lt;p&gt;In conclusion, we have seen that the common approach of using Q-learning with a constant learning rate and gradually decreasing the exploration rate, given sensible values and rates, will indeed find the optimal policy. However, this approach does not necessary converge to the true state-values. We have to tune the algorithm exactly the other way around: keep the exploration rate constant and sufficiently high, and decay the learning rate. For sufficiently low learning rates, averaging out the fluctuations does not meaningfully increase accuracy of the learned Q-values.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Jacks Car Rental as a Gym Environment</title>
      <link>/post/jacks-car-rental-gym/</link>
      <pubDate>Wed, 30 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/post/jacks-car-rental-gym/</guid>
      <description>
&lt;link href=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;This blogpost is about &lt;a href=&#34;https://en.wikipedia.org/wiki/Reinforcement_learning&#34;&gt;reinforcement learning&lt;/a&gt;, part of the Machine Learning (ML) / AI family of computer algorithms and techniques.
Reinforcement learning is all about agents taking decisions in complex environments. The decisions (&lt;strong&gt;actions&lt;/strong&gt;) take the agent from a current &lt;strong&gt;state&lt;/strong&gt; or situation, to a new &lt;strong&gt;state&lt;/strong&gt;. When the probability of ending up in a new state is only dependent on the current state and the action the agent takes in that state, we are facing a so-called &lt;strong&gt;Markov Decision Problem&lt;/strong&gt;, or &lt;strong&gt;MDP&lt;/strong&gt; for short.&lt;/p&gt;
&lt;p&gt;Back in 2016, people at OpenAI, a startup company that specializes in AI/ML, created a Python library called &lt;strong&gt;Gym&lt;/strong&gt; that provides standardized access to a range of MDP environments. Using Gym means keeping a sharp separation between the RL algorithm (“The agent”) and the environment (or task) it tries to solve / optimize / control / achieve. Gym allows us to easily benchmark RL algorithms on a range of different environments. It also allows us to more easily build on others work, and share our own work (i.e. on Github). Because when I implement something as a Gym Environment, others can then immediately apply their algorithms on it, and vice versa.&lt;/p&gt;
&lt;p&gt;In this blogpost, we solve a famous decision problem called “Jack’s Car Rental” by first turning it into a Gym environment and then use a RL algorithm called “Policy Iteration” (a form of “Dynamic Programming”) to solve for the optimal decisions to take in this environment.&lt;/p&gt;
&lt;p&gt;The Gym environment for Jack’s Car Rental is called &lt;code&gt;gym_jcr&lt;/code&gt; and can be installed from &lt;a href=&#34;https://github.com/gsverhoeven/gym_jcr&#34;&gt;https://github.com/gsverhoeven/gym_jcr&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;jacks-car-rental-problem&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Jack’s Car Rental problem&lt;/h1&gt;
&lt;p&gt;Learning Reinforcement learning (RL) as a student, means working through the famous &lt;a href=&#34;http://incompleteideas.net/book/the-book.html&#34;&gt;book on RL by Sutton and Barto&lt;/a&gt;. In chapter 4, Example 4.2 (2018 edition), Jack’s Car Rental problem is presented:&lt;/p&gt;
&lt;pre class=&#34;plaintext&#34;&gt;&lt;code&gt;Jack’s Car Rental 

Jack manages two locations for a nationwide car rental company. 
Each day, some number of customers arrive at each location to rent cars. 
If Jack has a car available, he rents it out and is credited $10 by 
the national company. If he is out of cars at that location, then the 
business is lost. Cars become available for renting the day after they 
are returned. To help ensure that cars are available where they are 
needed, Jack can move them between the two locations overnight, at a cost 
of $2 per car moved. We assume that the number of cars requested and 
returned at each location are Poisson random variables. Suppose Lambda is
3 and 4 for rental requests at the first and second locations and 
3 and 2 for returns. 

To simplify the problem slightly, we assume that there can be no more than
20 cars at each location (any additional cars are returned to the 
nationwide company, and thus disappear from the problem) and a maximum 
of five cars can be moved from one location to the other in one night. 

We take the discount rate to be gamma = 0.9 and formulate this as a 
continuing finite MDP, where the time steps are days, the state is the 
number of cars at each location at the end of the day, and the actions 
are the net numbers of cars moved between the two locations overnight.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In order to implement this MDP in Gym and solving it using DP (Dynamic Programming), we need to calculate for each state - action combination the probability of transitioning to all other states. Here a state is defined as the number of cars at the two locations A and B. Since there can be between 0 and 20 cars at each location, we have in total 21 x 21 = 441 states. We have 11 actions, moving up to five cars from A to B, moving up to five cars from B to A, or moving no cars at all. We also need the rewards &lt;strong&gt;R&lt;/strong&gt; for taking action &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; in state &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Luckily for us, Christian Herta and Patrick Baumann, as part of their project &lt;a href=&#34;https://www.deep-teaching.org/&#34;&gt;“Deep.Teaching”&lt;/a&gt;, created a Jupyter Notebook containing a well explained Python code solution for calculating &lt;strong&gt;P&lt;/strong&gt;, and &lt;strong&gt;R&lt;/strong&gt;, and published it as open source under the MIT license. I extracted their functions and put them in &lt;code&gt;jcr_mdp.py&lt;/code&gt;, containing two top level functions &lt;code&gt;create_P_matrix()&lt;/code&gt; and &lt;code&gt;create_R_matrix()&lt;/code&gt;, these are used when the Gym environment is initialized.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;jackscarrentalenv&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;JacksCarRentalEnv&lt;/h1&gt;
&lt;p&gt;My approach to creating the Gym environment for Jack’s Car Rental was to take the &lt;a href=&#34;https://gym.openai.com/envs/FrozenLake-v0/&#34;&gt;Frozen Lake Gym environment&lt;/a&gt;, and rework it to become JacksCarRentalEnv. I chose this environment because it has a similar structure as JCR, having discrete states and discrete actions. In addition, it is one of the few environments that create and expose the complete transition matrix &lt;strong&gt;P&lt;/strong&gt; needed for the DP algorithm.&lt;/p&gt;
&lt;p&gt;There is actually not much to it at this point, as most functionality is provided by the &lt;code&gt;DiscreteEnv&lt;/code&gt; class that our environment builds on. We need only to specify four objects:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;nS: number of states&lt;/li&gt;
&lt;li&gt;nA: number of actions&lt;/li&gt;
&lt;li&gt;P: transitions&lt;/li&gt;
&lt;li&gt;isd: initial state distribution (list or array of length nS)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;nS&lt;/code&gt; and &lt;code&gt;nA&lt;/code&gt; were already discussed above, there are 441 and 11 respectively.
For the &lt;code&gt;isd&lt;/code&gt; we simply choose an equal probability to start in any of the 441 states.&lt;/p&gt;
&lt;p&gt;This leaves us with the transitions &lt;strong&gt;P&lt;/strong&gt;. This needs to be in a particular format, a &lt;code&gt;dictionary dict of dicts of lists, where P[s][a] == [(probability, nextstate, reward, done), ...]&lt;/code&gt; according to the help of this class. So we take the &lt;strong&gt;P&lt;/strong&gt; and &lt;strong&gt;R&lt;/strong&gt; arrays created by the python code in &lt;code&gt;jcr_mdp.py&lt;/code&gt; and use these to fill the dictionary in the proper way (drawing inspiration from the Frozen Lake &lt;strong&gt;P&lt;/strong&gt; object :)).&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;P = {s : {a : [] for a in range(nA)} for s in range(nS)}

# prob, next_state, reward, done
for s in range(nS):
    # need a state vec to extract correct probs from Ptrans
    state_vec = np.zeros(nS)
    state_vec[s] = 1
    for a in range(nA):
        prob_vec = np.dot(Ptrans[:,:,a], state_vec)
        li = P[s][a]
        # add rewards for all transitions
        for ns in range(nS):
            li.append((prob_vec[ns], ns, R[s][a], False))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And were done! Let’s try it out.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import matplotlib.pyplot as plt
import numpy as np
import pickle

# Gym environment
import gym
import gym_jcr
# RL algorithm
from dp import *&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# n.b. can take up to 15 s
env = gym.make(&amp;quot;JacksCarRentalEnv-v0&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So what we have?&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# print the state space and action space
print(env.observation_space)
print(env.action_space)

# print the total number of states and actions
print(env.nS)
print(env.nA)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Discrete(441)
Discrete(11)
441
11&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let us check for state &lt;code&gt;s= 0&lt;/code&gt;, for each action &lt;code&gt;a&lt;/code&gt;, if the probabilities of transitioning to a new state &lt;code&gt;new_state&lt;/code&gt; sum to one (we need to end up somewhere right?).&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# from state 0, for each action the probs for going to new state
s = 0

for a in range(env.nA):
    prob = 0.0
    for new_state in range(env.nS):
        prob += env.P[s][a][new_state][0]
    print(prob, end = &amp;#39; &amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.9999999999999992 0.9999999999999992 0.9999999999999992 0.9999999999999992 0.9999999999999992 0.9999999999999992 0.9999999999999992 0.9999999999999992 0.9999999999999992 0.9999999999999992 0.9999999999999992 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Close enough. Let’s run our Dynamic Programming algorithm on it!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;policy-iteration-on-jcr&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Policy iteration on JCR&lt;/h1&gt;
&lt;p&gt;The &lt;code&gt;policy_iteration()&lt;/code&gt; function used below is from &lt;a href=&#34;https://github.com/gsverhoeven/hugo_source/blob/master/content/post/dp.py&#34;&gt;dp.py&lt;/a&gt;. This exact same code was used in a Jupyter tutorial notebook to solve the &lt;a href=&#34;https://gym.openai.com/envs/FrozenLake-v0/&#34;&gt;Frozen-Lake Gym environment&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We reproduce the results from the Sutton &amp;amp; Barto book (p81), where the algorithm converges after four iterations. This takes about 30 min on my computer.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fullrun = False

if fullrun == True:
    policy, V = policy_iteration(env, gamma = 0.9)
    with open(&amp;#39;policy.bin&amp;#39;, &amp;#39;wb&amp;#39;) as f:
        pickle.dump(policy, f)
    with open(&amp;#39;values.bin&amp;#39;, &amp;#39;wb&amp;#39;) as f:
        pickle.dump(V, f)
else:
    with open(&amp;#39;policy.bin&amp;#39;, &amp;#39;rb&amp;#39;) as f:
        policy = pickle.load(f)
    with open(&amp;#39;values.bin&amp;#39;, &amp;#39;rb&amp;#39;) as f:
        V = pickle.load(f)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-optimal-policy-as-a-contour-map&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Plot optimal policy as a contour map&lt;/h1&gt;
&lt;p&gt;For easy plotting, we need to transform the policy from a 2d state-action matrix to a 2d state-A, state-B matrix with the action values in the cells.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;MAX_CARS = 20

def get_state_vector(a, b):
    s = np.zeros((MAX_CARS+1)**2)
    s[a*(MAX_CARS+1)+b] = 1
    return s

policy_map = np.zeros([MAX_CARS+1, MAX_CARS+1])

for a in range(MAX_CARS+1):
    for b in range(MAX_CARS+1):
        state = get_state_vector(a, b)
        s = state.argmax()
        policy_map[a, b] = np.argmax(policy[s,:]) - 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We visualize the optimal policy as a 2d heatmap using &lt;code&gt;matplotlib.pyplot.imshow()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;plt.figure(figsize=(7,6))
hmap = plt.imshow(policy_map, cmap=&amp;#39;viridis&amp;#39;, origin=&amp;#39;lower&amp;#39;)
cbar = plt.colorbar(hmap)
cbar.ax.set_ylabel(&amp;#39;actions&amp;#39;)
plt.title(&amp;#39;Policy&amp;#39;)
plt.xlabel(&amp;quot;cars at B&amp;quot;)
plt.ylabel(&amp;quot;cars at A&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/post/2020-12-30-jacks_car_rental_gym_files/2020-12-30-jacks_car_rental_gym_13_1.png&#34; alt=&#34;Optimal policy for all states of Jack’s Car Rental&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Optimal policy for all states of Jack’s Car Rental&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion-and-outlook&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion and outlook&lt;/h1&gt;
&lt;p&gt;Conclusion: yes we can turn JCR into a Gym environment and solve it using the exact same (policy iteration) code that I had earlier used to solve the Frozen-Lake Gym environment!&lt;/p&gt;
&lt;p&gt;So now what? One obvious area of improvement is speed: It takes too long to load the environment. Also the DP algorithm is slow, because it uses for loops instead of matrix operations.&lt;/p&gt;
&lt;p&gt;Another thing is that currently the rewards that the environment returns are &lt;strong&gt;average expected rewards&lt;/strong&gt; that are received when taking action &lt;em&gt;a&lt;/em&gt; in state &lt;em&gt;s&lt;/em&gt; . However, they do not match the actual amount of cars rented when transitioning from a particular state &lt;em&gt;s&lt;/em&gt; to a new state &lt;em&gt;s’&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Finally, adding the modifications to the problem from Exercise 4.7 in Sutton &amp;amp; Barto could also be implemented, but this complicates the calculation of &lt;strong&gt;P&lt;/strong&gt; and &lt;strong&gt;R&lt;/strong&gt; even further.
For me, this is the real takeaway from this exercise: it is really hard to (correctly) compute the complete set of transition probabilities and rewards for an MDP, but it is much easier if we just need to simulate single transitions according to the MDP specification. Wikipedia has a nice paragraph on it under &lt;a href=&#34;https://en.wikipedia.org/wiki/Markov_decision_process#Simulator_models&#34;&gt;simulator models for MDPs&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Using posterior predictive distributions to get the Average Treatment Effect (ATE) with uncertainty</title>
      <link>/post/posterior-distribution-average-treatment-effect/</link>
      <pubDate>Fri, 04 Sep 2020 00:00:00 +0000</pubDate>
      <guid>/post/posterior-distribution-average-treatment-effect/</guid>
      <description>
&lt;link href=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;gertjan-verhoeven-misja-mikkers&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Gertjan Verhoeven &amp;amp; Misja Mikkers&lt;/h2&gt;
&lt;p&gt;Here we show how to use &lt;a href=&#34;https://mc-stan.org&#34;&gt;Stan&lt;/a&gt; with the &lt;a href=&#34;https://github.com/paul-buerkner/brms&#34;&gt;brms&lt;/a&gt; R-package to calculate the posterior predictive distribution of a covariate-adjusted average treatment effect. We fit a model on simulated data that mimics a (very clean) experiment with random treatment assignment.&lt;/p&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;Suppose we have data from a Randomized Controlled Trial (RCT) and we want to estimate the average treatment effect (ATE). Patients get treated, or not, depending only on a coin flip. This is encoded in the &lt;code&gt;Treatment&lt;/code&gt; variable. The outcome is a count variable &lt;code&gt;Admissions&lt;/code&gt;, representing the number of times the patient gets admitted to the hospital. The treatment is expected to reduce the number of hospital admissions for patients.&lt;/p&gt;
&lt;p&gt;To complicate matters (a bit): As is often the case with patients, not all patients are identical. Suppose that older patients have on average more Admissions. So &lt;code&gt;Age&lt;/code&gt; is a covariate.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;average-treatment-effect-ate&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Average treatment effect (ATE)&lt;/h3&gt;
&lt;p&gt;Now, after we fitted a model to the data, we want to actually &lt;strong&gt;use&lt;/strong&gt; our model to answer &amp;quot;What-if&amp;quot; questions (counterfactuals). Here we answer the following question:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What would the average reduction in Admissions be if we had treated &lt;strong&gt;ALL&lt;/strong&gt; the patients in the sample, compared to a situation where &lt;strong&gt;NO&lt;/strong&gt; patient in the sample would have received treatment?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Well, that is easy, we just take the fitted model, change treatment from zero to one for each, and observe the (&amp;quot;marginal&amp;quot;) effect on the outcome, right?&lt;/p&gt;
&lt;p&gt;Yes, but the uncertainty is harder. We have uncertainty in the estimated coefficients of the intercept and covariate, as well as in the coefficient of the treatment variable. And these uncertainties can be correlated (for example between the coefficients of intercept and covariate).&lt;/p&gt;
&lt;p&gt;Here we show how to use &lt;code&gt;posterior_predict()&lt;/code&gt; to simulate outcomes of the model using the sampled parameters. If we do this for two counterfactuals, all patients treated, and all patients untreated, and subtract these, we can easily calculate the posterior predictive distribution of the average treatment effect.&lt;/p&gt;
&lt;p&gt;Let&#39;s do it!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;load-packages&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Load packages&lt;/h3&gt;
&lt;p&gt;This tutorial uses &lt;a href=&#34;https://github.com/paul-buerkner/brms&#34;&gt;brms&lt;/a&gt;, a user friendly interface to full Bayesian modelling with &lt;a href=&#34;https://mc-stan.org/&#34;&gt;Stan&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(rstan)
library(brms) &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data-simulation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data simulation&lt;/h3&gt;
&lt;p&gt;We generate fake data that matches our problem setup.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Admissions&lt;/code&gt; are determined by patient &lt;code&gt;Age&lt;/code&gt;, whether the patient has &lt;code&gt;Treatment&lt;/code&gt;, and some random &lt;code&gt;Noise&lt;/code&gt; to capture unobserved effects that influence &lt;code&gt;Admissions&lt;/code&gt;. We exponentiate them to always get a positive number, and plug it in the Poisson distribution using &lt;code&gt;rpois()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123) 

id &amp;lt;- 1:200   
n_obs &amp;lt;- length(id)
b_tr &amp;lt;- -0.7
b_age &amp;lt;- 0.1

df_sim &amp;lt;- as.data.frame(id) %&amp;gt;% 
mutate(Age = rgamma(n_obs, shape = 5, scale = 2)) %&amp;gt;% # positive cont predictor
mutate(Noise = rnorm(n_obs, mean = 0, sd = 0.5)) %&amp;gt;% # add noise
mutate(Treatment = ifelse(runif(n_obs) &amp;lt; 0.5, 0, 1)) %&amp;gt;% # Flip a coin for treatment
mutate(Lambda = exp(b_age * Age + b_tr * Treatment + Noise)) %&amp;gt;% # generate lambda for the poisson dist
mutate(Admissions = rpois(n_obs, lambda = Lambda))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;summarize-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Summarize data&lt;/h3&gt;
&lt;p&gt;Ok, so what does our dataset look like?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(df_sim)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        id              Age             Noise            Treatment    
##  Min.   :  1.00   Min.   : 1.794   Min.   :-1.32157   Min.   :0.000  
##  1st Qu.: 50.75   1st Qu.: 6.724   1st Qu.:-0.28614   1st Qu.:0.000  
##  Median :100.50   Median : 8.791   Median : 0.04713   Median :0.000  
##  Mean   :100.50   Mean   : 9.474   Mean   : 0.02427   Mean   :0.495  
##  3rd Qu.:150.25   3rd Qu.:11.713   3rd Qu.: 0.36025   3rd Qu.:1.000  
##  Max.   :200.00   Max.   :24.835   Max.   : 1.28573   Max.   :1.000  
##      Lambda          Admissions    
##  Min.   : 0.2479   Min.   : 0.000  
##  1st Qu.: 1.1431   1st Qu.: 1.000  
##  Median : 1.8104   Median : 2.000  
##  Mean   : 2.6528   Mean   : 2.485  
##  3rd Qu.: 3.0960   3rd Qu.: 3.000  
##  Max.   :37.1296   Max.   :38.000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Treatment variable should reduce admissions. Lets visualize the distribution of Admission values for both treated and untreated patients.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = df_sim, aes(x = Admissions)) +
  geom_histogram(stat=&amp;quot;count&amp;quot;) +
  facet_wrap(~ Treatment) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-09-04-brms_posterior_pred_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The effect of the treatment on reducing admissions is clearly visible.&lt;/p&gt;
&lt;p&gt;We can also visualize the relationship between &lt;code&gt;Admissions&lt;/code&gt; and &lt;code&gt;Age&lt;/code&gt;, for both treated and untreated patients. We use the &lt;code&gt;viridis&lt;/code&gt; scales to provide colour maps that are designed to be perceived by viewers with common forms of colour blindness.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = df_sim, aes(x = Age, y = Admissions, color = as.factor(Treatment))) +
  geom_point() +
  scale_color_viridis_d(labels = c(&amp;quot;No Treatment&amp;quot;, &amp;quot;Treatment&amp;quot;)) +
  labs(color = &amp;quot;Treatment&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-09-04-brms_posterior_pred_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now lets fit our Bayesian Poisson regression model to it.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fit model&lt;/h3&gt;
&lt;p&gt;We use &lt;code&gt;brms&lt;/code&gt; default priors for convenience here. For a real application we would of course put effort into into crafting priors that reflect our current knowledge of the problem at hand.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model1 &amp;lt;- brm(
  formula = as.integer(Admissions) ~  Age + Treatment,
   data = df_sim,
  family = poisson(),
  warmup = 2000, iter = 5000, 
  cores = 2, 
  chains = 4,
  seed = 123,
  silent = TRUE,
  refresh = 0,
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Compiling Stan program...&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Start sampling&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;check-model-fit&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Check model fit&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(model1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: poisson 
##   Links: mu = log 
## Formula: as.integer(Admissions) ~ Age + Treatment 
##    Data: df_sim (Number of observations: 200) 
## Samples: 4 chains, each with iter = 5000; warmup = 2000; thin = 1;
##          total post-warmup samples = 12000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept    -0.05      0.12    -0.28     0.18 1.00     7410     7333
## Age           0.12      0.01     0.10     0.14 1.00     8052     8226
## Treatment    -0.83      0.10    -1.02    -0.63 1.00     7794     7606
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the posterior dists for &lt;span class=&#34;math inline&#34;&gt;\(\beta_{Age}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_{Treatment}\)&lt;/span&gt; cover the true values, so looking good. To get a fuller glimpse into the (correlated) uncertainty of the model parameters we make a pairs plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pairs(model1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-09-04-brms_posterior_pred_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As expected, the coefficients &lt;span class=&#34;math inline&#34;&gt;\(\beta_{Intercept}\)&lt;/span&gt; (added by &lt;code&gt;brms&lt;/code&gt;) and &lt;span class=&#34;math inline&#34;&gt;\(\beta_{Age}\)&lt;/span&gt; are highly correlated.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;first-attempt-calculate-individual-treatment-effects-using-the-model-fit-object&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;First attempt: Calculate Individual Treatment effects using the model fit object&lt;/h3&gt;
&lt;p&gt;Conceptually, the simplest approach for prediction is to take the most likely values for all the model parameters, and use these to calculate for each patient an individual treatment effect. This is what plain OLS regression does when we call &lt;code&gt;predict.lm()&lt;/code&gt; on a fitted model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;est_intercept &amp;lt;- fixef(model1, pars = &amp;quot;Intercept&amp;quot;)[,1]
est_age_eff &amp;lt;- fixef(model1, pars = &amp;quot;Age&amp;quot;)[,1]
est_t &amp;lt;- fixef(model1, pars = &amp;quot;Treatment&amp;quot;)[,1]

# brm fit parameters (intercept plus treatment)
ites &amp;lt;- exp(est_intercept + (est_age_eff * df_sim$Age) +  est_t) - exp(est_intercept + (est_age_eff * df_sim$Age))

ggplot(data.frame(ites), aes(x = ites)) + 
  geom_histogram() +
  geom_vline(xintercept = mean(ites), col = &amp;quot;red&amp;quot;) +
  ggtitle(&amp;quot;Effect of treatment on Admissions for each observation&amp;quot;) +
   expand_limits(x = 0) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-09-04-brms_posterior_pred_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Averaging the ITEs gives us the ATE, displayed in red.&lt;/p&gt;
&lt;p&gt;Ok, so &lt;strong&gt;on average&lt;/strong&gt;, our treatment reduces the number of Admissions by -1.9.&lt;/p&gt;
&lt;p&gt;You may wonder: why do we even have a distribution of treatment effects here? Should it not be the same for each patient? Here a peculiarity of the Poisson regression model comes to surface: The effect of changing &lt;code&gt;Treatment&lt;/code&gt; from 0 to 1 on the outcome depends on the value of &lt;code&gt;Age&lt;/code&gt; of the patient. This is because we &lt;strong&gt;exponentiate&lt;/strong&gt; the linear model before we plug it into the Poisson distribution.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;next-the-uncertainty-in-the-ate&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Next, the uncertainty in the ATE&lt;/h3&gt;
&lt;p&gt;How to get all this underlying, correlated uncertainty in the model parameters, that have varying effects depending on the covariates of patients, and properly propagate that to the ATE? What is the range of plausible values of the ATE consistent with the data &amp;amp; model?&lt;/p&gt;
&lt;p&gt;At this point, using only the summary statistics of the model fit (i.e. the coefficients), we hit a wall. To make progress we have to work with the full posterior distribution of model parameters, and use this to make predictions. That is why it is often called &amp;quot;the posterior predictive distribution&amp;quot; (Check &lt;a href=&#34;http://www.stat.columbia.edu/~gelman/book/BDA3.pdf&#34;&gt;BDA3&lt;/a&gt; for the full story).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;posterior-predictive-distribution-ppd-two-tricks&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Posterior predictive distribution (PPD): two tricks&lt;/h3&gt;
&lt;p&gt;Ok, you say, a Posterior Predictive Distribution, let&#39;s have it! Where can I get one?&lt;/p&gt;
&lt;p&gt;Luckily for us, most of the work is already done, because we have fitted our model. And thus we have a large collection of parameter draws (or samples, to confuse things a bit). All the correlated uncertainty is contained in these draws.&lt;/p&gt;
&lt;p&gt;This is the first trick. Conceptually, we imagine that each separate draw of the posterior represents a particular version of our model.&lt;/p&gt;
&lt;p&gt;In our example model fit, we have 12.000 samples from the posterior. In our imagination, we now have 12.000 versions of our model, where unlikely parameter combinations are present less often compared to likely parameter combinations. The full uncertainty of our model parameters is contained in this &amp;quot;collection of models&amp;quot; .&lt;/p&gt;
&lt;p&gt;The second trick is that we simulate (generate) predictions for all observations, from each of these 12.000 models. Under the hood, this means computing for each model (we have 12.000), for each observation (we have 200) the predicted lambda value given the covariates, and drawing a single value from a Poisson distribution with that &lt;span class=&#34;math inline&#34;&gt;\(\Lambda\)&lt;/span&gt; value (e.g. running &lt;code&gt;rpois(n = 1, lambda)&lt;/code&gt; ).&lt;/p&gt;
&lt;p&gt;This gives us a 12.000 x 200 matrix, that we can compute with.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;computing-with-the-ppd-brmsposterior_predict&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Computing with the PPD: brms::posterior_predict()&lt;/h3&gt;
&lt;p&gt;To compute PPD&#39;s, we can use &lt;code&gt;brms::posterior_predict()&lt;/code&gt;. We can feed it any dataset using the &lt;code&gt;newdata&lt;/code&gt; argument, and have it generate a PPD.&lt;/p&gt;
&lt;p&gt;For our application, the computation can be broken down in two steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Step 1: use &lt;code&gt;posterior_predict()&lt;/code&gt; on our dataset with &lt;code&gt;Treatment&lt;/code&gt; set to zero, do the same for our dataset with &lt;code&gt;Treatment&lt;/code&gt; set to one, and subtract the two matrices. This gives us a matrix of outcome differences / treatment effects.&lt;/li&gt;
&lt;li&gt;Step 2: Averaging over all cols (the N=200 simulated outcomes for each draw) should give us the distribution of the ATE. This distribution now represents the variability (uncertainty) of the estimate.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ok, step 1:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# create two versions of our dataset, with all Tr= 0 and all Tr=1
df_sim_t0 &amp;lt;- df_sim %&amp;gt;% mutate(Treatment = 0)

df_sim_t1 &amp;lt;- df_sim %&amp;gt;% mutate(Treatment = 1)

# simulate the PPDs
pp_t0 &amp;lt;- posterior_predict(model1, newdata = df_sim_t0)

pp_t1 &amp;lt;- posterior_predict(model1, newdata = df_sim_t1)

diff &amp;lt;- pp_t1 - pp_t0

dim(diff)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 12000   200&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And step 2 (averaging by row over the cols):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ATE_per_draw &amp;lt;- apply(diff, 1, mean)

# equivalent expression for tidyverse fans
#ATE_per_draw &amp;lt;- data.frame(diff) %&amp;gt;% rowwise() %&amp;gt;% summarise(avg = mean(c_across(cols = everything())))

length(ATE_per_draw)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 12000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, a distribution of plausible ATE values. Oo, that is so nice. Lets visualize it!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data.frame(ATE_per_draw), aes(x = ATE_per_draw)) +
  geom_histogram() + 
  geom_vline(xintercept = mean(ites), col = &amp;quot;red&amp;quot;) +
  ggtitle(&amp;quot;Posterior distribution of the Average Treatment Effect (ATE)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-09-04-brms_posterior_pred_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can compare this distribution with the point estimate of the ATE we obtained above using the model coefficients. It sits right in the middle (red line), just as it should be!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;demonstrating-the-versatility-uncertainty-in-the-sum-of-treatment-effects&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Demonstrating the versatility: uncertainty in the sum of treatment effects&lt;/h3&gt;
&lt;p&gt;Now suppose we are a policy maker, and we want to estimate the total reduction in Admissions if all patients get the treatment. And we want to quantify the range of plausible values of this summary statistic.&lt;/p&gt;
&lt;p&gt;To do so, we can easily adjust our code to summing instead of averaging all the treatment effects within each draw (i.e. by row):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;TTE_per_draw &amp;lt;- apply(diff, 1, sum)

ggplot(data.frame(TTE_per_draw), aes(x = TTE_per_draw)) +
  geom_histogram() + 
  geom_vline(xintercept = sum(ites), col = &amp;quot;red&amp;quot;) +
  ggtitle(&amp;quot;Posterior distribution of the Total Treatment Effect (TTE)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-09-04-brms_posterior_pred_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So our model predicts for the aggregate reduction of patient Admissions a value in the range of -500 to -250.&lt;/p&gt;
&lt;p&gt;This distribution can then be used to answer questions such as &amp;quot;what is the probability that our treatment reduces Admissions by at least 400&amp;quot;?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;TTE &amp;lt;- data.frame(TTE_per_draw) %&amp;gt;%
  mutate(counter = ifelse(TTE_per_draw &amp;lt; -400, 1, 0)) 

mean(TTE$counter) * 100&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 38.1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;take-home-message-ppd-with-brms-is-easy-and-powerful&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Take home message: PPD with brms is easy and powerful&lt;/h3&gt;
&lt;p&gt;We hope to have demonstrated that when doing a full bayesian analysis with &lt;code&gt;brms&lt;/code&gt; and &lt;code&gt;Stan&lt;/code&gt;, it is very easy to create Posterior Predictive Distributions using &lt;code&gt;posterior_predict()&lt;/code&gt;. And that if we &lt;em&gt;have&lt;/em&gt; a posterior predictive distribution, incorporating uncertainty in various &amp;quot;marginal effects&amp;quot; type analyses becomes dead-easy. These analyses include what-if scenarios using the original data, or scenarios using new data with different covariate distributions (for example if we have an RCT that is enriched in young students, and we want to apply it to the general population). Ok, that it is for today, happy modelling!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Building TensorFlow 2.2 on an old PC</title>
      <link>/post/deep-learning-tensorflow-keras/</link>
      <pubDate>Fri, 15 May 2020 00:00:00 +0000</pubDate>
      <guid>/post/deep-learning-tensorflow-keras/</guid>
      <description>
&lt;link href=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/viz/viz.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/DiagrammeR-styles/styles.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/grViz-binding/grViz.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;I don&#39;t change computers often. The fun for me is to make the most out of sparse resources. Linux fits nicely into this philosophy, because it can be adapted to run on really tiny computers (e.g. &lt;a href=&#34;http://www.picotux.com/&#34; class=&#34;uri&#34;&gt;http://www.picotux.com/&lt;/a&gt;), as well as huge supercomputers (&lt;a href=&#34;https://itsfoss.com/linux-runs-top-supercomputers/&#34; class=&#34;uri&#34;&gt;https://itsfoss.com/linux-runs-top-supercomputers/&lt;/a&gt;). I do like to keep up with new tech developments. And with the commoditization of deep learning in the form of Keras, I felt it was about time that I finally jumped on the Deep Learning bandwagon.&lt;/p&gt;
&lt;p&gt;And the nice thing about lagging behind: The choice for deep learning is now extremely simple. I need &lt;a href=&#34;https://keras.io/&#34;&gt;Keras&lt;/a&gt; with &lt;a href=&#34;https://www.tensorflow.org/&#34;&gt;TensorFlow&lt;/a&gt; as a computational backend. Which nowadays means installing TensorFlow since the Keras API has been incorporated into the TensorFlow project.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tensorflow-and-avx&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;TensorFlow and AVX&lt;/h1&gt;
&lt;p&gt;Then I ran into a problem: TensorFlow is all about FAST computation. And therefore it tries to exploit all hardware features that speed up computation. One obvious way to do so is utilizing specialized hardware such as GPU&#39;s and TPU&#39;s to do the number crunching. But even for CPU&#39;s, TensorFlow likes to make use of all the computational features that modern CPU&#39;s offer. One of these is the &amp;quot;Advanced Vector Instruction Set&amp;quot; , aka &lt;a href=&#34;https://en.wikipedia.org/wiki/Advanced_Vector_Extensions&#34;&gt;AVX&lt;/a&gt;. As most CPU&#39;s from 2011 or later support AVX, the TensorFlow folks decided to only make binaries available that require a CPU with AVX. Bummer for me: as my CPU is from 2010, I needed to compile TensorFlow myself.&lt;/p&gt;
&lt;p&gt;But come to think of it: What better rite of passage into the Deep Learning AI age is to compile TensorFlow from source on your own machine??? (Opening music of Space Odyssey 2001 in the background)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;building-tensorflow-on-a-really-old-computer&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Building TensorFlow on a really old computer&lt;/h1&gt;
&lt;p&gt;I followed the &lt;a href=&#34;https://www.tensorflow.org/install/source&#34;&gt;tutorial from TensorFlow&lt;/a&gt; to build from source on a Linux system (Ubuntu 18.04 LTS). Therefore, these notes are most useful to other Linux users, and my future self of course.&lt;/p&gt;
&lt;p&gt;Roughly this consisted of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Creating a virtual environment for Python 3.6.9&lt;/li&gt;
&lt;li&gt;Checking my GCC version (7.5.0, which is greater than 7.3 that is used for the official TF packages)&lt;/li&gt;
&lt;li&gt;Clone the &lt;a href=&#34;https://github.com/tensorflow/tensorflow&#34;&gt;TensorFlow repository&lt;/a&gt; from GitHub&lt;/li&gt;
&lt;li&gt;Git checkout the latest official TensorFlow release (v2.2)&lt;/li&gt;
&lt;li&gt;Installed the latest release of &lt;a href=&#34;https://docs.bazel.build/versions/master/install-ubuntu.html#install-with-installer-ubuntu&#34;&gt;Bazel&lt;/a&gt; (Google&#39;s Make program), version 3.1. Then install exactly the right version needed for TF2.2 (2.0.0, as specified by MIN_BAZEL_VERSION in &lt;code&gt;tensorflow/configure.py&lt;/code&gt;, use &lt;code&gt;.baselversion&lt;/code&gt; to easily install multiple bazel versions side by side)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then came the hard part, the final step:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tweak Bazel arguments endlessly to reduce resource usage to be able to complete the build process succesfully&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the end, I removed the &lt;code&gt;-c opt&lt;/code&gt;, so no special optimization for my CPU. And asked for &lt;strong&gt;one CPU&lt;/strong&gt; (I have two cores :-), &lt;strong&gt;one job&lt;/strong&gt;, and &lt;strong&gt;max 2GB of RAM usage&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;cd tf_build_env/
source bin/activate
cd ~/Github/tensorflow/
bazel build --config=opt --local_ram_resources=2048 --local_cpu_resources=HOST_CPUS-1 --jobs=1
  //tensorflow/tools/pip_package:build_pip_package&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I ran the build process in a terminal on the Ubuntu 18.04 Desktop, without any other programs loaded. My 2010 PC has in total 4 GB of RAM. As the Ubuntu Desktop + OS consumes about 1-1.5 GB on my system, this leaves about 2.5-3.0 GB for bazel. Now as it turns out, according to &lt;code&gt;htop&lt;/code&gt; memory consumption went up to 3.6 GB (of my 3.9GB max), but it succeeded in the end. This was after 10 hours of compiling! (I let it run overnight)&lt;/p&gt;
&lt;p&gt;The final step was to turn the compiled TensorFlow into a Python Wheel package ready to install using &lt;code&gt;pip&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg
# creates a &amp;#39;wheel&amp;#39; file called tensorflow-2.2.0-cp36-cp36m-linux_x86_64.whl&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To try it out, I created a new empty Python 3 virtual environment with only TensorFlow and Jupyter Notebook installed. To my delight it ran the &lt;a href=&#34;https://www.tensorflow.org/tutorials/keras/classification&#34;&gt;Fashion MNIST classification with Keras&lt;/a&gt; example flawlessly.&lt;/p&gt;
&lt;p&gt;And even on my ancient PC performance was quite good, training the model took around 1 minute. So, after glorious succes in Python, it was time to move on to R.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;keras-in-r-with-the-classic-mnist&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Keras in R with the classic MNIST&lt;/h1&gt;
&lt;p&gt;I had to install the development version of the R package &lt;code&gt;keras&lt;/code&gt; from GitHub to fix a bug that prevented Keras in R from working with TF v2.2.&lt;/p&gt;
&lt;p&gt;From the release notes: (&lt;a href=&#34;https://github.com/rstudio/keras/blob/master/NEWS.md&#34; class=&#34;uri&#34;&gt;https://github.com/rstudio/keras/blob/master/NEWS.md&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Fixed issue regarding the KerasMetricsCallback with TF v2.2 (#1020)&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools::install_github(&amp;quot;rstudio/keras&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For my first deep learning in R, I followed the tutorial from &lt;a href=&#34;https://tensorflow.rstudio.com/tutorials/beginners/&#34; class=&#34;uri&#34;&gt;https://tensorflow.rstudio.com/tutorials/beginners/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;First load all the required packages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tensorflow)

use_virtualenv(&amp;quot;~/venvs/keras_env&amp;quot;, required = TRUE)
# this was the same environment that I tested TensorFlow with Python

library(keras)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Read in the dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mnist &amp;lt;- dataset_mnist()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Rescale pixel values to be between 0 and 1.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mnist$train$x &amp;lt;- mnist$train$x/255
mnist$test$x &amp;lt;- mnist$test$x/255&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Plot the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x_train &amp;lt;- mnist$train$x
y_train &amp;lt;- mnist$train$y

# visualize the digits
par(mfcol=c(6,6))
par(mar=c(0, 0, 3, 0), xaxs=&amp;#39;i&amp;#39;, yaxs=&amp;#39;i&amp;#39;)
for (idx in 1:12) { 
    im &amp;lt;- x_train[idx,,]
    im &amp;lt;- t(apply(im, 2, rev)) 
    image(1:28, 1:28, im, col=gray((0:255)/255), 
          xaxt=&amp;#39;n&amp;#39;, main=paste(y_train[idx]))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-15-deep_learning_keras_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;keras-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Keras model&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model &amp;lt;- keras_model_sequential() %&amp;gt;% 
  layer_flatten(input_shape = c(28, 28)) %&amp;gt;% 
  layer_dense(units = 128, activation = &amp;quot;relu&amp;quot;) %&amp;gt;% 
  layer_dropout(0.2) %&amp;gt;% 
  layer_dense(10, activation = &amp;quot;softmax&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Model: &amp;quot;sequential&amp;quot;
## ________________________________________________________________________________
## Layer (type)                        Output Shape                    Param #     
## ================================================================================
## flatten (Flatten)                   (None, 784)                     0           
## ________________________________________________________________________________
## dense_1 (Dense)                     (None, 128)                     100480      
## ________________________________________________________________________________
## dropout (Dropout)                   (None, 128)                     0           
## ________________________________________________________________________________
## dense (Dense)                       (None, 10)                      1290        
## ================================================================================
## Total params: 101,770
## Trainable params: 101,770
## Non-trainable params: 0
## ________________________________________________________________________________&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It has over 100.000 parameters!!&lt;/p&gt;
&lt;p&gt;Python has a nice &lt;code&gt;plot_model()&lt;/code&gt; function, in R we can use the &lt;code&gt;deepviz&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools::install_github(&amp;quot;andrie/deepviz&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(deepviz)
library(magrittr)

model %&amp;gt;% plot_model()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: `select_()` is deprecated as of dplyr 0.7.0.
## Please use `select()` instead.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_warnings()` to see where this warning was generated.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: The `x` argument of `as_tibble.matrix()` must have unique column names if `.name_repair` is omitted as of tibble 2.0.0.
## Using compatibility `.name_repair`.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_warnings()` to see where this warning was generated.&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:672px;height:480px;&#34; class=&#34;grViz html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;diagram&#34;:&#34;digraph {\n\ngraph [layout = \&#34;neato\&#34;,\n       outputorder = \&#34;edgesfirst\&#34;,\n       bgcolor = \&#34;white\&#34;]\n\nnode [fontname = \&#34;Helvetica\&#34;,\n      fontsize = \&#34;10\&#34;,\n      shape = \&#34;circle\&#34;,\n      fixedsize = \&#34;true\&#34;,\n      width = \&#34;0.5\&#34;,\n      style = \&#34;filled\&#34;,\n      fillcolor = \&#34;aliceblue\&#34;,\n      color = \&#34;gray70\&#34;,\n      fontcolor = \&#34;gray50\&#34;]\n\nedge [fontname = \&#34;Helvetica\&#34;,\n     fontsize = \&#34;8\&#34;,\n     len = \&#34;1.5\&#34;,\n     color = \&#34;gray80\&#34;,\n     arrowsize = \&#34;0.5\&#34;]\n\n  \&#34;1\&#34; [label = \&#34;flatten\nFlatten\n\&#34;, shape = \&#34;rectangle\&#34;, fixedsize = \&#34;FALSE\&#34;, fillcolor = \&#34;#F0F8FF\&#34;, fontcolor = \&#34;#000000\&#34;, pos = \&#34;0,4!\&#34;] \n  \&#34;2\&#34; [label = \&#34;dense_1\nDense\nrelu\&#34;, shape = \&#34;rectangle\&#34;, fixedsize = \&#34;FALSE\&#34;, fillcolor = \&#34;#F0F8FF\&#34;, fontcolor = \&#34;#000000\&#34;, pos = \&#34;0,3!\&#34;] \n  \&#34;3\&#34; [label = \&#34;dropout\nDropout\n\&#34;, shape = \&#34;rectangle\&#34;, fixedsize = \&#34;FALSE\&#34;, fillcolor = \&#34;#F0F8FF\&#34;, fontcolor = \&#34;#000000\&#34;, pos = \&#34;0,2!\&#34;] \n  \&#34;4\&#34; [label = \&#34;dense\nDense\nsoftmax\&#34;, shape = \&#34;rectangle\&#34;, fixedsize = \&#34;FALSE\&#34;, fillcolor = \&#34;#F0F8FF\&#34;, fontcolor = \&#34;#000000\&#34;, pos = \&#34;0,1!\&#34;] \n  \&#34;1\&#34;-&gt;\&#34;2\&#34; \n  \&#34;2\&#34;-&gt;\&#34;3\&#34; \n  \&#34;3\&#34;-&gt;\&#34;4\&#34; \n}&#34;,&#34;config&#34;:{&#34;engine&#34;:&#34;dot&#34;,&#34;options&#34;:null}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;/div&gt;
&lt;div id=&#34;compile-the-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Compile the model&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model %&amp;gt;% 
  compile(
    loss = &amp;quot;sparse_categorical_crossentropy&amp;quot;,
    optimizer = &amp;quot;adam&amp;quot;,
    metrics = &amp;quot;accuracy&amp;quot;
  )&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-the-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Fit the model&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model %&amp;gt;% 
  fit(
    x = mnist$train$x, 
    y = mnist$train$y,
    epochs = 5,
    validation_split = 0.3,
    verbose = 1
  )&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;make-predictions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Make predictions&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictions &amp;lt;- predict(model, mnist$test$x)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Visualize a single prediction:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)

id &amp;lt;- 9

ggplot(data.frame(digit = 0:9, prob = predictions[id,]), 
       aes(x = factor(digit), y = prob)) + geom_col() +
  ggtitle(paste0(&amp;quot;prediction for true value of &amp;quot;, mnist$test$y[id]))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-15-deep_learning_keras_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;check-model-performance-on-the-test-set&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Check model performance on the test set&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model %&amp;gt;% 
  evaluate(mnist$test$x, mnist$test$y, verbose = 0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       loss   accuracy 
## 0.08686701 0.97399998&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our model achieved ~98% accuracy on the test set.&lt;/p&gt;
&lt;p&gt;Awesome.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Simulating Fake Data in R</title>
      <link>/post/simulating-fake-data/</link>
      <pubDate>Sat, 26 Oct 2019 00:00:00 +0000</pubDate>
      <guid>/post/simulating-fake-data/</guid>
      <description>
&lt;link href=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;This blog post is on simulating fake data. I&#39;m interested in creating synthetic versions of real datasets. For example if the data is too sensitive to be shared, or we only have summary statistics available (for example tables from a published research paper).&lt;/p&gt;
&lt;p&gt;If we want to mimic an existing dataset, it is desirable to&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Make sure that the simulated variables have the proper data type and comparable distribution of values and&lt;/li&gt;
&lt;li&gt;correlations between the variables in the real dataset are taken into account.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In addition, it would be nice if such functionality is available in a standard R package. After reviewing several R packages that can simulate data, I picked the &lt;a href=&#34;https://www.rdatagen.net/page/simstudy/&#34;&gt;simstudy&lt;/a&gt; package as most promising to explore in more detail. &lt;code&gt;simstudy&lt;/code&gt; is created by &lt;strong&gt;Keith Goldfeld&lt;/strong&gt; from New York University.&lt;/p&gt;
&lt;p&gt;In this blog post, I explain how &lt;code&gt;simstudy&lt;/code&gt; is able to generate correlated variables, having either continuous or binary values. Along the way, we learn about fancy statistical slang such as copula&#39;s and tetrachoric correlations. It turns out there is a close connection with psychometrics, which we&#39;ll briefly discuss.&lt;/p&gt;
&lt;p&gt;Let&#39;s start with correlated continuous variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Loading required packages
library(simstudy)
library(data.table)
library(ggplot2)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;copulas-simulating-continuous-correlated-variables&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Copulas: Simulating continuous correlated variables&lt;/h1&gt;
&lt;p&gt;Copulas are a fancy word for correlated (&amp;quot;coupled&amp;quot;) variables that each have a uniform distribution between 0 and 1.&lt;/p&gt;
&lt;p&gt;Using copulas, we can convert correlated multivariate normal data to data from any known continuous probability distribution, while keeping exactly the same correlation matrix. The normal data is something we can easily simulate, and by choosing appropriate probability distributions, we can approximate the variables in real datasets.&lt;/p&gt;
&lt;p&gt;Ok let&#39;s do it!&lt;/p&gt;
&lt;div id=&#34;step-1-correlated-multivariate-normal-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 1: correlated multivariate normal data&lt;/h2&gt;
&lt;p&gt;The workhorse for our simulated data is a function to simulate multivariate normal data. We&#39;ll use the &lt;code&gt;MASS&lt;/code&gt; package function &lt;code&gt;mvrnorm()&lt;/code&gt;. Other slightly faster (factor 3-4) implementations exist, see e.g. &lt;code&gt;mvnfast&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The trick is to first generate multivariate normal data with the required correlation structure, with mean 0 and standard deviation 1. This gives us correlated data, where each variable is marginally (by itself) normal distributed.&lt;/p&gt;
&lt;p&gt;Here I simulate two variables, but the same procedure holds for N variables. The Pearson correlation is set at &lt;code&gt;0.7&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)

corr &amp;lt;- 0.7

cov.mat &amp;lt;- matrix(c(1, corr, 
                  corr, 1), nrow = 2)

df &amp;lt;- data.frame(MASS::mvrnorm(n = 1e4, 
                               mu = c(0, 0), 
                               Sigma = cov.mat))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(The diagonal of &lt;code&gt;1&lt;/code&gt; makes sure the variables have SD of 1. The off diagonal value of 0.7 gives us a Pearson correlation of 0.7)&lt;/p&gt;
&lt;p&gt;Did it work?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(df, aes(x = X1, y = X2)) +
  geom_point(alpha = 0.3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-31-simulating-fake-data-in-R_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(df$X1, df$X2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6985089&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Great!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-2-transform-variables-to-uniform-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 2: transform variables to uniform distribution&lt;/h2&gt;
&lt;p&gt;Using the normal cumulative distribution function &lt;code&gt;pnorm()&lt;/code&gt;, we can transform our normally distributed variables to have a uniform distribution, while keeping the correlation structure intact!!!!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df$X1_U &amp;lt;- pnorm(df$X1)
df$X2_U &amp;lt;- pnorm(df$X2)

ggplot(df, aes(x = X1_U)) + geom_histogram(boundary = 0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-31-simulating-fake-data-in-R_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(df, aes(x = X1_U, y = X2_U)) +
  geom_point(alpha = 0.3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-31-simulating-fake-data-in-R_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And here&#39;s our copula! Two variables, each marginally (by itself) uniform, but with pre-specified correlation intact!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(df$X1_U, df$X2_U)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.677868&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;step-3-from-uniform-to-any-standard-probability-distribution-we-like&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 3: from uniform to any standard probability distribution we like&lt;/h2&gt;
&lt;p&gt;Now, if we plug in uniformly distributed data in a &lt;strong&gt;quantile function&lt;/strong&gt; of any arbitrary (known) probability distribution, we can make the variables have any distribution we like.&lt;/p&gt;
&lt;p&gt;Let&#39;s pick for example a &lt;strong&gt;Gamma&lt;/strong&gt; distribution (Continuous, positive) with shape 4 and rate 1 for X1, and Let&#39;s pick a &lt;strong&gt;Normal&lt;/strong&gt; distribution (Continuous, symmetric) with mean 10 and sd 2 for X2.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df$X1_GAM &amp;lt;- qgamma(df$X1_U, shape = 4, rate =1)
df$X2_NORM &amp;lt;- qnorm(df$X2_U, mean = 10, sd = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(df, aes(x = X1_GAM)) + 
  geom_histogram(boundary = 0) +
  geom_vline(xintercept = 4, col = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-31-simulating-fake-data-in-R_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(df, aes(x = X2_NORM)) + 
  geom_histogram(boundary = 0) +
  geom_vline(xintercept = 10, col = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-31-simulating-fake-data-in-R_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Ok, that worked nicely. But what about their correlation?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(df$X1_GAM, df$X2_NORM)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.682233&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Whoa!! They still have (almost) the same correlation we started out with before all our transformation magic.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;simstudy-in-action&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Simstudy in action&lt;/h1&gt;
&lt;p&gt;Now let&#39;s see how &lt;code&gt;simstudy&lt;/code&gt; helps us generating this type of simulated data. Simstudy works with &amp;quot;definition tables&amp;quot; that allow us to specify, for each variable, which distribution and parameters to use, as well as the desired correlations between the variables.&lt;/p&gt;
&lt;p&gt;After specifing a definition table, we can call one of its workhorse functions &lt;code&gt;genCorFlex()&lt;/code&gt; to generate the data.&lt;/p&gt;
&lt;p&gt;N.b. Simstudy uses different parameters for the Gamma distribution, compared to R&#39;s &lt;code&gt;rgamma()&lt;/code&gt; function. Under water, it uses the &lt;code&gt;gammaGetShapeRate()&lt;/code&gt; to transform the &amp;quot;mean&amp;quot; and &amp;quot;variance/ dispersion&amp;quot; to the more conventional &amp;quot;shape&amp;quot; and &amp;quot;rate&amp;quot; parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)

corr &amp;lt;- 0.7

corr.mat &amp;lt;- matrix(c(1, corr, 
                  corr, 1), nrow = 2)

# check that gamma parameters correspond to same shape and rate pars as used above
#simstudy::gammaGetShapeRate(mean = 4, dispersion = 0.25)


def &amp;lt;- defData(varname = &amp;quot;X1_GAM&amp;quot;, 
               formula = 4, variance = 0.25, dist = &amp;quot;gamma&amp;quot;)

def &amp;lt;- defData(def, varname = &amp;quot;X2_NORM&amp;quot;, 
               formula = 10, variance = 2, dist = &amp;quot;normal&amp;quot;)



dt &amp;lt;- genCorFlex(1e4, def, corMatrix = corr.mat)

cor(dt[,-&amp;quot;id&amp;quot;])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            X1_GAM   X2_NORM
## X1_GAM  1.0000000 0.6823006
## X2_NORM 0.6823006 1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(dt, aes(x = X1_GAM)) + 
  geom_histogram(boundary = 0) +
  geom_vline(xintercept = 4, col = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-31-simulating-fake-data-in-R_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;generate-correlated-binary-variables&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Generate correlated binary variables&lt;/h1&gt;
&lt;p&gt;As it turns out, the copula approach does not work for binary variables. Well, it sort of works, but the correlations we get are lower than we actually specify.&lt;/p&gt;
&lt;p&gt;Come to think of it: two binary variables cannot have all the correlations we like. To see why, check this out.&lt;/p&gt;
&lt;div id=&#34;feasible-correlations-for-two-binary-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Feasible correlations for two binary variables&lt;/h2&gt;
&lt;p&gt;Let&#39;s suppose we have a binary variable that equals 1 with probability 0.2, and zero otherwise. This variable will never be fully correlated with a binary variable that equals 1 with probability 0.8, and zero otherwise.&lt;/p&gt;
&lt;p&gt;To see this, I created two binary vectors that have a fraction 0.2 and 0.8 of 1&#39;s, and let&#39;s see if we can arrange the values in both vectors in such a way that minimizes and maximizes their correlation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# maximal correlation
x1 &amp;lt;- c(0, 0, 0, 0, 1)
x2 &amp;lt;- c(0, 1, 1, 1, 1)

mean(x1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(x2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(x1, x2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.25&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# minimal correlation
x1 &amp;lt;- c(1, 0, 0, 0, 0)
x2 &amp;lt;- c(0, 1, 1, 1, 1)

cor(x1, x2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To get these vectors to be maximally correlated, we need to match &lt;code&gt;1&lt;/code&gt;&#39;s in &lt;code&gt;x1&lt;/code&gt; as much as possible with &lt;code&gt;1&lt;/code&gt;s in &lt;code&gt;x2&lt;/code&gt;. To get these vectors to be maximally anti-correlated, we need to match &lt;code&gt;1&lt;/code&gt;s in &lt;code&gt;x1&lt;/code&gt; with as many &lt;code&gt;0&lt;/code&gt;s in &lt;code&gt;x2&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In this example, we conclude that the feasible correlation range is &lt;code&gt;{-1, 0.25}&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;simstudy&lt;/code&gt; package contains a function to check for feasible boundaries, that contains this piece of code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1 &amp;lt;- 0.2
p2 &amp;lt;- 0.8

# lowest correlation
l &amp;lt;- (p1 * p2)/((1 - p1) * (1 - p2))

max(-sqrt(l), -sqrt(1/l))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# highest correlation
u &amp;lt;- (p1 * (1 - p2))/(p2 * (1 - p1))

min(sqrt(u), sqrt(1/u))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.25&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This confirms our example above.&lt;/p&gt;
&lt;p&gt;Note that if we want to mimic a real dataset with binary correlated variables, the correlations are a given, and are obviously all feasible because we obtain them from actual data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-model-for-two-correlated-binary-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A model for two correlated binary variables&lt;/h2&gt;
&lt;p&gt;Ok let&#39;s suppose we want a two binary vectors &lt;code&gt;B1&lt;/code&gt; and &lt;code&gt;B2&lt;/code&gt; , with means &lt;code&gt;p1 = 0.2&lt;/code&gt; and &lt;code&gt;p2 = 0.8&lt;/code&gt; and (feasible) Pearson correlation 0.1.&lt;/p&gt;
&lt;p&gt;How? How?&lt;/p&gt;
&lt;p&gt;The idea is that to get two binary variables to have an exact particular correlation, we imagine an underlying (&amp;quot;latent&amp;quot;) bivariate (2D) normal distribution. This normal distribution has the means fixed to 0, and the standard deviations fixed to 1.&lt;/p&gt;
&lt;p&gt;Why? Because a) we know it very well theoretically and b) we know how to simulate efficiently from such a distribution, using &lt;code&gt;mvrnorm()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In this bivariate normal distribution, we draw a quadrant (i.e. two thresholds). The thresholds define transformations to binary variables. Below the threshold, the binary value is 0, above it is 1. We have to pick the thresholds such that the resulting binary variables have the desired mean (i.e. percentage of 1&#39;s).&lt;/p&gt;
&lt;p&gt;This approach reduces the problem to finding the right values of three parameters: multivariate normal correlation, and the two thresholds (above, we already fixed the means and variance to zero and one respectively).&lt;/p&gt;
&lt;p&gt;For now, we&#39;ll just pick some value for the correlation in the bivariate normal, say 0.5, and focus on where to put the threshholds.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)

corr &amp;lt;- 0.5

cov.mat &amp;lt;- matrix(c(1, corr, 
                  corr, 1), nrow = 2)

df &amp;lt;- data.frame(MASS::mvrnorm(n = 10000, 
                               mu = c(0, 0), 
                               Sigma = cov.mat))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(The diagonal of &lt;code&gt;1&lt;/code&gt; makes sure the variables have SD of 1. The off diagonal value of 0.7 gives us a Pearson correlation of 0.7)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(df, aes(x = X1, y = X2)) +
  geom_point(alpha = 0.3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-31-simulating-fake-data-in-R_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Ok, where to put the thresholds? That&#39;s simple, we just need to use the &lt;code&gt;quantile distribution function&lt;/code&gt; to partition the marginal normal variables into 0 and 1 portions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df$B1 &amp;lt;- ifelse(df$X1 &amp;lt; qnorm(0.2), 1, 0)
df$B2 &amp;lt;- ifelse(df$X2 &amp;lt; qnorm(0.8), 1, 0)

mean(df$B1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.197&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(df$B2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7988&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&#39;s check it out visually:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(df, aes(x = X1, y = X2)) +
  geom_point(alpha = 0.3) + 
  geom_vline(xintercept = qnorm(0.2), col = &amp;quot;red&amp;quot;) +
  geom_hline(yintercept = qnorm(0.8), col = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-31-simulating-fake-data-in-R_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Nice.&lt;/p&gt;
&lt;p&gt;Ok, so now what is the correlation for these two binary variables?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(df$B1, df$B2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1877482&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ok, so if X1 and X2 have a correlation of 0.5, this results in a correlation of 0.19 between the binary variables B1 and B2.&lt;/p&gt;
&lt;p&gt;But we need B1 and B2 to have a correlation of 0.1!&lt;/p&gt;
&lt;p&gt;At this point, there is only one free parameter left, the correlation of the normally distributed variables &lt;code&gt;X1&lt;/code&gt; and &lt;code&gt;X2&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We could of course manually try to find which correlation we must choose between &lt;code&gt;X1&lt;/code&gt; and &lt;code&gt;X2&lt;/code&gt; to get the desired correlation of 0.1 in the binary variables. But that would be very unpractical.&lt;/p&gt;
&lt;p&gt;Fortunately, Emrich and Piedmonte (1991) published an iterative method to solve this puzzle. And this method has been implemented in &lt;code&gt;simstudy&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;simstudy:::.findRhoBin(p1 = 0.2, 
                       p2 = 0.8, d = 0.1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2218018&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&#39;s see if it works:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)

corr &amp;lt;- 0.2218018

cov.mat &amp;lt;- matrix(c(1, corr, 
                  corr, 1), nrow = 2)

df &amp;lt;- data.frame(MASS::mvrnorm(n = 1e6, 
                               mu = c(0, 0), 
                               Sigma = cov.mat))

df$B1 &amp;lt;- ifelse(df$X1 &amp;lt; qnorm(0.2), 1, 0)
df$B2 &amp;lt;- ifelse(df$X2 &amp;lt; qnorm(0.8), 1, 0)

cor(df$B1, df$B2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.09957392&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Great!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;relation-to-psychometrics&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Relation to psychometrics&lt;/h1&gt;
&lt;p&gt;So what has psychometrics to do with all this simulation of correlated binary vector stuff?&lt;/p&gt;
&lt;p&gt;Well, psychometrics is all about theorizing about unobserved, latent, imaginary &amp;quot;constructs&amp;quot;, such as &lt;strong&gt;attitude&lt;/strong&gt;, &lt;strong&gt;general intelligence&lt;/strong&gt; or a &lt;strong&gt;personality trait&lt;/strong&gt;. To measure these constructs, questionnaires are used. The questions are called &lt;strong&gt;items&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Now imagine a situation where we are interested in a particular construct, say &lt;strong&gt;general intelligence&lt;/strong&gt;, and we design two questions to measure (hope to learn more about) the construct. Furthermore, assume that one question is more difficult than the other question. The answers to both questions can either be wrong or right.&lt;/p&gt;
&lt;p&gt;We can model this by assuming that the (imaginary) variable &amp;quot;intelligence&amp;quot; of each respondent is located on a two-dimensional plane, with the distribution of the respondents determined by a bivariate normal distribution. Dividing this plane into four quadrants then gives us the measurable answers (right or wrong) to both questions. Learning the answers to both questions then gives us an approximate location of a respondent on our &amp;quot;intelligence&amp;quot; plane!&lt;/p&gt;
&lt;div id=&#34;phi-tetrachoric-correlation-and-the-psych-package&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Phi, tetrachoric correlation and the psych package&lt;/h2&gt;
&lt;p&gt;Officially, the Pearson correlation between two binary vectors is called the &lt;a href=&#34;https://en.wikipedia.org/wiki/Phi_coefficient&#34;&gt;Phi coefficient&lt;/a&gt;. This name was actually chosen by Karl Pearson himself.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;psych&lt;/strong&gt; packages contains a set of convenient functions for calculating Phi coefficients from empirical two by two tables (of two binary vectors), and finding the corresponding Pearson coefficient for the 2d (latent) normal. This coefficient is called the &lt;strong&gt;tetrachoric correlation&lt;/strong&gt;. Again a fine archaic slang word for again a basic concept.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(psych)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;psych&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:ggplot2&amp;#39;:
## 
##     %+%, alpha&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# convert simulated binary vectors B1 and B2 to 2x2 table
twobytwo &amp;lt;- table(df$B1, df$B2)/nrow(df)

phi(twobytwo, digits = 6)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.099574&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(df$B1, df$B2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.09957392&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# both give the same result&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can use &lt;strong&gt;phi2tetra&lt;/strong&gt; to find the tetrachoric correlation that corresponds to the combination of a &amp;quot;Phi coefficient&amp;quot;, i.e. the correlation between the two binary vectors, as well as their marginals. This is a wrapper that builds the two by two frequency table and then calls &lt;code&gt;tetrachoric()&lt;/code&gt; . This in turn uses &lt;code&gt;optimize&lt;/code&gt; (Maximum Likelihood method?) to find the tetrachoric correlation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;phi2tetra(0.1, c(0.2, 0.8))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2217801&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# compare with EP method
simstudy:::.findRhoBin(0.2, 0.8, 0.1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2218018&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Comparing with the Emrich and Piedmonte method, we find that they give identical answers. Great, case closed!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;simstudy-in-action-ii&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Simstudy in action II&lt;/h1&gt;
&lt;p&gt;Now that we feel confident in our methods and assumptions, let&#39;s see &lt;code&gt;simstudy&lt;/code&gt; in action.&lt;/p&gt;
&lt;p&gt;Let&#39;s generate two binary variables, that have marginals of 20% and 80% respectively, and a Pearson correlation coefficient of 0.1.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)
corr &amp;lt;- 0.1

corr.mat &amp;lt;- matrix(c(1, corr, 
                  corr, 1), nrow = 2)

res &amp;lt;- simstudy::genCorGen(10000, nvars = 2, 
                 params1 = c(0.2, 0.8),
                 corMatrix = corr.mat,
                 dist = &amp;quot;binary&amp;quot;, 
                 method = &amp;quot;ep&amp;quot;, wide = TRUE)

# let&amp;#39;s check the result
cor(res[, -c(&amp;quot;id&amp;quot;)])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            V1         V2
## V1 1.00000000 0.09682531
## V2 0.09682531 1.00000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Awesome, it worked!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;Recall, my motivation for simulating fake data with particular variable types and correlation structure is to mimic real datasets.&lt;/p&gt;
&lt;p&gt;So are we there yet? Well, we made some progress. We now can handle correlated continuous data, as well as correlated binary data.&lt;/p&gt;
&lt;p&gt;But we need to solve two more problems:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;To simulate a particular dataset, we still need to determine for each variable its data type (binary or continuous), and if it&#39;s continuous, what is the most appropriate probability distribution (Normal, Gamma, Log-normal, etc).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;we haven&#39;t properly solved correlation between dissimilar data types, e.g. a correlation between a continuous and a binary variable.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Judging from the literature (Amatya &amp;amp; Demirtas 2016) and packages such as &lt;code&gt;SimMultiCorrData&lt;/code&gt; by Allison Fialkowski, these are both solved, and I only need to learn about them! So, to be continued.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Exploring Process Mining in R</title>
      <link>/post/exploring-process-mining/</link>
      <pubDate>Thu, 20 Jun 2019 00:00:00 +0000</pubDate>
      <guid>/post/exploring-process-mining/</guid>
      <description>
&lt;link href=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;In this post, we&#39;ll explore the &lt;a href=&#34;https://www.bupar.net&#34;&gt;BupaR&lt;/a&gt; suite of &lt;em&gt;Process Mining&lt;/em&gt; packages created by &lt;em&gt;Gert Janssenswillen&lt;/em&gt; from Hasselt University.&lt;/p&gt;
&lt;p&gt;We start with exploring the &lt;code&gt;patients&lt;/code&gt; dataset contained in the &lt;code&gt;eventdataR&lt;/code&gt; package. According to the documentation, this is an &amp;quot;Artifical eventlog about patients&amp;quot;.&lt;/p&gt;
&lt;div id=&#34;getting-started&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Getting started&lt;/h1&gt;
&lt;p&gt;After installing all required packages, we can load the whole &amp;quot;bupaverse&amp;quot; by loading the &lt;code&gt;bupaR&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(bupaR)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in library(package, lib.loc = lib.loc, character.only = TRUE,
## logical.return = TRUE, : there is no package called &amp;#39;xesreadR&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in library(package, lib.loc = lib.loc, character.only = TRUE,
## logical.return = TRUE, : there is no package called &amp;#39;processmonitR&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in library(package, lib.loc = lib.loc, character.only = TRUE,
## logical.return = TRUE, : there is no package called &amp;#39;petrinetR&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(processmapR)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, our dataset is already in &lt;code&gt;eventlog&lt;/code&gt; format, but typically this not the case. Here&#39;s how to turn a data.frame into an object of class &lt;code&gt;eventlog&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;patients &amp;lt;- eventdataR::patients

df &amp;lt;- eventlog(patients,
               case_id = &amp;quot;patient&amp;quot;,
               activity_id = &amp;quot;handling&amp;quot;,
               activity_instance_id = &amp;quot;handling_id&amp;quot;,
               lifecycle_id = &amp;quot;registration_type&amp;quot;,
               timestamp = &amp;quot;time&amp;quot;,
               resource_id = &amp;quot;employee&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: The `add` argument of `group_by()` is deprecated as of dplyr 1.0.0.
## Please use the `.add` argument instead.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_warnings()` to see where this warning was generated.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&#39;s check it out.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Number of events:  5442
## Number of cases:  500
## Number of traces:  7
## Number of distinct activities:  7
## Average trace length:  10.884
## 
## Start eventlog:  2017-01-02 11:41:53
## End eventlog:  2018-05-05 07:16:02&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                   handling      patient          employee  handling_id       
##  Blood test           : 474   Length:5442        r1:1000   Length:5442       
##  Check-out            : 984   Class :character   r2:1000   Class :character  
##  Discuss Results      : 990   Mode  :character   r3: 474   Mode  :character  
##  MRI SCAN             : 472                      r4: 472                     
##  Registration         :1000                      r5: 522                     
##  Triage and Assessment:1000                      r6: 990                     
##  X-Ray                : 522                      r7: 984                     
##  registration_type      time                         .order    
##  complete:2721     Min.   :2017-01-02 11:41:53   Min.   :   1  
##  start   :2721     1st Qu.:2017-05-06 17:15:18   1st Qu.:1361  
##                    Median :2017-09-08 04:16:50   Median :2722  
##                    Mean   :2017-09-02 20:52:34   Mean   :2722  
##                    3rd Qu.:2017-12-22 15:44:11   3rd Qu.:4082  
##                    Max.   :2018-05-05 07:16:02   Max.   :5442  
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we learn that there are 500 &amp;quot;cases&amp;quot;, i.e. patients. There are 7 different activities.&lt;/p&gt;
&lt;p&gt;Let&#39;s check out the data for a single patient:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df %&amp;gt;% filter(patient == 1) %&amp;gt;% 
  arrange(handling_id) #%&amp;gt;% &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Log of 12 events consisting of:
## 1 trace 
## 1 case 
## 6 instances of 6 activities 
## 6 resources 
## Events occurred from 2017-01-02 11:41:53 until 2017-01-09 19:45:45 
##  
## Variables were mapped as follows:
## Case identifier:     patient 
## Activity identifier:     handling 
## Resource identifier:     employee 
## Activity instance identifier:    handling_id 
## Timestamp:           time 
## Lifecycle transition:        registration_type 
## 
## # A tibble: 12 x 7
##    handling patient employee handling_id registration_ty… time               
##    &amp;lt;fct&amp;gt;    &amp;lt;chr&amp;gt;   &amp;lt;fct&amp;gt;    &amp;lt;chr&amp;gt;       &amp;lt;fct&amp;gt;            &amp;lt;dttm&amp;gt;             
##  1 Registr… 1       r1       1           start            2017-01-02 11:41:53
##  2 Registr… 1       r1       1           complete         2017-01-02 12:40:20
##  3 Blood t… 1       r3       1001        start            2017-01-05 08:59:04
##  4 Blood t… 1       r3       1001        complete         2017-01-05 14:34:27
##  5 MRI SCAN 1       r4       1238        start            2017-01-05 21:37:12
##  6 MRI SCAN 1       r4       1238        complete         2017-01-06 01:54:23
##  7 Discuss… 1       r6       1735        start            2017-01-07 07:57:49
##  8 Discuss… 1       r6       1735        complete         2017-01-07 10:18:08
##  9 Check-o… 1       r7       2230        start            2017-01-09 17:09:43
## 10 Check-o… 1       r7       2230        complete         2017-01-09 19:45:45
## 11 Triage … 1       r2       501         start            2017-01-02 12:40:20
## 12 Triage … 1       r2       501         complete         2017-01-02 22:32:25
## # … with 1 more variable: .order &amp;lt;int&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt; # select(handling, handling_id, registration_type) # does not work&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We learn that each &amp;quot;handling&amp;quot; has a separate start and complete timestamp.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;traces&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Traces&lt;/h1&gt;
&lt;p&gt;The summary info of the event log also counts so-called &amp;quot;traces&amp;quot;. A trace is defined a unique sequence of events in the event log. Apparently, there are only seven different traces (possible sequences). Let&#39;s visualize them.&lt;/p&gt;
&lt;p&gt;To visualize all traces, we set &lt;code&gt;coverage&lt;/code&gt; to 1.0.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df %&amp;gt;% processmapR::trace_explorer(type = &amp;quot;frequent&amp;quot;, coverage = 1.0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: `rename_()` is deprecated as of dplyr 0.7.0.
## Please use `rename()` instead.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_warnings()` to see where this warning was generated.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-23-exploring-process-mining_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt; So there are a few traces (0.6%) that do not end with a check-out. Ignoring these rare cases, we find that there are two types of cases:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cases that get an X-ray&lt;/li&gt;
&lt;li&gt;Cases that get a blood test followed by an MRI scan&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;the-dotted-chart&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The dotted chart&lt;/h1&gt;
&lt;p&gt;A really powerful visualization in process mining comes in the form of a &amp;quot;dotted chart&amp;quot;. The dotted chart function produces a &lt;code&gt;ggplot&lt;/code&gt; graph, which is nice, because so we can actually tweak the graph as we can with regular ggplot objects.&lt;/p&gt;
&lt;p&gt;It has two nice use cases. The first is when we plot actual time on the x-axis, and sort the cases by starting date.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df %&amp;gt;% dotted_chart(x = &amp;quot;absolute&amp;quot;, sort = &amp;quot;start&amp;quot;) + ggtitle(&amp;quot;All cases&amp;quot;) +
  theme_gray()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Joining, by = &amp;quot;patient&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-23-exploring-process-mining_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The slope of this graphs learns us the rate of new cases, and if this changes over time. Here it appears constant, with 500 cases divided over five quarter years.&lt;/p&gt;
&lt;p&gt;The second is to align all cases relative to the first event, and sort on duration of the whole sequence of events.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df %&amp;gt;% dotted_chart(x = &amp;quot;relative&amp;quot;, sort = &amp;quot;duration&amp;quot;) + ggtitle(&amp;quot;All cases&amp;quot;) +
  theme_gray()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Joining, by = &amp;quot;patient&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-23-exploring-process-mining_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A nice pattern emerges, where all cases start with registration, then quickly proceed to triage and assessment, after that, a time varying period of 1-10 days follows where either the blood test + MRI scan, or the X-ray is performed, followed by discussing the results. Finally, check out occurs.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;To conclude, the process mining approach to analyze time series event data appears highly promising. The dotted chart is a great addition to my data visualization repertoire, and the process mining folks appear to have at lot more goodies, such as Trace Alignment.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Designing an introductory course on Causal Inference</title>
      <link>/post/causal-inference-course/</link>
      <pubDate>Sat, 11 May 2019 00:00:00 +0000</pubDate>
      <guid>/post/causal-inference-course/</guid>
      <description>
&lt;link href=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;(Short intro) This is me learning causal inference (CI) by self-study together with colleagues using online resources.&lt;/p&gt;
&lt;p&gt;(Longer intro) A modern data scientist needs to become skilled in at least three topics (I left out visualization):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(Bayesian) Statistical modeling&lt;/li&gt;
&lt;li&gt;Machine Learning&lt;/li&gt;
&lt;li&gt;Causal inference&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For the first two topics, great introductory books exist that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;focus on learning-by-doing and&lt;/li&gt;
&lt;li&gt;are low on math and high on simulation / programming in R&lt;/li&gt;
&lt;li&gt;are fun / well written&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For Bayesian statistical modeling, we have the awesome textbook &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;&amp;quot;Statistical Rethinking&amp;quot;&lt;/a&gt; by Richard mcElreath.&lt;/p&gt;
&lt;p&gt;For Machine Learning, we have the (free) book &lt;a href=&#34;http://www-bcf.usc.edu/~gareth/ISL/&#34;&gt;&amp;quot;Introduction to Statistical Learning&amp;quot;&lt;/a&gt; by James, Witten, Hastie &amp;amp; Tibshirani.&lt;/p&gt;
&lt;p&gt;However, for Causal Inference, such a book does not exist yet AFAIK. Therefore, I tried to piece together a Causal Inference course based on the criteria mentioned above.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;designing-an-introductory-causal-inference-course&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Designing an introductory causal inference course&lt;/h1&gt;
&lt;p&gt;Explicit goal was to contrast/combine the causal graph (DAG) approach with what some call &amp;quot;Quasi-experimental designs&amp;quot;, i.e. the econometric causal effects toolkit (Regression Discontinuity Design, matching, instrumental variables etc).&lt;/p&gt;
&lt;p&gt;In the end, I decided to combine the two causal chapters from Gelman &amp;amp; Hill (2007) &lt;a href=&#34;https://statmodeling.stat.columbia.edu/2007/12/08/causal_inferenc_2/&#34;&gt;(freely available on Gelman&#39;s website)&lt;/a&gt; with the introductory chapter on Causal Graphical Models by Felix Elwert &lt;a href=&#34;https://www.ssc.wisc.edu/~felwert/causality/&#34;&gt;(freely available on Elwert&#39;s website)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The Gelman &amp;amp; Hill chapters already come with a set of exercises. However, for DAGs, i could not find a suitable set of exercises.&lt;/p&gt;
&lt;p&gt;So I created two R markdown notebooks with exercises in R, that make use of the &lt;a href=&#34;http://dagitty.net/&#34;&gt;DAGitty tool&lt;/a&gt;, created by Johannes Textor and freely available as R package.&lt;/p&gt;
&lt;p&gt;Some exercises are taken from &lt;a href=&#34;http://bayes.cs.ucla.edu/PRIMER/&#34;&gt;Causal inference in statistics: A Primer&lt;/a&gt; by Pearl, Glymour &amp;amp; Jewell. (I probably should own this book. So I just ordered it :))&lt;/p&gt;
&lt;p&gt;All materials are available in a &lt;a href=&#34;https://github.com/gsverhoeven/causal_course&#34;&gt;GitHub repository&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;outline-of-the-course&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Outline of the course&lt;/h1&gt;
&lt;p&gt;The course has four parts.&lt;/p&gt;
&lt;div id=&#34;general-introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;General introduction&lt;/h2&gt;
&lt;p&gt;The course starts with the first causal chapter of Gelman &amp;amp; Hill&#39;s book, &amp;quot;Causal inference using regression on the treatment variable&amp;quot;. This creates a first complete experience with identifying and estimating causal effects. However, there are no causal diagrams, which is unfortunate.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;identification-of-causal-effects-using-dags&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Identification of Causal effects using DAGs&lt;/h2&gt;
&lt;p&gt;Next we dive into causal identification using the causal diagram approach. For this we use the chapter &amp;quot;Causal Graphical Models&amp;quot; by Felix Elwert. Two R markdown Notebooks with exercises using Dagitty complete this part.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;identification-and-estimation-strategies&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Identification and estimation strategies&lt;/h2&gt;
&lt;p&gt;We then continue with the second causal chapter of Gelman &amp;amp; Hill &amp;quot;Causal inference using more advanced models&amp;quot;. This covers matching, regression discontinuity design, and instrumental variables. This material is combined with a paper by Scheiner et al, that contains DAGs for these methods. In our study group DAGs greatly facilitated discussion of the various designs.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;varying-treatment-effects-using-machine-learning&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Varying treatment effects using Machine Learning&lt;/h2&gt;
&lt;p&gt;Finally, and this part of the course has yet to take place, is the topic of estimating heterogeneous (i.e. subgroup, or even individual) treatment effects. This covers recent developements based on (forests of) regression trees. The plan is to cover both bayesian (BART, Chipman &amp;amp; mcCullough) and non-bayesian (GRF, Athey &amp;amp; Wager) methods.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;looking-back-so-far&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Looking back so far&lt;/h1&gt;
&lt;p&gt;The causal diagram / DAG approach is nonparametric and its purpose is to&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Make assumptions on the data generating process explicit&lt;/li&gt;
&lt;li&gt;Formalize identification of causal effects&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thus, it is separate from, and complements statistical estimation. The distinction between identification and estimation is not so explicitly made in the Gelman &amp;amp; Hill chapters, at least this is my impression. It would really benefit from adding DAGs, as Richard mcElreath is doing in his upcoming second edition of Statistical Rethinking.&lt;/p&gt;
&lt;p&gt;After having worked through these materials, I think reading Shalizi&#39;s chapters on Causal Effects would be a smart move. This is part III of his book &lt;a href=&#34;https://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/&#34;&gt;&amp;quot;Advanced Data Analysis from an Elementary Point of View&amp;quot;&lt;/a&gt;, which is awesome in its clarity, practical remarks a.k.a. normative statements by the author, and breadth.&lt;/p&gt;
&lt;p&gt;If you have a question, would like to comment or share ideas feel free to &lt;a href=&#34;https://gsverhoeven.github.io/#contact&#34;&gt;contact me&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The validation set approach in caret</title>
      <link>/post/validation-set-approach-in-caret/</link>
      <pubDate>Thu, 21 Mar 2019 00:00:00 +0000</pubDate>
      <guid>/post/validation-set-approach-in-caret/</guid>
      <description>
&lt;link href=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;In this blog post, we explore how to implement the &lt;em&gt;validation set approach&lt;/em&gt; in &lt;code&gt;caret&lt;/code&gt;. This is the most basic form of the train/test machine learning concept. For example, the classic machine learning textbook &lt;a href=&#34;http://www-bcf.usc.edu/~gareth/ISL/&#34;&gt;&amp;quot;An introduction to Statistical Learning&amp;quot;&lt;/a&gt; uses the validation set approach to introduce resampling methods.&lt;/p&gt;
&lt;p&gt;In practice, one likes to use k-fold Cross validation, or Leave-one-out cross validation, as they make better use of the data. This is probably the reason that the validation set approach is not one of &lt;code&gt;caret&lt;/code&gt;&#39;s preset methods.&lt;/p&gt;
&lt;p&gt;But for teaching purposes it would be very nice to have a &lt;code&gt;caret&lt;/code&gt; implementation.&lt;/p&gt;
&lt;p&gt;This would allow for an easy demonstration of the variability one gets when choosing different partionings. It also allows direct demonstration of why k-fold CV is superior to the validation set approach with respect to bias/variance.&lt;/p&gt;
&lt;p&gt;We pick the &lt;code&gt;BostonHousing&lt;/code&gt; dataset for our example code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Boston Housing 
knitr::kable(head(Boston))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;crim&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;zn&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;indus&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;chas&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;nox&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;rm&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;age&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;dis&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;rad&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;tax&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;ptratio&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;black&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;lstat&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;medv&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.00632&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;18&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.31&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.538&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.575&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;65.2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.0900&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;296&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;15.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;396.90&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.98&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;24.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.02731&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.07&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.469&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.421&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;78.9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.9671&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;242&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17.8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;396.90&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9.14&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;21.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.02729&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.07&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.469&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.185&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;61.1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.9671&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;242&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17.8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;392.83&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.03&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;34.7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.03237&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.18&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.458&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.998&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;45.8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.0622&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;222&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;18.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;394.63&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.94&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;33.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.06905&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.18&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.458&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.147&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;54.2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.0622&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;222&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;18.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;396.90&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.33&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;36.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.02985&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.18&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.458&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.430&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;58.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.0622&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;222&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;18.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;394.12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.21&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;28.7&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Our model is predicting &lt;code&gt;medv&lt;/code&gt; (Median house value) using predictors &lt;code&gt;indus&lt;/code&gt; and &lt;code&gt;chas&lt;/code&gt; in a multiple linear regression. We split the data in half, 50% for fitting the model, and 50% to use as a validation set.&lt;/p&gt;
&lt;div id=&#34;stratified-sampling-vs-random-sampling&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Stratified sampling vs random sampling&lt;/h1&gt;
&lt;p&gt;To check if we understand what &lt;code&gt;caret&lt;/code&gt; does, we first implement the validation set approach ourselves. To be able to compare, we need exactly the same data partitions for our manual approach and the &lt;code&gt;caret&lt;/code&gt; approach. As &lt;code&gt;caret&lt;/code&gt; requires a particular format (a named list of sets of train indices) we conform to this standard. However, all &lt;code&gt;caret&lt;/code&gt; partitioning functions seem to perform &lt;strong&gt;stratified random sampling&lt;/strong&gt;. This means that it first partitions the data in equal sized groups based on the outcome variable, and then samples at random &lt;strong&gt;within those groups&lt;/strong&gt; to partitions that have similar distributions for the outcome variable.&lt;/p&gt;
&lt;p&gt;This not desirable for teaching, as it adds more complexity. In addition, it would be nice to be able to compare stratified vs. random sampling.&lt;/p&gt;
&lt;p&gt;We therefore write a function that generates truly random partitions of the data. We let it generate partitions in the format that &lt;code&gt;trainControl&lt;/code&gt; likes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# internal function from caret package, needed to play nice with resamples()
prettySeq &amp;lt;- function(x) paste(&amp;quot;Resample&amp;quot;, gsub(&amp;quot; &amp;quot;, &amp;quot;0&amp;quot;, format(seq(along = x))), sep = &amp;quot;&amp;quot;)

createRandomDataPartition &amp;lt;- function(y, times, p) {
  vec &amp;lt;- 1:length(y)
  n_samples &amp;lt;- round(p * length(y))
  
  result &amp;lt;- list()
  for(t in 1:times){
    indices &amp;lt;- sample(vec, n_samples, replace = FALSE)
    result[[t]] &amp;lt;- indices
    #names(result)[t] &amp;lt;- paste0(&amp;quot;Resample&amp;quot;, t)
  }
  names(result) &amp;lt;- prettySeq(result)
  result
}

createRandomDataPartition(1:10, times = 2, p = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $Resample1
## [1]  4  3  7  9 10
## 
## $Resample2
## [1]  8  6  1  7 10&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-validation-set-approach-without-caret&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The validation set approach without caret&lt;/h1&gt;
&lt;p&gt;Here is the validation set approach without using caret. We create a single random partition of the data in train and validation set, fit the model on the training data, predict on the validation data, and calculate the RMSE error on the test predictions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)
parts &amp;lt;- createRandomDataPartition(Boston$medv, times = 1, p = 0.5)

train &amp;lt;- parts$Resample1

# fit ols on train data
lm.fit &amp;lt;- lm(medv ~ indus + chas , data = Boston[train,])

# predict on held out data
preds &amp;lt;- predict(lm.fit, newdata = Boston[-train,])

# calculate RMSE validation error
sqrt(mean((preds - Boston[-train,]$medv)^2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 7.930076&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we feed &lt;code&gt;caret&lt;/code&gt; the same data partition, we expect &lt;em&gt;exactly&lt;/em&gt; the same test error for the held-out data. Let&#39;s find out!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-validation-set-approach-in-caret&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The validation set approach in caret&lt;/h1&gt;
&lt;p&gt;Now we use the &lt;code&gt;caret&lt;/code&gt; package. Regular usage requires two function calls, one to &lt;code&gt;trainControl&lt;/code&gt; to control the resampling behavior, and one to &lt;code&gt;train&lt;/code&gt; to do the actual model fitting and prediction generation.&lt;/p&gt;
&lt;p&gt;As the validation set approach is not one of the predefined methods, we need to make use of the &lt;code&gt;index&lt;/code&gt; argument to explicitely define the train partitions outside of &lt;code&gt;caret&lt;/code&gt;. It automatically predicts on the records that are not contained in the train partitions.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;index&lt;/code&gt; argument plays well with the &lt;code&gt;createDataPartition&lt;/code&gt; (Stratfied sampling) and &lt;code&gt;createRandomDataPartition&lt;/code&gt; (our own custom function that performs truly random sampling) functions, as these functions both generate partitions in precisely the format that &lt;code&gt;index&lt;/code&gt; wants: lists of training set indices.&lt;/p&gt;
&lt;p&gt;In the code below, we generate four different 50/50 partitions of the data.&lt;/p&gt;
&lt;p&gt;We set &lt;code&gt;savePredictions&lt;/code&gt; to &lt;code&gt;TRUE&lt;/code&gt; to be able to verify the calculated metrics such as the test RMSE.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)

# create four partitions
parts &amp;lt;- createRandomDataPartition(Boston$medv, times = 4, p = 0.5)

ctrl &amp;lt;- trainControl(method = &amp;quot;repeatedcv&amp;quot;, 
                     ## The method doesn&amp;#39;t matter
                     ## since we are defining the resamples
                     index= parts, 
                     ##verboseIter = TRUE, 
                     ##repeats = 1,
                     savePredictions = TRUE
                     ##returnResamp = &amp;quot;final&amp;quot;
                     ) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can run caret and fit the model four times:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res &amp;lt;- train(medv ~ indus + chas, data = Boston, method = &amp;quot;lm&amp;quot;,
             trControl = ctrl)

res&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear Regression 
## 
## 506 samples
##   2 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 1 times) 
## Summary of sample sizes: 253, 253, 253, 253 
## Resampling results:
## 
##   RMSE      Rsquared   MAE     
##   7.906538  0.2551047  5.764773
## 
## Tuning parameter &amp;#39;intercept&amp;#39; was held constant at a value of TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the result returned by &lt;code&gt;train&lt;/code&gt; we can verify that it has fitted a model on four different datasets, each of size &lt;code&gt;253&lt;/code&gt;. By default it reports the average test error over the four validation sets. We can also extract the four individual test errors:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# strangely enough, resamples() always wants at least two train() results
# see also the man page for resamples()
resamples &amp;lt;- resamples(list(MOD1 = res, 
                            MOD2 = res))

resamples$values$`MOD1~RMSE`&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 7.930076 8.135428 7.899054 7.661595&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# check that we recover the RMSE reported by train() in the Resampling results
mean(resamples$values$`MOD1~RMSE`)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 7.906538&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(resamples)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## summary.resamples(object = resamples)
## 
## Models: MOD1, MOD2 
## Number of resamples: 4 
## 
## MAE 
##          Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA&amp;#39;s
## MOD1 5.516407 5.730172 5.809746 5.764773 5.844347 5.923193    0
## MOD2 5.516407 5.730172 5.809746 5.764773 5.844347 5.923193    0
## 
## RMSE 
##          Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA&amp;#39;s
## MOD1 7.661595 7.839689 7.914565 7.906538 7.981414 8.135428    0
## MOD2 7.661595 7.839689 7.914565 7.906538 7.981414 8.135428    0
## 
## Rsquared 
##           Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA&amp;#39;s
## MOD1 0.2339796 0.2377464 0.2541613 0.2551047 0.2715197 0.2781167    0
## MOD2 0.2339796 0.2377464 0.2541613 0.2551047 0.2715197 0.2781167    0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the RMSE value for the first train/test partition is exactly equal to our own implementation of the validation set approach. Awesome.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;validation-set-approach-stratified-sampling-versus-random-sampling&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Validation set approach: stratified sampling versus random sampling&lt;/h1&gt;
&lt;p&gt;Since we now know what we are doing, let&#39;s perform a simulation study to compare stratified random sampling with truly random sampling, using the validation set approach, and repeating this proces say a few thousand times to get a nice distribution of test errors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# simulation settings
n_repeats &amp;lt;- 3000
train_fraction &amp;lt;- 0.8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First we fit the models on the random sampling data partitions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)
parts &amp;lt;- createRandomDataPartition(Boston$medv, times = n_repeats, p = train_fraction)

ctrl &amp;lt;- trainControl(method = &amp;quot;repeatedcv&amp;quot;,  ## The method doesn&amp;#39;t matter
                     index= parts, 
                     savePredictions = TRUE
                     ) 

rand_sampl_res &amp;lt;- train(medv ~ indus + chas, data = Boston, method = &amp;quot;lm&amp;quot;,
             trControl = ctrl)

rand_sampl_res&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear Regression 
## 
## 506 samples
##   2 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 1 times) 
## Summary of sample sizes: 405, 405, 405, 405, 405, 405, ... 
## Resampling results:
## 
##   RMSE      Rsquared   MAE     
##   7.868972  0.2753001  5.790874
## 
## Tuning parameter &amp;#39;intercept&amp;#39; was held constant at a value of TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we fit the models on the stratified sampling data partitions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)
parts &amp;lt;- createDataPartition(Boston$medv, times = n_repeats, p = train_fraction, list = T)

ctrl &amp;lt;- trainControl(method = &amp;quot;repeatedcv&amp;quot;,  ## The method doesn&amp;#39;t matter
                     index= parts, 
                     savePredictions = TRUE
                     ) 

strat_sampl_res &amp;lt;- train(medv ~ indus + chas, data = Boston, method = &amp;quot;lm&amp;quot;,
             trControl = ctrl)

strat_sampl_res&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear Regression 
## 
## 506 samples
##   2 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 1 times) 
## Summary of sample sizes: 407, 407, 407, 407, 407, 407, ... 
## Resampling results:
## 
##   RMSE     Rsquared  MAE     
##   7.83269  0.277719  5.769507
## 
## Tuning parameter &amp;#39;intercept&amp;#39; was held constant at a value of TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, we merge the two results to compare the distributions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;resamples &amp;lt;- resamples(list(RAND = rand_sampl_res, 
                          STRAT = strat_sampl_res))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;analyzing-caret-resampling-results&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Analyzing caret resampling results&lt;/h1&gt;
&lt;p&gt;We now analyse our resampling results. We can use the &lt;code&gt;summary&lt;/code&gt; method on our resamples object:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(resamples)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## summary.resamples(object = resamples)
## 
## Models: RAND, STRAT 
## Number of resamples: 3000 
## 
## MAE 
##           Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA&amp;#39;s
## RAND  4.406326 5.475846 5.775077 5.790874 6.094820 7.582886    0
## STRAT 4.401729 5.477664 5.758201 5.769507 6.058652 7.356133    0
## 
## RMSE 
##           Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA&amp;#39;s
## RAND  5.328128 7.323887 7.847369 7.868972 8.408855 10.78024    0
## STRAT 5.560942 7.304199 7.828765 7.832690 8.328966 10.44186    0
## 
## Rsquared 
##             Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA&amp;#39;s
## RAND  0.06982417 0.2259553 0.2733762 0.2753001 0.3249820 0.5195017    0
## STRAT 0.05306875 0.2263577 0.2752015 0.2777190 0.3277577 0.4977015    0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also use the plot function provided by the &lt;code&gt;caret&lt;/code&gt; package. It plots the mean of our performance metric (RMSE), as well as estimation uncertainty of this mean. Note that the confidence intervals here are based on a normal approximation (One sample t-test).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# caret:::ggplot.resamples
# t.test(resamples$values$`RAND~RMSE`)
ggplot(resamples, metric = &amp;quot;RMSE&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-03-21-caret_validation_set_approach_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;My personal preference is to more directly display both distributions. This is done by &lt;code&gt;bwplot()&lt;/code&gt; (&lt;code&gt;caret&lt;/code&gt; does not have ggplot version of this function).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bwplot(resamples, metric = &amp;quot;RMSE&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-03-21-caret_validation_set_approach_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It does seems that stratified sampling paints a slightly more optimistic picture of the test error when compared to truly random sampling. However, we can also see that random sampling has somewhat higher variance when compared to stratified sampling.&lt;/p&gt;
&lt;p&gt;Based on these results, it seems like stratified sampling is indeed a reasonable default setting for &lt;code&gt;caret&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;update-lgocv&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Update: LGOCV&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)


ctrl &amp;lt;- trainControl(method = &amp;quot;LGOCV&amp;quot;,  ## The method doesn&amp;#39;t matter
                     repeats = n_repeats,
                     number = 1,
                     p = 0.5,
                     savePredictions = TRUE
                     ) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: `repeats` has no meaning for this resampling method.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lgocv_res &amp;lt;- train(medv ~ indus + chas, data = Boston, method = &amp;quot;lm&amp;quot;,
             trControl = ctrl)

lgocv_res&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear Regression 
## 
## 506 samples
##   2 predictor
## 
## No pre-processing
## Resampling: Repeated Train/Test Splits Estimated (1 reps, 50%) 
## Summary of sample sizes: 254 
## Resampling results:
## 
##   RMSE      Rsquared   MAE     
##   8.137926  0.2389733  5.763309
## 
## Tuning parameter &amp;#39;intercept&amp;#39; was held constant at a value of TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Arduino Weather Station with datalogging</title>
      <link>/post/arduino-atmospheric-datalogger/</link>
      <pubDate>Tue, 05 Mar 2019 00:00:00 +0000</pubDate>
      <guid>/post/arduino-atmospheric-datalogger/</guid>
      <description>
&lt;link href=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;In this post, I show how to create a Arduino-based atmospheric sensor circuit capable of storing large amounts of data on a microSD card.&lt;/p&gt;
&lt;p&gt;Nowadays, one can buy a commercial Thermo/Hygro datalogger for 50 Euro online (i.e. &lt;a href=&#34;https://www.vitalitools.nl/lascar-electronics-el-usb-2-datalogger&#34; class=&#34;uri&#34;&gt;https://www.vitalitools.nl/lascar-electronics-el-usb-2-datalogger&lt;/a&gt;). However, I decided that it would be a nice project to learn more about Arduino, in particular how to interface it with a microSD card. So i made one myself. Working with SD cards has the advantage of having a huge storage capacity. To give you an impression: Below we analyse 10K measurements stored in a 60 Kb file, the SD card can hold 4 Gb!&lt;/p&gt;
&lt;div id=&#34;components&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Components&lt;/h1&gt;
&lt;p&gt;After some research I ordered:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A microSD card reader/writer with SPI interface (Catalex card)&lt;/li&gt;
&lt;li&gt;A Bosch BME-280 temperature/pressure/humidity sensor with I2C interface&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As the BME-280 sensor operates at 3.3V and my Arduino Nano at 5V, I also ordered a four channel Logic Level Converter to convert the 5V I2C on the Arduino side of the LLC to 3.3V on the BME-280 side.&lt;/p&gt;
&lt;p&gt;To make the circuit Mains powered, i took an old Samsung mobile phone Charger (5V 0.7A), cutoff the plug and attached it to the breadboard.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;circuit-programming&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Circuit &amp;amp; Programming&lt;/h1&gt;
&lt;p&gt;The breadboard layout (created using &lt;a href=&#34;http://fritzing.org&#34;&gt;Fritzing&lt;/a&gt;) is shown below:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/post/2019-03-06-hygro_thermo_datalogger_files/figure-html/fritzing_datalogger_bb.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;At first i was using the Arduino 5V pin (with Arduino connected to USB at the front of my Desktop PC, these USB ports might have lower current) to power both the SD card and the Level converter. Separately they would work fine, but together in one circuit the SD card gave erratic results. I guessed that current consumption was too high, and during testing I used the 5V charger as power supply for the SD card. During actual usage I used the 5V charger to power both the SD card AND the Arduino Nano, which worked nicely.&lt;/p&gt;
&lt;p&gt;Coding was simple, i just combined the example code and libraries for a SPI SD card and for a BME-280 I2C sensor. I put the code on &lt;a href=&#34;https://github.com/gsverhoeven/datalogger_bme280&#34;&gt;GitHub&lt;/a&gt; anyway as a reference.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-collection-and-preparation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data collection and preparation&lt;/h1&gt;
&lt;p&gt;I ended up testing the device by letting it collect measurements in four different places within the house. In the following order:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The living room&lt;/li&gt;
&lt;li&gt;The basement&lt;/li&gt;
&lt;li&gt;First floor bedroom&lt;/li&gt;
&lt;li&gt;First floor bathroom&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After collecting the data I put the microSD card in a microSD card reader and copied the &lt;code&gt;DATALOG.TXT&lt;/code&gt; CSV file to my pc for analysis in &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- read.csv2(&amp;quot;DATALOG.TXT&amp;quot;, header = F)
colnames(df) &amp;lt;- c(&amp;quot;Time&amp;quot;, &amp;quot;Temp&amp;quot;, &amp;quot;Hum&amp;quot;, &amp;quot;Pressure&amp;quot;)
# give the four traces a unique ID
df$start_trace &amp;lt;- ifelse(df$Time == 0, 1, 0)
df$trace_id &amp;lt;- cumsum(df$start_trace)

mdf &amp;lt;- melt(df, id.vars = c(&amp;quot;Time&amp;quot;, &amp;quot;trace_id&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in melt(df, id.vars = c(&amp;quot;Time&amp;quot;, &amp;quot;trace_id&amp;quot;)): The melt generic in
## data.table has been passed a data.frame and will attempt to redirect to the
## relevant reshape2 method; please note that reshape2 is deprecated, and this
## redirection is now deprecated as well. To continue using melt methods from
## reshape2 while both libraries are attached, e.g. melt.list, you can prepend the
## namespace like reshape2::melt(df). In the next version, this warning will become
## an error.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: attributes are not identical across measure variables; they will be
## dropped&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# label the four traces
trace_id &amp;lt;- 1:4
trace_name &amp;lt;- c(&amp;quot;Living room&amp;quot;, &amp;quot;Basement&amp;quot;, 
                &amp;quot;Bedroom 1st floor&amp;quot;,  &amp;quot;Bathroom 1st floor&amp;quot;)

cod &amp;lt;- data.table(trace_id, trace_name = 
                    factor(trace_name, levels = trace_name))

mdf &amp;lt;- data.table(merge(mdf, cod, by = &amp;quot;trace_id&amp;quot;))
mdf &amp;lt;- mdf[, value := as.numeric(value)]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Analysis&lt;/h1&gt;
&lt;div id=&#34;pressure&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Pressure&lt;/h2&gt;
&lt;p&gt;We start with the pressure measurements. This is supposed to be a proxy for altitude.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(mdf[mdf$variable == &amp;quot;Pressure&amp;quot; &amp;amp; Time &amp;gt; 1], 
       aes(x = Time, y = value, 
           color = variable, group = variable)) +
  geom_point(col = &amp;quot;grey&amp;quot;) + 
  facet_grid(~ trace_name) + geom_smooth(size = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &amp;#39;loess&amp;#39; and formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-03-06-hygro_thermo_datalogger_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The basement, which is the lowest, has the highest pressure. But the difference between living room (ground floor) and the two rooms at the first floor is less pronounced. What is not so clear is what drives the changes in pressure WHILE the sensor is at a particular location, i.e. in the basement, or on the 1st floor. But no time to dwell on that, let&#39;s move on to the temperature!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;temperature&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Temperature&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(mdf[mdf$variable == &amp;quot;Temp&amp;quot; &amp;amp; Time &amp;gt; 1], 
       aes(x = Time, y = value, 
           color = variable, group = variable)) +
  geom_point() + facet_grid(~ trace_name)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-03-06-hygro_thermo_datalogger_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here, it appears that the sequence of the rooms can explain the slowly changing patterns of temperature. We started out in the Living room at 21C (The thermostat was set at 20C at that time). Then towards the cold basement. It appears that temperature needed some time to equilibrate, possibly because the breadboard was placed on an elevated plastic box, insulating it from below. In the bedroom it was placed on the (cold) floor, and it was already cold from the basement. Then in the bathroom, the final location, it went up, probably due to the floor being heated to keep the bathroom at 18C.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;relative-humidity&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Relative Humidity&lt;/h2&gt;
&lt;p&gt;Finally, the relative humidity. This appears super strongly correlated with the temperature.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(mdf[mdf$variable == &amp;quot;Hum&amp;quot; &amp;amp; Time &amp;gt; 1], 
       aes(x = Time, y = value, color = variable, 
           group = variable)) +
  geom_point() + facet_grid(~ trace_name)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-03-06-hygro_thermo_datalogger_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here we see that the living room is at a agreeable 45% RH. The basement has a higher RH percentage, expected because it&#39;s colder.&lt;/p&gt;
&lt;p&gt;According to Wikipedia:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Humans can be comfortable within a wide range of humidities depending on the temperature—from 30% to 70%[14]—but ideally between 50%[15] and 60%.[16] Very low humidity can create discomfort, respiratory problems, and aggravate allergies in some individuals.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The bedroom is also at a nice humidity level of 55% RH. The bathroom floor was being heated, and this unsurprisingly reduces the local RH to below 40%.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;It all seems to work pretty well. Measurement quality appears reasonable, with temperature and humidity consistent and with little noise, whereas the pressure reading needs some averaging / smoothing to get a stable signal.&lt;/p&gt;
&lt;p&gt;I had great fun making this device!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>BART vs Causal forests showdown</title>
      <link>/post/bart_vs_grf/bart-vs-grf-showdown/</link>
      <pubDate>Sat, 15 Dec 2018 00:00:00 +0000</pubDate>
      <guid>/post/bart_vs_grf/bart-vs-grf-showdown/</guid>
      <description>
&lt;link href=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;load-packages&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Load packages&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# library(devtools)
#devtools::install_github(&amp;quot;vdorie/dbarts&amp;quot;)
library(dbarts)
library(ggplot2)
library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ──&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ✔ tibble  3.0.4     ✔ dplyr   1.0.2
## ✔ tidyr   1.1.2     ✔ stringr 1.4.0
## ✔ readr   1.4.0     ✔ forcats 0.5.0
## ✔ purrr   0.3.4&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ tidyr::extract() masks dbarts::extract()
## ✖ dplyr::filter()  masks stats::filter()
## ✖ dplyr::lag()     masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(grf)
#devtools::install_github(&amp;quot;vdorie/aciccomp/2017&amp;quot;)
library(aciccomp2017)
library(cowplot)

source(&amp;quot;CalcPosteriors.R&amp;quot;)

fullrun &amp;lt;- 0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;dataset-1-simulated-dataset-from-friedman-mars-paper&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Dataset 1: Simulated dataset from Friedman MARS paper&lt;/h1&gt;
&lt;p&gt;This is not a causal problem but a prediction problem.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## y = f(x) + epsilon , epsilon ~ N(0, sigma)
## x consists of 10 variables, only first 5 matter

f &amp;lt;- function(x) {
    10 * sin(pi * x[,1] * x[,2]) + 20 * (x[,3] - 0.5)^2 +
      10 * x[,4] + 5 * x[,5]
}

set.seed(99)
sigma &amp;lt;- 1.0
n     &amp;lt;- 100

x  &amp;lt;- matrix(runif(n * 10), n, 10)
Ey &amp;lt;- f(x)
y  &amp;lt;- rnorm(n, Ey, sigma)

df &amp;lt;- data.frame(x, y, y_true = Ey)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;fit-bart-model-on-simulated-friedman-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;fit BART model on simulated Friedman data&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;if(fullrun){
## run BART
  set.seed(99)
  bartFit &amp;lt;- bart(x, y)
  saveRDS(bartFit, &amp;quot;s1.rds&amp;quot;)
} else { bartFit &amp;lt;- readRDS(&amp;quot;s1.rds&amp;quot;)}

plot(bartFit)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bart_vs_grf/BART-vs-grf_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;MCMC or sigma looks ok.&lt;/p&gt;
&lt;div id=&#34;compare-bart-fit-to-true-values&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;compare BART fit to true values&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df2 &amp;lt;- data.frame(df, 
  ql = apply(bartFit$yhat.train, length(dim(bartFit$yhat.train)), quantile,probs=0.05),
  qm = apply(bartFit$yhat.train, length(dim(bartFit$yhat.train)), quantile,probs=.5),
  qu &amp;lt;- apply(bartFit$yhat.train, length(dim(bartFit$yhat.train)), quantile,probs=0.95)
)

bartp &amp;lt;- ggplot(df2, aes(x= y, y = qm)) + geom_linerange(aes(ymin = ql, ymax = qu), col = &amp;quot;grey&amp;quot;) +
  geom_point() + geom_smooth() +
  geom_abline(intercept = 0, slope = 1, col = &amp;quot;red&amp;quot;, size = 1)

bartp&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &amp;#39;loess&amp;#39; and formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bart_vs_grf/BART-vs-grf_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This looks nice.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-grf-regression-forest-on-friedman-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fit Grf regression forest on Friedman data&lt;/h2&gt;
&lt;p&gt;From the manual: Trains a regression forest that can be used to estimate the conditional mean function mu(x) = E[Y | X = x]&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;if(fullrun){
  reg.forest = regression_forest(x, y, num.trees = 2000)
  saveRDS(reg.forest, &amp;quot;s00.rds&amp;quot;)
} else {reg.forest &amp;lt;- readRDS(&amp;quot;s00.rds&amp;quot;)}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df3 &amp;lt;- CalcPredictionsGRF(x, reg.forest)

df3 &amp;lt;- data.frame(df3, y)

ggplot(df3, aes(x= y, y = qm)) + geom_linerange(aes(ymin = ql, ymax = qu), col = &amp;quot;grey&amp;quot;) +
  geom_point() + 
  geom_abline(intercept = 0, slope = 1, col = &amp;quot;red&amp;quot;, size = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bart_vs_grf/BART-vs-grf_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is pretty bad compared to BART. What&#39;s wrong here?&lt;/p&gt;
&lt;p&gt;From reference.md: &lt;strong&gt;GRF isn&#39;t working well on a small dataset&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you observe poor performance on a dataset with a small number of examples, it may be worth trying out two changes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Disabling honesty. As noted in the section on honesty above, when honesty is enabled, the training subsample is further split in half before performing splitting. This may not leave enough information for the algorithm to determine high-quality splits.&lt;/li&gt;
&lt;li&gt;Skipping the variance estimate computation, by setting ci.group.size to 1 during training, then increasing sample.fraction. Because of how variance estimation is implemented, sample.fraction cannot be greater than 0.5 when it is enabled. If variance estimates are not needed, it may help to disable this computation and use a larger subsample size for training.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Dataset is pretty small (n=100). Maybe turn of honesty? We cannot turn off variance estimate computation, because we want the CI&#39;s&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;if(fullrun){
  reg.forest2 = regression_forest(x, y, num.trees = 2000,
                                 honesty = FALSE)
  saveRDS(reg.forest2, &amp;quot;s001.rds&amp;quot;)
} else {reg.forest2 &amp;lt;- readRDS(&amp;quot;s001.rds&amp;quot;)}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df2 &amp;lt;- CalcPredictionsGRF(x, reg.forest2)

df2 &amp;lt;- data.frame(df2, y)

grfp &amp;lt;- ggplot(df2, aes(x= y, y = qm)) + geom_linerange(aes(ymin = ql, ymax = qu), col = &amp;quot;grey&amp;quot;) +
  geom_point() + geom_smooth() +
  geom_abline(intercept = 0, slope = 1, col = &amp;quot;red&amp;quot;, size = 1)

grfp&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &amp;#39;loess&amp;#39; and formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bart_vs_grf/BART-vs-grf_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Ah! better now. But Grf still worse than BART. We ran with 2000 trees and turned of honesty. Perhaps dataset too small? Maybe check out the sample.fraction parameter? Sample.fraction is set by default at 0.5, so only half of data is used to grow tree. OR use tune.parameters = TRUE&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;compare-methods&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Compare methods&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gp &amp;lt;- plot_grid(bartp, grfp)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &amp;#39;loess&amp;#39; and formula &amp;#39;y ~ x&amp;#39;
## `geom_smooth()` using method = &amp;#39;loess&amp;#39; and formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gp &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bart_vs_grf/BART-vs-grf_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;dataset-2-simulated-data-from-acic-2017&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Dataset 2: Simulated data from ACIC 2017&lt;/h1&gt;
&lt;p&gt;This is a bigger dataset, N=4302.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Treatment effect &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; is a function of covariates x3, x24, x14, x15&lt;/li&gt;
&lt;li&gt;Probability of treatment &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; is a function of covariates x1, x43, x10.&lt;/li&gt;
&lt;li&gt;Outcome is a function of x43&lt;/li&gt;
&lt;li&gt;Noise is a function of x21&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(input_2017[, c(3,24,14,15)])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   x_3  x_24 x_14 x_15
## 1  20 white    0    2
## 2   0 black    0    0
## 3   0 white    0    1
## 4  10 white    0    0
## 5   0 black    0    0
## 6   1 white    0    0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check transformed covariates used to create simulated datasets.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# zit hidden in package
head(aciccomp2017:::transformedData_2017)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              x_1   x_3  x_10  x_14  x_15 x_21 x_24       x_43
## 2665 -1.18689448  gt_0 leq_0 leq_0  gt_0    J    E -1.0897971
## 22   -0.04543705 leq_0 leq_0 leq_0 leq_0    J    B  1.1223750
## 2416  0.13675482 leq_0 leq_0 leq_0  gt_0    J    E  0.6136700
## 1350 -0.24062700  gt_0 leq_0 leq_0 leq_0    J    E -0.2995632
## 3850  1.02054653 leq_0 leq_0 leq_0 leq_0    I    B  0.6136700
## 4167 -1.18689448  gt_0 leq_0 leq_0 leq_0    K    E -1.5961206&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we find that we should not take the functions in Dorie 2018 (debrief.pdf) literately. x_3 used to calculate the treatment effect is &lt;strong&gt;derived&lt;/strong&gt; from x_3 in the input data. x_24 used to calculate the treatment effect is &lt;strong&gt;derived&lt;/strong&gt; from x_24 in the input data. Both have become binary variables.&lt;/p&gt;
&lt;p&gt;Turns out that this was a feature of the 2016 ACIC and IS mentioned in the debrief.pdf&lt;/p&gt;
&lt;p&gt;We pick the iid, strong signal, low noise, low confounding first. Actually from estimated PS (W.hat) it seems that every obs has probability of treatment 50%.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;parameters_2017[21,]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    errors magnitude noise confounding
## 21    iid         1     0           0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# easiest?&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Grab the first replicate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim &amp;lt;- dgp_2017(21, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;fit-bart-to-acic-2017-dataset&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fit BART to ACIC 2017 dataset&lt;/h2&gt;
&lt;p&gt;Need also counterfactual predictions. Most efficient seems to create x.test with Z reversed. This will give use a y.test as well as y.train in the output. We expect draws for both. Plotting a histogram of the difference gives us the treatment effect with uncertainty.&lt;/p&gt;
&lt;p&gt;From the MCMC draws for sigma we infer that we need to drop more &amp;quot;burn in&amp;quot; samples.&lt;/p&gt;
&lt;p&gt;Prepare data for BART, including x.test with treatment reversed:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# combine x and y
y &amp;lt;- sim$y
x &amp;lt;- model.matrix(~. ,cbind(z = sim$z, input_2017))

# flip z for counterfactual predictions (needed for BART)
x.test &amp;lt;- model.matrix(~. ,cbind(z = 1 - sim$z, input_2017))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## run BART
#fullrun &amp;lt;- 0
if(fullrun){
  set.seed(99)

  bartFit &amp;lt;- bart(x, y, x.test, nskip = 350, ntree = 1000)
  saveRDS(bartFit, &amp;quot;s2.rds&amp;quot;)
} else { bartFit &amp;lt;- readRDS(&amp;quot;s2.rds&amp;quot;)}

plot(bartFit)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bart_vs_grf/BART-vs-grf_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;extract-individual-treatment-effect-ite-cate-plus-uncertainty-from-bartfit&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Extract individual treatment effect (ITE / CATE) plus uncertainty from bartfit&lt;/h3&gt;
&lt;p&gt;This means switching z from 0 to 1 and looking at difference in y + uncertainty in y.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#source(&amp;quot;calcPosteriors.R&amp;quot;)
sim &amp;lt;- CalcPosteriorsBART(sim, bartFit, &amp;quot;z&amp;quot;)

sim &amp;lt;- sim %&amp;gt;% arrange(alpha)

bartp &amp;lt;- ggplot(sim, aes(x = 1:nrow(sim), qm))  + 
  geom_linerange(aes(ymin = ql, ymax = qu), col = &amp;quot;grey&amp;quot;) +
  geom_point() + 
  geom_smooth() + geom_point(aes(y = alpha), col = &amp;quot;red&amp;quot;) + ylim(-2.5, 4.5)

bartp&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &amp;#39;gam&amp;#39; and formula &amp;#39;y ~ s(x, bs = &amp;quot;cs&amp;quot;)&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 1 rows containing missing values (geom_smooth).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bart_vs_grf/BART-vs-grf_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This looks sort of ok, but still weird. Some points it gets REALLY wrong.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;calculate-coverage&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Calculate coverage&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim &amp;lt;- sim %&amp;gt;% mutate(in_ci = ql &amp;lt; alpha &amp;amp; qu &amp;gt; alpha) 

mean(sim$in_ci)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.4363087&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Pretty bad coverage. Look into whats going on here. Here it should be 0.9&lt;/p&gt;
&lt;p&gt;The iid plot for method 2 gives coverage 0.7 (where it should be 0.95)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;calculate-rmse-of-cate&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Calculate RMSE of CATE&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sqrt(mean((sim$alpha - sim$ite)^2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1587338&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For All i.i.d. (averaged over 250 replicates averaged over 8 scenarios) method 2 (BART should have RMSE of CATE of 0.35-0.4)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-grf-to-acic-2017-dataset&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fit grf to ACIC 2017 dataset&lt;/h2&gt;
&lt;p&gt;need large num.trees for CI.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# prep data for Grf
# combine x and y
sim &amp;lt;- dgp_2017(21, 1)

Y &amp;lt;- sim$y
X &amp;lt;- model.matrix(~. ,input_2017)
W = sim$z

# Train a causal forest.
#fullrun &amp;lt;- 0

if(fullrun){
  grf.fit_alt &amp;lt;- causal_forest(X, Y, W, num.trees = 500)
  saveRDS(grf.fit_alt, &amp;quot;s3.rds&amp;quot;)
} else{grf.fit_alt &amp;lt;- readRDS(&amp;quot;s3.rds&amp;quot;)}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It appears that using 4000 trees consumes too much memory (bad_alloc)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;compare-predictions-vs-true-value&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Compare predictions vs true value&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_sep2 &amp;lt;- CalcPredictionsGRF(X, grf.fit_alt)

df_sep2 &amp;lt;- data.frame(df_sep2, Y, W, TAU = sim$alpha)

df_sep2 &amp;lt;- df_sep2 %&amp;gt;% arrange(TAU)

grfp &amp;lt;- ggplot(df_sep2, aes(x = 1:nrow(df_sep2), y = qm))   +
  geom_linerange(aes(ymin = ql, ymax = qu), col = &amp;quot;grey&amp;quot;) + geom_point() + geom_smooth() + 
  geom_point(aes(y = TAU), col = &amp;quot;red&amp;quot;) + ylim(-2.5, 4.5)

grfp&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &amp;#39;gam&amp;#39; and formula &amp;#39;y ~ s(x, bs = &amp;quot;cs&amp;quot;)&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bart_vs_grf/BART-vs-grf_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This works ok now.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;compare-both-methods&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Compare both methods&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gp &amp;lt;- plot_grid(bartp, grfp)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &amp;#39;gam&amp;#39; and formula &amp;#39;y ~ s(x, bs = &amp;quot;cs&amp;quot;)&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 1 rows containing missing values (geom_smooth).&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &amp;#39;gam&amp;#39; and formula &amp;#39;y ~ s(x, bs = &amp;quot;cs&amp;quot;)&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gp&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bart_vs_grf/BART-vs-grf_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;dataset-3-simulated-data-used-by-grf-example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Dataset 3: simulated data used by grf example&lt;/h1&gt;
&lt;p&gt;THis dataset is used in the Grf manual page. Size N = 2000. Probability of treatment function of X1. Treatment effect function of X1.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Generate data.
set.seed(123)
n = 2000; p = 10
X = matrix(rnorm(n*p), n, p)

# treatment
W = rbinom(n, 1, 0.4 + 0.2 * (X[,1] &amp;gt; 0))
# outcome (parallel max)
Y = pmax(X[,1], 0) * W + X[,2] + pmin(X[,3], 0) + rnorm(n)

# TAU is true treatment effect
df &amp;lt;- data.frame(X, W, Y, TAU = pmax(X[,1], 0))&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;fit-grf&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fit GRF&lt;/h2&gt;
&lt;p&gt;Default settings are honesty = TRUE.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Train a causal forest.
if(fullrun){
  tau.forest = causal_forest(X, Y, W, num.trees = 2000)
  saveRDS(tau.forest, &amp;quot;s4.rds&amp;quot;)
} else {tau.forest &amp;lt;- readRDS(&amp;quot;s4.rds&amp;quot;)}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;oob-predictions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;OOB predictions&lt;/h2&gt;
&lt;p&gt;From the GRF manual:&lt;/p&gt;
&lt;p&gt;Given a test example, the GRF algorithm computes a prediction as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;For each tree, the test example is &amp;#39;pushed down&amp;#39; to determine what leaf it falls in.
Given this information, we create a list of neighboring training examples, weighted by how many times the example fell in the same leaf as the test example.
A prediction is made using this weighted list of neighbors, using the relevant approach for the type of forest. In causal prediction, we calculate the treatment effect using the outcomes and treatment status of the neighbor examples.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Those familiar with classic random forests might note that this approach differs from the way forest prediction is usually described. The traditional view is that to predict for a test example, each tree makes a prediction on that example. To make a final prediction, the tree predictions are combined in some way, for example through averaging or through &#39;majority voting&#39;. It&#39;s worth noting that for regression forests, the GRF algorithm described above is identical this &#39;ensemble&#39; approach, where each tree predicts by averaging the outcomes in each leaf, and predictions are combined through a weighted average.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Estimate treatment effects for the training data using out-of-bag prediction.
tau.hat.oob = predict(tau.forest)

res &amp;lt;- data.frame(df, pred = tau.hat.oob$predictions)

ggplot(res, aes(x = X1, y = pred)) + geom_point() + geom_smooth() + geom_abline(intercept = 0, slope = 1) +
  geom_line(aes(y = TAU), col = &amp;quot;red&amp;quot;, size = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &amp;#39;gam&amp;#39; and formula &amp;#39;y ~ s(x, bs = &amp;quot;cs&amp;quot;)&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bart_vs_grf/BART-vs-grf_files/figure-html/unnamed-chunk-24-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ate-att&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ATE &amp;amp; ATT&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Estimate the conditional average treatment effect on the full sample (CATE).
average_treatment_effect(tau.forest, target.sample = &amp;quot;all&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   estimate    std.err 
## 0.37316437 0.04795009&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(res$TAU)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.4138061&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Estimate the conditional average treatment effect on the treated sample (CATT).
# Here, we don&amp;#39;t expect much difference between the CATE and the CATT, since
# treatment assignment was randomized.
average_treatment_effect(tau.forest, target.sample = &amp;quot;treated&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   estimate    std.err 
## 0.47051526 0.04850751&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(res[res$W == 1,]$TAU)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5010723&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-more-trees-for-cis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fit more trees for CI&#39;s&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Add confidence intervals for heterogeneous treatment effects; growing more
# trees is now recommended.
if(fullrun){
  tau.forest_big = causal_forest(X, Y, W, num.trees = 4000)
  saveRDS(tau.forest_big, &amp;quot;s5.rds&amp;quot;)
} else {tau.forest_big &amp;lt;- readRDS(&amp;quot;s5.rds&amp;quot;)}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-cis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plot CI&#39;s&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## PM
#source(&amp;quot;CalcPosteriors.R&amp;quot;)
df_res &amp;lt;- CalcPredictionsGRF(df, tau.forest_big)

grfp &amp;lt;- ggplot(df_res, aes(x = X1, y = qm)) + 
  geom_linerange(aes(ymin = ql, ymax = qu), col = &amp;quot;grey&amp;quot;) +
  geom_point()  + 
  geom_smooth() + geom_line(aes(y = TAU), col = &amp;quot;red&amp;quot;, size = 1) +
   ylim(-1,3.5)

grfp&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &amp;#39;gam&amp;#39; and formula &amp;#39;y ~ s(x, bs = &amp;quot;cs&amp;quot;)&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bart_vs_grf/BART-vs-grf_files/figure-html/unnamed-chunk-28-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-bart-on-this-dataset&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fit BART on this dataset&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x.train &amp;lt;- model.matrix(~. ,data.frame(W, X))
x.test &amp;lt;- model.matrix(~. ,data.frame(W = 1 - W, X))
y.train &amp;lt;- Y

if(fullrun){
  bartFit &amp;lt;- bart(x.train, y.train, x.test, ntree = 2000, ndpost = 1000, nskip = 100)
  saveRDS(bartFit, &amp;quot;s10.rds&amp;quot;)
} else {bartFit &amp;lt;- readRDS(&amp;quot;s10.rds&amp;quot;)}
plot(bartFit)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bart_vs_grf/BART-vs-grf_files/figure-html/unnamed-chunk-29-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bart-check-fit-and-cis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;BART: Check fit and CI&#39;s&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#source(&amp;quot;calcPosteriors.R&amp;quot;)
sim &amp;lt;- CalcPosteriorsBART(df, bartFit, treatname = &amp;quot;W&amp;quot;)


bartp &amp;lt;- ggplot(sim, aes(x = X1, qm)) + geom_linerange(aes(ymin = ql, ymax = qu), col = &amp;quot;grey&amp;quot;) +
  geom_point() + geom_smooth() +
  geom_line(aes(y = TAU), col = &amp;quot;red&amp;quot;, size = 1) + ylim(-1,3.5)

bartp&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &amp;#39;gam&amp;#39; and formula &amp;#39;y ~ s(x, bs = &amp;quot;cs&amp;quot;)&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bart_vs_grf/BART-vs-grf_files/figure-html/unnamed-chunk-30-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;compare&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Compare&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gp &amp;lt;- plot_grid(bartp, grfp)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &amp;#39;gam&amp;#39; and formula &amp;#39;y ~ s(x, bs = &amp;quot;cs&amp;quot;)&amp;#39;
## `geom_smooth()` using method = &amp;#39;gam&amp;#39; and formula &amp;#39;y ~ s(x, bs = &amp;quot;cs&amp;quot;)&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gp&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bart_vs_grf/BART-vs-grf_files/figure-html/unnamed-chunk-31-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here Grf appears more accurate. Mental note: Both W and TAU function of X1.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;dataset-4-fit-separate-grf-forests-for-y-and-w&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Dataset 4: Fit separate grf forests for Y and W&lt;/h1&gt;
&lt;p&gt;This dataset has a complex propensity of treatment function (Exponential of X1 and X2), as well as hetergeneous treatment effect that is exponential function of X3. It has size N=4000.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# In some examples, pre-fitting models for Y and W separately may
# be helpful (e.g., if different models use different covariates).
# In some applications, one may even want to get Y.hat and W.hat
# using a completely different method (e.g., boosting).
set.seed(123)
# Generate new data.
n = 4000; p = 20
X = matrix(rnorm(n * p), n, p)
TAU = 1 / (1 + exp(-X[, 3]))
W = rbinom(n ,1, 1 / (1 + exp(-X[, 1] - X[, 2])))
Y = pmax(X[, 2] + X[, 3], 0) + rowMeans(X[, 4:6]) / 2 + W * TAU + rnorm(n)

df_sep4 &amp;lt;- data.frame(X, TAU, W, Y)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;grf-two-step-first-fit-model-for-w-ps&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Grf two-step: First fit model for W (PS)&lt;/h2&gt;
&lt;p&gt;Regression forest to predict W from X. This is a propensity score.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;if(fullrun){
  forest.W &amp;lt;- regression_forest(X, W, tune.parameters = c(&amp;quot;min.node.size&amp;quot;, &amp;quot;honesty.prune.leaves&amp;quot;), 
                               num.trees = 2000)
  saveRDS(forest.W, &amp;quot;s6.rds&amp;quot;)
} else {forest.W &amp;lt;- readRDS(&amp;quot;s6.rds&amp;quot;)}

W.hat = predict(forest.W)$predictions&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;grfthen-fit-model-for-y-selecting-covariates&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Grf:Then Fit model for Y, selecting covariates&lt;/h3&gt;
&lt;p&gt;This predict Y from X, ignoring treatment.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;if(fullrun){
  forest.Y = regression_forest(X, Y, tune.parameters = c(&amp;quot;min.node.size&amp;quot;, &amp;quot;honesty.prune.leaves&amp;quot;), 
                               num.trees = 2000)
  saveRDS(forest.Y, &amp;quot;s7.rds&amp;quot;)
} else {forest.Y &amp;lt;- readRDS(&amp;quot;s7.rds&amp;quot;)}

Y.hat = predict(forest.Y)$predictions&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;grfselect-variables-that-predict-y.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Grf:Select variables that predict Y.&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;forest.Y.varimp = variable_importance(forest.Y)
# Note: Forests may have a hard time when trained on very few variables
# (e.g., ncol(X) = 1, 2, or 3). We recommend not being too aggressive
# in selection.
selected.vars = which(forest.Y.varimp / mean(forest.Y.varimp) &amp;gt; 0.2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This selects five variables of 20. Indeed these are the variables that were used to simulate Y.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;grf-finally-fit-causal-forest-using-ps-and-selected-covariates&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Grf: Finally, Fit causal forest using PS and selected covariates&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;if(fullrun){
tau.forest2 = causal_forest(X[, selected.vars], Y, W,
                           W.hat = W.hat, Y.hat = Y.hat,
                           tune.parameters = c(&amp;quot;min.node.size&amp;quot;, &amp;quot;honesty.prune.leaves&amp;quot;), 
                           num.trees = 4000)
  saveRDS(tau.forest2, &amp;quot;s8.rds&amp;quot;)
} else {tau.forest2 &amp;lt;- readRDS(&amp;quot;s8.rds&amp;quot;)}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;grf-check-fit-and-cis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Grf: Check fit and CI&#39;s&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_sep2 &amp;lt;- CalcPredictionsGRF(df_sep4[,selected.vars], tau.forest2)

grfp &amp;lt;- ggplot(df_sep2, aes(x = X3, y = qm))   +
  geom_linerange(aes(ymin = ql, ymax = qu), col = &amp;quot;grey&amp;quot;) + geom_point() + 
  geom_smooth() + 
  geom_line(aes(y = TAU), col = &amp;quot;red&amp;quot;, size = 1) + ylim(-0.7,2)

grfp&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &amp;#39;gam&amp;#39; and formula &amp;#39;y ~ s(x, bs = &amp;quot;cs&amp;quot;)&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bart_vs_grf/BART-vs-grf_files/figure-html/unnamed-chunk-37-1.png&#34; width=&#34;672&#34; /&gt; This looks ok.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-bart-on-this-dataset-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fit BART on this dataset&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x.train &amp;lt;- model.matrix(~. ,data.frame(W, X))
x.test &amp;lt;- model.matrix(~. ,data.frame(W = 1 - W, X))
y.train &amp;lt;- Y

if(fullrun){
  bartFit &amp;lt;- bart(x.train, y.train, x.test, ntree = 4000)
  saveRDS(bartFit, &amp;quot;s9.rds&amp;quot;)
} else {bartFit &amp;lt;- readRDS(&amp;quot;s9.rds&amp;quot;)}
plot(bartFit)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bart_vs_grf/BART-vs-grf_files/figure-html/unnamed-chunk-38-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bart-check-fit-and-cis-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;BART: Check fit and CI&#39;s&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#source(&amp;quot;calcPosteriors.R&amp;quot;)
sim &amp;lt;- CalcPosteriorsBART(df_sep4, bartFit, treatname = &amp;quot;W&amp;quot;)


bartp &amp;lt;- ggplot(sim, aes(x = X3, qm)) + geom_linerange(aes(ymin = ql, ymax = qu), col = &amp;quot;grey&amp;quot;) +
  geom_point() + geom_smooth() +
  geom_line(aes(y = TAU), col = &amp;quot;red&amp;quot;, size = 1) + ylim(-0.7,2)

bartp&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &amp;#39;gam&amp;#39; and formula &amp;#39;y ~ s(x, bs = &amp;quot;cs&amp;quot;)&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bart_vs_grf/BART-vs-grf_files/figure-html/unnamed-chunk-39-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;compare-bart-and-grf&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Compare BART and grf&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gp &amp;lt;- plot_grid(bartp, grfp)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &amp;#39;gam&amp;#39; and formula &amp;#39;y ~ s(x, bs = &amp;quot;cs&amp;quot;)&amp;#39;
## `geom_smooth()` using method = &amp;#39;gam&amp;#39; and formula &amp;#39;y ~ s(x, bs = &amp;quot;cs&amp;quot;)&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gp&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bart_vs_grf/BART-vs-grf_files/figure-html/unnamed-chunk-40-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Very similar results. BART appears slightly more accurate, especially for low values of X3.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Improving a parametric regression model using machine learning</title>
      <link>/post/interaction_detection/interaction-detection/</link>
      <pubDate>Fri, 01 Sep 2017 00:00:00 +0000</pubDate>
      <guid>/post/interaction_detection/interaction-detection/</guid>
      <description>
&lt;link href=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;The idea is that comparing the predictions of an RF model with the predictions of an OLS model can inform us in what ways the OLS model fails to capture all non-linearities and interactions between the predictors. Subsequently, using partial dependence plots of the RF model can guide the modelling of the non-linearities in the OLS model. After this step, the discrepancies between the RF predictions and the OLS predictions should be caused by non-modeled interactions. Using an RF to predict the discrepancy itself can then be used to discover which predictors are involved in these interactions. We test this method on the classic &lt;code&gt;Boston Housing&lt;/code&gt; dataset to predict median house values (&lt;code&gt;medv&lt;/code&gt;). We indeed recover interactions that, as it turns, have already been found and documented in the literature.&lt;/p&gt;
&lt;div id=&#34;load-packages&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Load packages&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls())
#library(randomForest)
#library(party)
library(ranger)
library(data.table)
library(ggplot2)
library(MASS)

rdunif &amp;lt;- function(n,k) sample(1:k, n, replace = T)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;step-1-run-a-rf-on-the-boston-housing-set&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Step 1: Run a RF on the Boston Housing set&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_ranger &amp;lt;- ranger(medv ~ ., data = Boston,
                                  importance = &amp;quot;permutation&amp;quot;, num.trees = 500,
                                  mtry = 5, replace = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Extract the permutation importance measure.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;myres_tmp &amp;lt;- ranger::importance(my_ranger);
myres &amp;lt;- cbind(names(myres_tmp), myres_tmp,  i = 1)
#my_rownames &amp;lt;- row.names(myres)
myres &amp;lt;- data.table(myres)
setnames(myres, &amp;quot;V1&amp;quot;, &amp;quot;varname&amp;quot;)
setnames(myres, &amp;quot;myres_tmp&amp;quot;, &amp;quot;MeanDecreaseAccuracy&amp;quot;)
myres &amp;lt;- myres[, varname := as.factor(varname)]
myres &amp;lt;- myres[, MeanDecreaseAccuracy := as.numeric(MeanDecreaseAccuracy)]
myres &amp;lt;- myres[, i := as.integer(i)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(myres, 
       aes(x = reorder(factor(varname), MeanDecreaseAccuracy), y = MeanDecreaseAccuracy)) + 
  geom_point() + coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/interaction_detection/interaction-detection_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-an-ols-to-the-boston-housing&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Fit an OLS to the Boston Housing&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_glm &amp;lt;- glm(medv ~., data = Boston, 
              family = &amp;quot;gaussian&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;compare-predictions-of-both-models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Compare predictions of both models&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pred_RF &amp;lt;- predict(my_ranger, data = Boston)
#pred_RF$predictions
pred_GLM &amp;lt;- predict(my_glm, data = Boston)

plot(pred_RF$predictions, pred_GLM)
abline(0, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/interaction_detection/interaction-detection_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt; # Run a RF on the discrepancy&lt;/p&gt;
&lt;p&gt;Discrepancy is defined as the difference between the predictions of both models for each observation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pred_diff &amp;lt;- pred_RF$predictions - pred_GLM

my_ranger_diff &amp;lt;- ranger(Ydiff ~ . - medv, data = data.table(Ydiff = pred_diff, Boston),
                                  importance = &amp;quot;permutation&amp;quot;, num.trees = 500,
                                  mtry = 5, replace = TRUE)
my_ranger_diff&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Ranger result
## 
## Call:
##  ranger(Ydiff ~ . - medv, data = data.table(Ydiff = pred_diff,      Boston), importance = &amp;quot;permutation&amp;quot;, num.trees = 500, mtry = 5,      replace = TRUE) 
## 
## Type:                             Regression 
## Number of trees:                  500 
## Sample size:                      506 
## Number of independent variables:  13 
## Mtry:                             5 
## Target node size:                 5 
## Variable importance mode:         permutation 
## Splitrule:                        variance 
## OOB prediction error (MSE):       5.151991 
## R squared (OOB):                  0.6674631&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It turns out the RF can &amp;quot;explain&amp;quot; 67% of these discrepancies.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;myres_tmp &amp;lt;- ranger::importance(my_ranger_diff)
myres &amp;lt;- cbind(names(myres_tmp), myres_tmp,  i = 1)
#my_rownames &amp;lt;- row.names(myres)
myres &amp;lt;- data.table(myres)
setnames(myres, &amp;quot;V1&amp;quot;, &amp;quot;varname&amp;quot;)
setnames(myres, &amp;quot;myres_tmp&amp;quot;, &amp;quot;MeanDecreaseAccuracy&amp;quot;)
myres &amp;lt;- myres[, varname := as.factor(varname)]
myres &amp;lt;- myres[, MeanDecreaseAccuracy := as.numeric(MeanDecreaseAccuracy)]
myres &amp;lt;- myres[, i := as.integer(i)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(myres, 
       aes(x = reorder(factor(varname), MeanDecreaseAccuracy), y = MeanDecreaseAccuracy)) + 
  geom_point() + coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/interaction_detection/interaction-detection_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It turns out that &lt;code&gt;rm&lt;/code&gt; and &lt;code&gt;lstat&lt;/code&gt; are the variables that best predict the discrepancy.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_glm_int &amp;lt;- glm(medv ~. + rm:lstat, data = Boston, 
              family = &amp;quot;gaussian&amp;quot;)
summary(my_glm_int)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = medv ~ . + rm:lstat, family = &amp;quot;gaussian&amp;quot;, data = Boston)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -21.5738   -2.3319   -0.3584    1.8149   27.9558  
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)   6.073638   5.038175   1.206 0.228582    
## crim         -0.157100   0.028808  -5.453 7.85e-08 ***
## zn            0.027199   0.012020   2.263 0.024083 *  
## indus         0.052272   0.053475   0.978 0.328798    
## chas          2.051584   0.750060   2.735 0.006459 ** 
## nox         -15.051627   3.324807  -4.527 7.51e-06 ***
## rm            7.958907   0.488520  16.292  &amp;lt; 2e-16 ***
## age           0.013466   0.011518   1.169 0.242918    
## dis          -1.120269   0.175498  -6.383 4.02e-10 ***
## rad           0.320355   0.057641   5.558 4.49e-08 ***
## tax          -0.011968   0.003267  -3.664 0.000276 ***
## ptratio      -0.721302   0.115093  -6.267 8.06e-10 ***
## black         0.003985   0.002371   1.681 0.093385 .  
## lstat         1.844883   0.191833   9.617  &amp;lt; 2e-16 ***
## rm:lstat     -0.418259   0.032955 -12.692  &amp;lt; 2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for gaussian family taken to be 16.98987)
## 
##     Null deviance: 42716  on 505  degrees of freedom
## Residual deviance:  8342  on 491  degrees of freedom
## AIC: 2886
## 
## Number of Fisher Scoring iterations: 2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The interaction we have added is indeed highly significant.&lt;/p&gt;
&lt;p&gt;Compare approximate out-of-sample prediction accuracy using AIC:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;AIC(my_glm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3027.609&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;AIC(my_glm_int)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2886.043&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, the addition of the interaction greatly increases the prediction accuracy.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;repeat-this-process&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Repeat this process&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pred_RF &amp;lt;- predict(my_ranger, data = Boston)
#pred_RF$predictions
pred_GLM &amp;lt;- predict(my_glm_int, data = Boston)

plot(pred_RF$predictions, pred_GLM)
abline(0, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/interaction_detection/interaction-detection_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pred_diff &amp;lt;- pred_RF$predictions - pred_GLM

my_ranger_diff2 &amp;lt;- ranger(Ydiff ~ . - medv, data = data.table(Ydiff = pred_diff, Boston),
                                  importance = &amp;quot;permutation&amp;quot;, num.trees = 500,
                                  mtry = 5, replace = TRUE)
my_ranger_diff2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Ranger result
## 
## Call:
##  ranger(Ydiff ~ . - medv, data = data.table(Ydiff = pred_diff,      Boston), importance = &amp;quot;permutation&amp;quot;, num.trees = 500, mtry = 5,      replace = TRUE) 
## 
## Type:                             Regression 
## Number of trees:                  500 
## Sample size:                      506 
## Number of independent variables:  13 
## Mtry:                             5 
## Target node size:                 5 
## Variable importance mode:         permutation 
## Splitrule:                        variance 
## OOB prediction error (MSE):       5.604118 
## R squared (OOB):                  0.4399596&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;myres_tmp &amp;lt;- ranger::importance(my_ranger_diff2)
myres &amp;lt;- cbind(names(myres_tmp), myres_tmp,  i = 1)
#my_rownames &amp;lt;- row.names(myres)
myres &amp;lt;- data.table(myres)
setnames(myres, &amp;quot;V1&amp;quot;, &amp;quot;varname&amp;quot;)
setnames(myres, &amp;quot;myres_tmp&amp;quot;, &amp;quot;MeanDecreaseAccuracy&amp;quot;)
myres &amp;lt;- myres[, varname := as.factor(varname)]
myres &amp;lt;- myres[, MeanDecreaseAccuracy := as.numeric(MeanDecreaseAccuracy)]
myres &amp;lt;- myres[, i := as.integer(i)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(myres, 
       aes(x = reorder(factor(varname), MeanDecreaseAccuracy), y = MeanDecreaseAccuracy)) + 
  geom_point() + coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/interaction_detection/interaction-detection_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now the variables that best predict the discrepancy are &lt;code&gt;lstat&lt;/code&gt; and &lt;code&gt;dis&lt;/code&gt;. Add these two variables as an interaction.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_glm_int2 &amp;lt;- glm(medv ~. + rm:lstat + lstat:dis, data = Boston, 
              family = &amp;quot;gaussian&amp;quot;)
summary(my_glm_int2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = medv ~ . + rm:lstat + lstat:dis, family = &amp;quot;gaussian&amp;quot;, 
##     data = Boston)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -23.3918   -2.2997   -0.4077    1.6475   27.6766  
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)   1.552991   5.107295   0.304 0.761201    
## crim         -0.139370   0.028788  -4.841 1.73e-06 ***
## zn            0.042984   0.012550   3.425 0.000667 ***
## indus         0.066690   0.052878   1.261 0.207834    
## chas          1.760779   0.743688   2.368 0.018290 *  
## nox         -11.544280   3.404577  -3.391 0.000753 ***
## rm            8.640503   0.513593  16.824  &amp;lt; 2e-16 ***
## age          -0.002127   0.012067  -0.176 0.860140    
## dis          -1.904982   0.268056  -7.107 4.22e-12 ***
## rad           0.304689   0.057000   5.345 1.39e-07 ***
## tax          -0.011220   0.003228  -3.476 0.000554 ***
## ptratio      -0.641380   0.115418  -5.557 4.51e-08 ***
## black         0.003756   0.002339   1.606 0.108924    
## lstat         1.925223   0.190368  10.113  &amp;lt; 2e-16 ***
## rm:lstat     -0.466947   0.034897 -13.381  &amp;lt; 2e-16 ***
## dis:lstat     0.076716   0.020009   3.834 0.000143 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for gaussian family taken to be 16.52869)
## 
##     Null deviance: 42716.3  on 505  degrees of freedom
## Residual deviance:  8099.1  on 490  degrees of freedom
## AIC: 2873.1
## 
## Number of Fisher Scoring iterations: 2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;AIC(my_glm_int2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2873.087&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;AIC(my_glm_int)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2886.043&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We conclude that the second interaction also results in significant model improvement.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-more-ambitious-goal-try-and-improve-harrison-rubinfelds-model-formula-for-boston-housing&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A more ambitious goal: Try and improve Harrison &amp;amp; Rubinfeld&#39;s model formula for Boston housing&lt;/h1&gt;
&lt;p&gt;So far, we assumed that all relationships are linear. Harrison and Rubinfeld have created a model without interactions, but with transformations to correct for skewness, heteroskedasticity etc. Let&#39;s see if we can improve upon this model equation by applying our method to search for interactions. Their formula predicts &lt;code&gt;log(medv)&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Harrison and Rubinfeld (1978) model
my_glm_hr &amp;lt;- glm(log(medv) ~ I(rm^2) + age + log(dis) + log(rad) + tax + ptratio + 
                     black + I(black^2) + log(lstat) + crim + zn + indus + chas + I(nox^2), data = Boston, 
              family = &amp;quot;gaussian&amp;quot;)

summary(my_glm_hr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = log(medv) ~ I(rm^2) + age + log(dis) + log(rad) + 
##     tax + ptratio + black + I(black^2) + log(lstat) + crim + 
##     zn + indus + chas + I(nox^2), family = &amp;quot;gaussian&amp;quot;, data = Boston)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -0.73091  -0.09274  -0.00710   0.09800   0.78607  
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  4.474e+00  1.579e-01  28.343  &amp;lt; 2e-16 ***
## I(rm^2)      6.634e-03  1.313e-03   5.053 6.15e-07 ***
## age          3.491e-05  5.245e-04   0.067 0.946950    
## log(dis)    -1.927e-01  3.325e-02  -5.796 1.22e-08 ***
## log(rad)     9.613e-02  1.905e-02   5.047 6.35e-07 ***
## tax         -4.295e-04  1.222e-04  -3.515 0.000481 ***
## ptratio     -2.977e-02  5.024e-03  -5.926 5.85e-09 ***
## black        1.520e-03  5.068e-04   3.000 0.002833 ** 
## I(black^2)  -2.597e-06  1.114e-06  -2.331 0.020153 *  
## log(lstat)  -3.695e-01  2.491e-02 -14.833  &amp;lt; 2e-16 ***
## crim        -1.157e-02  1.246e-03  -9.286  &amp;lt; 2e-16 ***
## zn           7.257e-05  5.034e-04   0.144 0.885430    
## indus       -1.943e-04  2.360e-03  -0.082 0.934424    
## chas         9.180e-02  3.305e-02   2.777 0.005690 ** 
## I(nox^2)    -6.566e-01  1.129e-01  -5.815 1.09e-08 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for gaussian family taken to be 0.03299176)
## 
##     Null deviance: 84.376  on 505  degrees of freedom
## Residual deviance: 16.199  on 491  degrees of freedom
## AIC: -273.48
## 
## Number of Fisher Scoring iterations: 2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_ranger_log &amp;lt;- ranger(log(medv) ~ ., data = Boston,
                                  importance = &amp;quot;permutation&amp;quot;, num.trees = 500,
                                  mtry = 5, replace = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pred_RF &amp;lt;- predict(my_ranger_log, data = Boston)
#pred_RF$predictions
pred_GLM &amp;lt;- predict(my_glm_hr, data = Boston)

plot(pred_RF$predictions, pred_GLM)
abline(0, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/interaction_detection/interaction-detection_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For low predicted values both models differ in a systematic way. This suggests that there exists a remaining pattern that is picked up by RF but not by the OLS model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pred_diff &amp;lt;- pred_RF$predictions - pred_GLM

my_ranger_log_diff &amp;lt;- ranger(Ydiff ~ . - medv, data = data.table(Ydiff = pred_diff, Boston),
                                  importance = &amp;quot;permutation&amp;quot;, num.trees = 500,
                                  mtry = 5, replace = TRUE)
my_ranger_log_diff&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Ranger result
## 
## Call:
##  ranger(Ydiff ~ . - medv, data = data.table(Ydiff = pred_diff,      Boston), importance = &amp;quot;permutation&amp;quot;, num.trees = 500, mtry = 5,      replace = TRUE) 
## 
## Type:                             Regression 
## Number of trees:                  500 
## Sample size:                      506 
## Number of independent variables:  13 
## Mtry:                             5 
## Target node size:                 5 
## Variable importance mode:         permutation 
## Splitrule:                        variance 
## OOB prediction error (MSE):       0.009132647 
## R squared (OOB):                  0.5268126&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The RF indicates that 54% of the discrepancy can be &amp;quot;explained&amp;quot; by RF.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;myres_tmp &amp;lt;- ranger::importance(my_ranger_log_diff)
myres &amp;lt;- cbind(names(myres_tmp), myres_tmp,  i = 1)
#my_rownames &amp;lt;- row.names(myres)
myres &amp;lt;- data.table(myres)
setnames(myres, &amp;quot;V1&amp;quot;, &amp;quot;varname&amp;quot;)
setnames(myres, &amp;quot;myres_tmp&amp;quot;, &amp;quot;MeanDecreaseAccuracy&amp;quot;)
myres &amp;lt;- myres[, varname := as.factor(varname)]
myres &amp;lt;- myres[, MeanDecreaseAccuracy := as.numeric(MeanDecreaseAccuracy)]
myres &amp;lt;- myres[, i := as.integer(i)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(myres, 
       aes(x = reorder(factor(varname), MeanDecreaseAccuracy), y = MeanDecreaseAccuracy)) + 
  geom_point() + coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/interaction_detection/interaction-detection_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Add the top 2 vars as an interaction to their model equation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_glm_hr_int &amp;lt;- glm(log(medv) ~ I(rm^2) + age + log(dis) + log(rad) + tax + ptratio + 
                     black + I(black^2) + log(lstat) + crim + zn + indus + chas + I(nox^2) +
                   lstat:nox, data = Boston, 
              family = &amp;quot;gaussian&amp;quot;)
summary(my_glm_hr_int)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = log(medv) ~ I(rm^2) + age + log(dis) + log(rad) + 
##     tax + ptratio + black + I(black^2) + log(lstat) + crim + 
##     zn + indus + chas + I(nox^2) + lstat:nox, family = &amp;quot;gaussian&amp;quot;, 
##     data = Boston)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -0.70340  -0.09274  -0.00665   0.10068   0.75004  
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  4.243e+00  1.613e-01  26.304  &amp;lt; 2e-16 ***
## I(rm^2)      7.053e-03  1.286e-03   5.484 6.66e-08 ***
## age         -3.146e-04  5.174e-04  -0.608  0.54354    
## log(dis)    -2.254e-01  3.317e-02  -6.795 3.15e-11 ***
## log(rad)     9.829e-02  1.862e-02   5.278 1.96e-07 ***
## tax         -4.589e-04  1.196e-04  -3.838  0.00014 ***
## ptratio     -2.990e-02  4.910e-03  -6.089 2.30e-09 ***
## black        1.445e-03  4.955e-04   2.917  0.00370 ** 
## I(black^2)  -2.470e-06  1.089e-06  -2.268  0.02376 *  
## log(lstat)  -2.143e-01  3.989e-02  -5.373 1.20e-07 ***
## crim        -1.046e-02  1.238e-03  -8.448 3.40e-16 ***
## zn           7.309e-04  5.099e-04   1.434  0.15234    
## indus       -8.166e-05  2.307e-03  -0.035  0.97178    
## chas         8.746e-02  3.231e-02   2.707  0.00704 ** 
## I(nox^2)    -3.618e-01  1.256e-01  -2.880  0.00415 ** 
## lstat:nox   -2.367e-02  4.819e-03  -4.911 1.24e-06 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for gaussian family taken to be 0.03150809)
## 
##     Null deviance: 84.376  on 505  degrees of freedom
## Residual deviance: 15.439  on 490  degrees of freedom
## AIC: -295.79
## 
## Number of Fisher Scoring iterations: 2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;AIC(my_glm_hr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -273.4788&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;AIC(my_glm_hr_int)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -295.7931&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This results in a significant improvement!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;repeat-this-procedure&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Repeat this procedure&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pred_RF &amp;lt;- predict(my_ranger_log, data = Boston)
#pred_RF$predictions
pred_GLM &amp;lt;- predict(my_glm_hr_int, data = Boston)

plot(pred_RF$predictions, pred_GLM)
abline(0, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/interaction_detection/interaction-detection_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pred_diff &amp;lt;- pred_RF$predictions - pred_GLM

my_ranger_log_diff2 &amp;lt;- ranger(Ydiff ~ . - medv, data = data.table(Ydiff = pred_diff, Boston),
                                  importance = &amp;quot;permutation&amp;quot;, num.trees = 500,
                                  mtry = 5, replace = TRUE)
my_ranger_log_diff2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Ranger result
## 
## Call:
##  ranger(Ydiff ~ . - medv, data = data.table(Ydiff = pred_diff,      Boston), importance = &amp;quot;permutation&amp;quot;, num.trees = 500, mtry = 5,      replace = TRUE) 
## 
## Type:                             Regression 
## Number of trees:                  500 
## Sample size:                      506 
## Number of independent variables:  13 
## Mtry:                             5 
## Target node size:                 5 
## Variable importance mode:         permutation 
## Splitrule:                        variance 
## OOB prediction error (MSE):       0.008653356 
## R squared (OOB):                  0.5201821&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;myres_tmp &amp;lt;- ranger::importance(my_ranger_log_diff2)
myres &amp;lt;- cbind(names(myres_tmp), myres_tmp,  i = 1)
#my_rownames &amp;lt;- row.names(myres)
myres &amp;lt;- data.table(myres)
setnames(myres, &amp;quot;V1&amp;quot;, &amp;quot;varname&amp;quot;)
setnames(myres, &amp;quot;myres_tmp&amp;quot;, &amp;quot;MeanDecreaseAccuracy&amp;quot;)
myres &amp;lt;- myres[, varname := as.factor(varname)]
myres &amp;lt;- myres[, MeanDecreaseAccuracy := as.numeric(MeanDecreaseAccuracy)]
myres &amp;lt;- myres[, i := as.integer(i)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(myres, 
       aes(x = reorder(factor(varname), MeanDecreaseAccuracy), y = MeanDecreaseAccuracy)) + 
  geom_point() + coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/interaction_detection/interaction-detection_files/figure-html/unnamed-chunk-26-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now we add lstat and dis as an interaction.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_glm_hr_int2 &amp;lt;- glm(log(medv) ~ I(rm^2) + age + log(dis) + log(rad) + tax + ptratio + 
                     black + I(black^2) + log(lstat) + crim + zn + indus + chas + I(nox^2) +
                   lstat:nox + lstat:dis, data = Boston, 
              family = &amp;quot;gaussian&amp;quot;)
summary(my_glm_hr_int2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = log(medv) ~ I(rm^2) + age + log(dis) + log(rad) + 
##     tax + ptratio + black + I(black^2) + log(lstat) + crim + 
##     zn + indus + chas + I(nox^2) + lstat:nox + lstat:dis, family = &amp;quot;gaussian&amp;quot;, 
##     data = Boston)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -0.70136  -0.08746  -0.00589   0.08857   0.76349  
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  4.535e+00  1.712e-01  26.481  &amp;lt; 2e-16 ***
## I(rm^2)      7.498e-03  1.266e-03   5.924 5.94e-09 ***
## age         -1.262e-03  5.504e-04  -2.293  0.02226 *  
## log(dis)    -4.065e-01  5.203e-02  -7.813 3.43e-14 ***
## log(rad)     9.668e-02  1.828e-02   5.290 1.85e-07 ***
## tax         -4.622e-04  1.173e-04  -3.940 9.35e-05 ***
## ptratio     -2.640e-02  4.881e-03  -5.409 9.93e-08 ***
## black        1.313e-03  4.871e-04   2.696  0.00727 ** 
## I(black^2)  -2.172e-06  1.071e-06  -2.029  0.04303 *  
## log(lstat)  -3.181e-01  4.553e-02  -6.987 9.23e-12 ***
## crim        -1.049e-02  1.215e-03  -8.635  &amp;lt; 2e-16 ***
## zn           9.078e-04  5.019e-04   1.809  0.07108 .  
## indus       -2.733e-04  2.264e-03  -0.121  0.90395    
## chas         7.166e-02  3.191e-02   2.246  0.02515 *  
## I(nox^2)    -2.569e-01  1.255e-01  -2.048  0.04113 *  
## lstat:nox   -2.729e-02  4.798e-03  -5.689 2.21e-08 ***
## lstat:dis    3.906e-03  8.754e-04   4.462 1.01e-05 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for gaussian family taken to be 0.03033711)
## 
##     Null deviance: 84.376  on 505  degrees of freedom
## Residual deviance: 14.835  on 489  degrees of freedom
## AIC: -313.99
## 
## Number of Fisher Scoring iterations: 2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;AIC(my_glm_hr_int2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -313.9904&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;AIC(my_glm_hr_int)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -295.7931&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And again we find an improvement in model fit.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;have-these-interactions-already-been-reported-on-in-the-literature&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Have these interactions already been reported on in the literature?&lt;/h1&gt;
&lt;p&gt;Tom Minka reports on his website an analysis of interactions in the Boston Housing set:&lt;/p&gt;
&lt;p&gt;(&lt;a href=&#34;http://alumni.media.mit.edu/~tpminka/courses/36-350.2001/lectures/day30/&#34; class=&#34;uri&#34;&gt;http://alumni.media.mit.edu/~tpminka/courses/36-350.2001/lectures/day30/&lt;/a&gt;) &lt;code&gt;&amp;gt; summary(fit3) Coefficients:                   Estimate Std. Error t value Pr(&amp;gt;|t|)     (Intercept)      -227.5485    49.2363  -4.622 4.87e-06 *** lstat              50.8553    20.3184   2.503 0.012639 *   rm                 38.1245     7.0987   5.371 1.21e-07 *** dis               -16.8163     2.9174  -5.764 1.45e-08 *** ptratio            14.9592     2.5847   5.788 1.27e-08 *** lstat:rm           -6.8143     3.1209  -2.183 0.029475 *   lstat:dis           4.8736     1.3940   3.496 0.000514 *** lstat:ptratio      -3.3209     1.0345  -3.210 0.001412 **  rm:dis              2.0295     0.4435   4.576 5.99e-06 *** rm:ptratio         -1.9911     0.3757  -5.299 1.76e-07 *** lstat:rm:dis       -0.5216     0.2242  -2.327 0.020364 *   lstat:rm:ptratio    0.3368     0.1588   2.121 0.034423 *&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Rob mcCulloch, using BART (bayesian additive regression trees) also examines interactions in the Boston Housing data. There the co-occurence within trees is used to discover interactions:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;The second, interaction detection, uncovers which pairs of variables interact in analogous fashion by keeping track of the percentage of trees in the sum in which both variables occur.  This exploits the fact that a sum-of-trees model captures an interaction between xi and xj by using them both for splitting rules in the same tree.&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.rob-mcculloch.org/some_papers_and_talks/papers/working/cgm_as.pdf&#34; class=&#34;uri&#34;&gt;http://www.rob-mcculloch.org/some_papers_and_talks/papers/working/cgm_as.pdf&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;boston_uit_bart_book.png&#34; /&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;We conclude that this appears a fruitfull approach to at least discovering where a regression model can be improved.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
